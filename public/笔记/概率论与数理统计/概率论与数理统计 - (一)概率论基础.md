![(一)概率论基础](https://via.placeholder.com/800x200?text=Probability+Basics)

# 概率论与数理统计 - (一)概率论基础

学习概率的基本概念和公理。

---

# 概率论与数理统计

> 💡 **课程信息**
> - 学习时长：120小时
> - 难度等级：⭐⭐⭐⭐ (高)
> - 前置课程：微积分
> - **机器学习的数学基础**

---

## 📚 课程概述

### 在计算机科学中的应用

| 主题 | CS应用 |
|------|--------|
| **贝叶斯定理** | 垃圾邮件过滤、贝叶斯分类器 |
| **概率分布** | 随机算法分析、性能建模 |
| **期望与方差** | 算法复杂度分析 |
| **大数定律** | 蒙特卡洛方法 |
| **统计推断** | A/B测试、机器学习评估 |

---

## 1. 概率论基础

### 1.1 概率公理

**样本空间** $\Omega$：所有可能结果的集合

**概率公理（Kolmogorov）：**

1. $P(A) \geq 0$
2. $P(\Omega) = 1$
3. $P(A \cup B) = P(A) + P(B)$ （$A, B$ 互斥）

### 1.2 条件概率

$$
P(A|B) = \frac{P(A \cap B)}{P(B)}
$$

**贝叶斯定理：**

$$
P(A|B) = \frac{P(B|A) P(A)}{P(B)}
$$

**全概率公式：**

$$
P(B) = \sum_{i=1}^{n} P(B|A_i) P(A_i)
$$

**示例：垃圾邮件过滤**

```python
import numpy as np

class NaiveBayesSpamFilter:
    def __init__(self):
        self.word_prob_spam = {}
        self.word_prob_ham = {}
        self.p_spam = 0.5
        self.p_ham = 0.5
    
    def train(self, emails, labels):
        """
        训练贝叶斯分类器
        emails: 邮件列表（每个邮件是单词列表）
        labels: 标签列表（1=spam, 0=ham）
        """
        spam_emails = [emails[i] for i in range(len(emails)) if labels[i] == 1]
        ham_emails = [emails[i] for i in range(len(emails)) if labels[i] == 0]
        
        self.p_spam = len(spam_emails) / len(emails)
        self.p_ham = len(ham_emails) / len(emails)
        
        # 统计词频
        spam_words = [word for email in spam_emails for word in email]
        ham_words = [word for email in ham_emails for word in email]
        
        vocab = set(spam_words + ham_words)
        
        for word in vocab:
            self.word_prob_spam[word] = (spam_words.count(word) + 1) / (len(spam_words) + len(vocab))
            self.word_prob_ham[word] = (ham_words.count(word) + 1) / (len(ham_words) + len(vocab))
    
    def predict(self, email):
        """预测邮件是否为垃圾邮件"""
        # P(spam|email) ∝ P(email|spam) * P(spam)
        log_p_spam = np.log(self.p_spam)
        log_p_ham = np.log(self.p_ham)
        
        for word in email:
            if word in self.word_prob_spam:
                log_p_spam += np.log(self.word_prob_spam[word])
                log_p_ham += np.log(self.word_prob_ham[word])
        
        return 1 if log_p_spam > log_p_ham else 0

# 训练数据
emails = [
    ['free', 'money', 'now'],
    ['meeting', 'schedule', 'tomorrow'],
    ['win', 'prize', 'free'],
    ['project', 'deadline', 'update']
]
labels = [1, 0, 1, 0]  # 1=spam, 0=ham

classifier = NaiveBayesSpamFilter()
classifier.train(emails, labels)

test_email = ['free', 'prize']
print(f"预测: {'垃圾邮件' if classifier.predict(test_email) == 1 else '正常邮件'}")
```

### 1.3 事件独立性

**定义：**

$$
P(A \cap B) = P(A) \cdot P(B)
$$

**应用：随机算法分析**

```python
def random_quicksort_analysis():
    """
    快速排序的期望复杂度分析
    基于独立性假设
    """
    import random
    
    def quicksort_comparisons(arr, count=[0]):
        if len(arr) <= 1:
            return arr
        
        pivot = arr[0]
        count[0] += len(arr) - 1  # 与pivot比较次数
        
        left = [x for x in arr[1:] if x <= pivot]
        right = [x for x in arr[1:] if x > pivot]
        
        return quicksort_comparisons(left, count) + [pivot] + quicksort_comparisons(right, count)
    
    n = 100
    trials = 1000
    total_comparisons = 0
    
    for _ in range(trials):
        arr = list(range(n))
        random.shuffle(arr)
        count = [0]
        quicksort_comparisons(arr, count)
        total_comparisons += count[0]
    
    avg_comparisons = total_comparisons / trials
    theoretical = n * np.log(n) * 1.386  # ≈ 2n ln n
    
    print(f"平均比较次数: {avg_comparisons:.0f}")
    print(f"理论值 (2n ln n): {theoretical:.0f}")

random_quicksort_analysis()
```

---

## 2. 随机变量与分布