![(ä¸€)ç›‘ç£å­¦ä¹ ](https://via.placeholder.com/800x200?text=Supervised+Learning)

# æœºå™¨å­¦ä¹  - (ä¸€)ç›‘ç£å­¦ä¹ 

æŒæ¡ç›‘ç£å­¦ä¹ ç»å…¸ç®—æ³•ã€‚

---

# æœºå™¨å­¦ä¹ 

> ğŸ’¡ **è¯¾ç¨‹ä¿¡æ¯**
> - å­¦ä¹ æ—¶é•¿ï¼š200å°æ—¶
> - éš¾åº¦ç­‰çº§ï¼šâ­â­â­â­â­ (æé«˜)
> - **AIæ ¸å¿ƒæŠ€æœ¯æ·±åº¦å­¦ä¹ **

---

## ğŸ“š æœºå™¨å­¦ä¹ æµç¨‹

```
æ•°æ®æ”¶é›† â†’ æ•°æ®é¢„å¤„ç† â†’ ç‰¹å¾å·¥ç¨‹ â†’ æ¨¡å‹é€‰æ‹© â†’ è®­ç»ƒ â†’ è¯„ä¼° â†’ éƒ¨ç½²
```

---

## 1. ç›‘ç£å­¦ä¹ 

### 1.1 å†³ç­–æ ‘

```python
import numpy as np
from collections import Counter

class DecisionTree:
    def __init__(self, max_depth=10, min_samples_split=2):
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.tree = None
    
    def entropy(self, y):
        """è®¡ç®—ç†µ"""
        counts = Counter(y)
        probs = [count / len(y) for count in counts.values()]
        return -sum(p * np.log2(p) for p in probs if p > 0)
    
    def information_gain(self, X, y, feature_idx, threshold):
        """è®¡ç®—ä¿¡æ¯å¢ç›Š"""
        left_mask = X[:, feature_idx] <= threshold
        right_mask = ~left_mask
        
        if len(y[left_mask]) == 0 or len(y[right_mask]) == 0:
            return 0
        
        n = len(y)
        left_entropy = self.entropy(y[left_mask])
        right_entropy = self.entropy(y[right_mask])
        
        weighted_entropy = (len(y[left_mask]) / n) * left_entropy + \
                          (len(y[right_mask]) / n) * right_entropy
        
        return self.entropy(y) - weighted_entropy
    
    def best_split(self, X, y):
        """æ‰¾åˆ°æœ€ä½³åˆ†å‰²"""
        best_gain = 0
        best_feature = None
        best_threshold = None
        
        for feature_idx in range(X.shape[1]):
            thresholds = np.unique(X[:, feature_idx])
            
            for threshold in thresholds:
                gain = self.information_gain(X, y, feature_idx, threshold)
                
                if gain > best_gain:
                    best_gain = gain
                    best_feature = feature_idx
                    best_threshold = threshold
        
        return best_feature, best_threshold, best_gain
    
    def build_tree(self, X, y, depth=0):
        """æ„å»ºå†³ç­–æ ‘"""
        # åœæ­¢æ¡ä»¶
        if (depth >= self.max_depth or 
            len(y) < self.min_samples_split or 
            len(np.unique(y)) == 1):
            return Counter(y).most_common(1)[0][0]
        
        # æ‰¾åˆ°æœ€ä½³åˆ†å‰²
        feature, threshold, gain = self.best_split(X, y)
        
        if gain == 0:
            return Counter(y).most_common(1)[0][0]
        
        # åˆ†å‰²æ•°æ®
        left_mask = X[:, feature] <= threshold
        right_mask = ~left_mask
        
        # é€’å½’æ„å»ºå­æ ‘
        left_subtree = self.build_tree(X[left_mask], y[left_mask], depth + 1)
        right_subtree = self.build_tree(X[right_mask], y[right_mask], depth + 1)
        
        return {
            'feature': feature,
            'threshold': threshold,
            'left': left_subtree,
            'right': right_subtree
        }
    
    def fit(self, X, y):
        """è®­ç»ƒå†³ç­–æ ‘"""
        self.tree = self.build_tree(X, y)
    
    def predict_sample(self, x, tree):
        """é¢„æµ‹å•ä¸ªæ ·æœ¬"""
        if not isinstance(tree, dict):
            return tree
        
        if x[tree['feature']] <= tree['threshold']:
            return self.predict_sample(x, tree['left'])
        else:
            return self.predict_sample(x, tree['right'])
    
    def predict(self, X):
        """é¢„æµ‹"""
        return [self.predict_sample(x, self.tree) for x in X]

# ç¤ºä¾‹
from sklearn.datasets import make_classification
X, y = make_classification(n_samples=100, n_features=4, n_classes=2, random_state=42)

dt = DecisionTree(max_depth=5)
dt.fit(X, y)
predictions = dt.predict(X[:5])
print(f"é¢„æµ‹ç»“æœ: {predictions}")
```

### 1.2 æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰

```python
import numpy as np

class SVM:
    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):
        self.lr = learning_rate
        self.lambda_param = lambda_param
        self.n_iters = n_iters
        self.w = None
        self.b = None
    
    def fit(self, X, y):
        """è®­ç»ƒSVM"""
        n_samples, n_features = X.shape
        
        # å°†æ ‡ç­¾è½¬æ¢ä¸º-1å’Œ1
        y_ = np.where(y <= 0, -1, 1)
        
        # åˆå§‹åŒ–æƒé‡
        self.w = np.zeros(n_features)
        self.b = 0
        
        for i in range(self.n_iters):
            for idx, x_i in enumerate(X):
                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1
                
                if condition:
                    # æ­£ç¡®åˆ†ç±»
                    self.w -= self.lr * (2 * self.lambda_param * self.w)
                else:
                    # é”™è¯¯åˆ†ç±»æˆ–åœ¨è¾¹ç•Œä¸Š
                    self.w -= self.lr * (2 * self.lambda_param * self.w - np.dot(x_i, y_[idx]))
                    self.b -= self.lr * y_[idx]
    
    def predict(self, X):
        """é¢„æµ‹"""
        approximation = np.dot(X, self.w) - self.b
        return np.sign(approximation)

# ç¤ºä¾‹
X = np.array([[3, 3], [4, 3], [1, 1]])
y = np.array([1, 1, -1])

svm = SVM()
svm.fit(X, y)
predictions = svm.predict(X)
print(f"SVMé¢„æµ‹: {predictions}")
```

### 1.3 éšæœºæ£®æ—

```python
import random

class RandomForest:
    def __init__(self, n_trees=10, max_depth=10, min_samples_split=2, n_features=None):
        self.n_trees = n_trees
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.n_features = n_features
        self.trees = []
    
    def bootstrap_sample(self, X, y):
        """è‡ªåŠ©é‡‡æ ·"""
        n_samples = X.shape[0]
        idxs = np.random.choice(n_samples, n_samples, replace=True)
        return X[idxs], y[idxs]
    
    def fit(self, X, y):
        """è®­ç»ƒéšæœºæ£®æ—"""
        self.trees = []
        
        for _ in range(self.n_trees):
            tree = DecisionTree(
                max_depth=self.max_depth,
                min_samples_split=self.min_samples_split
            )
            
            X_sample, y_sample = self.bootstrap_sample(X, y)
            tree.fit(X_sample, y_sample)
            self.trees.append(tree)
    
    def predict(self, X):
        """é¢„æµ‹ï¼ˆæŠ•ç¥¨ï¼‰"""
        predictions = np.array([tree.predict(X) for tree in self.trees])
        return [Counter(predictions[:, i]).most_common(1)[0][0] for i in range(len(X))]

# ç¤ºä¾‹
rf = RandomForest(n_trees=5)
rf.fit(X, y)
rf_predictions = rf.predict(X[:3])
print(f"éšæœºæ£®æ—é¢„æµ‹: {rf_predictions}")
```

---

## 2. æ— ç›‘ç£å­¦ä¹ 