# 深度学习 - (六)正则化技术

学习防止过拟合技术。

---

## 6. 正则化技术

### 6.1 Dropout

```python
class Dropout:
    """Dropout层"""
    def __init__(self, drop_rate=0.5):
        self.drop_rate = drop_rate
        self.mask = None
    
    def forward(self, x, training=True):
        """前向传播"""
        if training:
            # 生成掩码
            self.mask = (np.random.rand(*x.shape) > self.drop_rate).astype(float)
            # Inverted dropout（缩放）
            return x * self.mask / (1 - self.drop_rate)
        else:
            # 测试时不dropout
            return x
    
    def backward(self, dout):
        """反向传播"""
        return dout * self.mask / (1 - self.drop_rate)

# 示例
x = np.random.randn(32, 128)  # batch_size=32, features=128
dropout = Dropout(drop_rate=0.5)

# 训练模式
x_train = dropout.forward(x, training=True)
print(f"训练模式 - 激活神经元比例: {np.mean(x_train != 0):.2f}")

# 测试模式
x_test = dropout.forward(x, training=False)
print(f"测试模式 - 激活神经元比例: {np.mean(x_test != 0):.2f}")
```

### 6.2 批归一化（Batch Normalization）

```python
class BatchNormalization:
    """批归一化层"""
    def __init__(self, num_features, eps=1e-5, momentum=0.1):
        self.num_features = num_features
        self.eps = eps
        self.momentum = momentum
        
        # 可学习参数
        self.gamma = np.ones(num_features)
        self.beta = np.zeros(num_features)
        
        # 运行时统计量
        self.running_mean = np.zeros(num_features)
        self.running_var = np.ones(num_features)
        
        # 缓存
        self.x_norm = None
        self.x_centered = None
        self.std = None
    
    def forward(self, x, training=True):
        """前向传播"""
        if training:
            # 计算批次统计量
            batch_mean = np.mean(x, axis=0)
            batch_var = np.var(x, axis=0)
            
            # 更新运行时统计量
            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean
            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var
            
            # 标准化
            self.x_centered = x - batch_mean
            self.std = np.sqrt(batch_var + self.eps)
            self.x_norm = self.x_centered / self.std
        else:
            # 使用运行时统计量
            self.x_norm = (x - self.running_mean) / np.sqrt(self.running_var + self.eps)
        
        # 缩放和平移
        out = self.gamma * self.x_norm + self.beta
        
        return out
    
    def backward(self, dout):
        """反向传播"""
        N = dout.shape[0]
        
        # 反向传播到gamma和beta
        dgamma = np.sum(dout * self.x_norm, axis=0)
        dbeta = np.sum(dout, axis=0)
        
        # 反向传播到x
        dx_norm = dout * self.gamma
        dvar = np.sum(dx_norm * self.x_centered * -0.5 * (self.std ** -3), axis=0)
        dmean = np.sum(dx_norm * -1 / self.std, axis=0) + dvar * np.sum(-2 * self.x_centered, axis=0) / N
        
        dx = dx_norm / self.std + dvar * 2 * self.x_centered / N + dmean / N
        
        return dx

# 示例
x = np.random.randn(32, 64)  # batch_size=32, features=64
bn = BatchNormalization(num_features=64)

# 训练模式
x_train = bn.forward(x, training=True)
print(f"训练模式 - 均值: {np.mean(x_train, axis=0)[:5]}")
print(f"训练模式 - 方差: {np.var(x_train, axis=0)[:5]}")

# 测试模式
x_test = bn.forward(x, training=False)
print(f"测试模式 - 均值: {np.mean(x_test, axis=0)[:5]}")
```

### 6.3 层归一化（Layer Normalization）

```python
class LayerNormalization:
    """层归一化"""
    def __init__(self, normalized_shape, eps=1e-5):
        self.eps = eps
        self.gamma = np.ones(normalized_shape)
        self.beta = np.zeros(normalized_shape)
    
    def forward(self, x):
        """前向传播"""
        # 在最后一个维度上计算均值和方差
        mean = np.mean(x, axis=-1, keepdims=True)
        var = np.var(x, axis=-1, keepdims=True)
        
        # 标准化
        x_norm = (x - mean) / np.sqrt(var + self.eps)
        
        # 缩放和平移
        out = self.gamma * x_norm + self.beta
        
        return out

# 示例（通常用于Transformer）
x = np.random.randn(32, 10, 512)  # (batch, seq_len, d_model)
ln = LayerNormalization(normalized_shape=512)
x_norm = ln.forward(x)

print(f"输入形状: {x.shape}")
print(f"输出形状: {x_norm.shape}")
print(f"每个样本归一化后均值: {np.mean(x_norm[0], axis=-1)[:5]}")
```

---

**本章完**
