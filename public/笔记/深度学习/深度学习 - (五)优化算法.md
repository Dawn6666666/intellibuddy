# 深度学习 - (五)优化算法

掌握深度学习优化方法。

---

## 5. 优化算法

### 5.1 梯度下降变种

```python
class Optimizer:
    """优化器基类"""
    
    @staticmethod
    def sgd(weights, gradients, learning_rate=0.01):
        """随机梯度下降"""
        return weights - learning_rate * gradients
    
    @staticmethod
    def momentum(weights, gradients, velocity, learning_rate=0.01, momentum=0.9):
        """动量优化"""
        velocity = momentum * velocity - learning_rate * gradients
        weights = weights + velocity
        return weights, velocity
    
    @staticmethod
    def adagrad(weights, gradients, cache, learning_rate=0.01, eps=1e-8):
        """AdaGrad优化"""
        cache += gradients ** 2
        weights -= learning_rate * gradients / (np.sqrt(cache) + eps)
        return weights, cache
    
    @staticmethod
    def rmsprop(weights, gradients, cache, learning_rate=0.001, decay_rate=0.9, eps=1e-8):
        """RMSProp优化"""
        cache = decay_rate * cache + (1 - decay_rate) * gradients ** 2
        weights -= learning_rate * gradients / (np.sqrt(cache) + eps)
        return weights, cache

class Adam:
    """Adam优化器"""
    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, eps=1e-8):
        self.lr = learning_rate
        self.beta1 = beta1
        self.beta2 = beta2
        self.eps = eps
        self.m = None  # 一阶矩估计
        self.v = None  # 二阶矩估计
        self.t = 0     # 时间步
    
    def update(self, weights, gradients):
        """Adam更新"""
        if self.m is None:
            self.m = np.zeros_like(weights)
            self.v = np.zeros_like(weights)
        
        self.t += 1
        
        # 更新偏差一阶矩和二阶矩估计
        self.m = self.beta1 * self.m + (1 - self.beta1) * gradients
        self.v = self.beta2 * self.v + (1 - self.beta2) * (gradients ** 2)
        
        # 偏差修正
        m_hat = self.m / (1 - self.beta1 ** self.t)
        v_hat = self.v / (1 - self.beta2 ** self.t)
        
        # 更新权重
        weights -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)
        
        return weights

# 示例
weights = np.random.randn(10, 5)
gradients = np.random.randn(10, 5) * 0.1

# Adam优化
adam = Adam(learning_rate=0.001)
for i in range(100):
    weights = adam.update(weights, gradients)
    if i % 20 == 0:
        print(f"Iteration {i}, 权重范数: {np.linalg.norm(weights):.4f}")
```

### 5.2 学习率调度

```python
class LearningRateScheduler:
    """学习率调度器"""
    
    @staticmethod
    def step_decay(initial_lr, epoch, drop_every=10, drop_rate=0.5):
        """阶梯衰减"""
        return initial_lr * (drop_rate ** (epoch // drop_every))
    
    @staticmethod
    def exponential_decay(initial_lr, epoch, decay_rate=0.95):
        """指数衰减"""
        return initial_lr * (decay_rate ** epoch)
    
    @staticmethod
    def cosine_annealing(initial_lr, epoch, T_max):
        """余弦退火"""
        return initial_lr * (1 + np.cos(np.pi * epoch / T_max)) / 2
    
    @staticmethod
    def warmup_linear(initial_lr, epoch, warmup_epochs, total_epochs):
        """预热+线性衰减"""
        if epoch < warmup_epochs:
            # 预热阶段
            return initial_lr * (epoch + 1) / warmup_epochs
        else:
            # 线性衰减
            progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)
            return initial_lr * (1 - progress)

# 可视化学习率变化
initial_lr = 0.1
epochs = 100

print("=== 学习率调度对比 ===")
for epoch in [0, 20, 40, 60, 80]:
    step_lr = LearningRateScheduler.step_decay(initial_lr, epoch)
    exp_lr = LearningRateScheduler.exponential_decay(initial_lr, epoch)
    cos_lr = LearningRateScheduler.cosine_annealing(initial_lr, epoch, epochs)
    
    print(f"Epoch {epoch:3d} | Step: {step_lr:.6f} | Exp: {exp_lr:.6f} | Cos: {cos_lr:.6f}")
```

---

**本章完**
