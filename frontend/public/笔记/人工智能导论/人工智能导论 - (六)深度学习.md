# 人工智能导论 - (六)深度学习

深入学习深度学习。

---

## 6. 深度学习

### 6.1 卷积神经网络（CNN）

```python
import numpy as np

class Conv2D:
    """2D卷积层"""
    def __init__(self, in_channels, out_channels, kernel_size, stride=1):
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        
        # 初始化权重
        self.weights = np.random.randn(out_channels, in_channels, kernel_size, kernel_size) * 0.1
        self.bias = np.zeros(out_channels)
    
    def forward(self, x):
        """前向传播"""
        batch_size, _, height, width = x.shape
        out_height = (height - self.kernel_size) // self.stride + 1
        out_width = (width - self.kernel_size) // self.stride + 1
        
        output = np.zeros((batch_size, self.out_channels, out_height, out_width))
        
        for i in range(out_height):
            for j in range(out_width):
                h_start = i * self.stride
                h_end = h_start + self.kernel_size
                w_start = j * self.stride
                w_end = w_start + self.kernel_size
                
                # 提取感受野
                receptive_field = x[:, :, h_start:h_end, w_start:w_end]
                
                # 卷积操作
                for k in range(self.out_channels):
                    output[:, k, i, j] = np.sum(receptive_field * self.weights[k], axis=(1, 2, 3)) + self.bias[k]
        
        return output

class MaxPooling:
    """最大池化层"""
    def __init__(self, pool_size=2, stride=2):
        self.pool_size = pool_size
        self.stride = stride
    
    def forward(self, x):
        """前向传播"""
        batch_size, channels, height, width = x.shape
        out_height = (height - self.pool_size) // self.stride + 1
        out_width = (width - self.pool_size) // self.stride + 1
        
        output = np.zeros((batch_size, channels, out_height, out_width))
        
        for i in range(out_height):
            for j in range(out_width):
                h_start = i * self.stride
                h_end = h_start + self.pool_size
                w_start = j * self.stride
                w_end = w_start + self.pool_size
                
                pool_region = x[:, :, h_start:h_end, w_start:w_end]
                output[:, :, i, j] = np.max(pool_region, axis=(2, 3))
        
        return output

# 示例：简单CNN
class SimpleCNN:
    """简单的CNN模型"""
    def __init__(self):
        self.conv1 = Conv2D(1, 8, 3)
        self.pool1 = MaxPooling(2, 2)
        self.conv2 = Conv2D(8, 16, 3)
        self.pool2 = MaxPooling(2, 2)
    
    def forward(self, x):
        """前向传播"""
        x = self.conv1.forward(x)
        x = np.maximum(x, 0)  # ReLU
        x = self.pool1.forward(x)
        
        x = self.conv2.forward(x)
        x = np.maximum(x, 0)
        x = self.pool2.forward(x)
        
        return x

# 使用
cnn = SimpleCNN()
x = np.random.randn(1, 1, 28, 28)  # batch_size=1, channels=1, 28x28图像
output = cnn.forward(x)
print(f"输出形状: {output.shape}")
```

### 6.2 循环神经网络（RNN）

```python
class RNN:
    """简单RNN"""
    def __init__(self, input_size, hidden_size, output_size):
        self.hidden_size = hidden_size
        
        # 权重初始化
        self.Wxh = np.random.randn(hidden_size, input_size) * 0.01
        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01
        self.Why = np.random.randn(output_size, hidden_size) * 0.01
        
        self.bh = np.zeros((hidden_size, 1))
        self.by = np.zeros((output_size, 1))
    
    def forward(self, inputs, h_prev):
        """前向传播
        inputs: 时间步输入列表
        h_prev: 初始隐藏状态
        """
        xs, hs, ys, ps = {}, {}, {}, {}
        hs[-1] = np.copy(h_prev)
        
        for t in range(len(inputs)):
            xs[t] = inputs[t]
            
            # 隐藏状态更新
            hs[t] = np.tanh(np.dot(self.Wxh, xs[t]) + np.dot(self.Whh, hs[t-1]) + self.bh)
            
            # 输出
            ys[t] = np.dot(self.Why, hs[t]) + self.by
            
            # Softmax
            ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))
        
        return xs, hs, ps

# 示例
rnn = RNN(input_size=10, hidden_size=20, output_size=10)
inputs = [np.random.randn(10, 1) for _ in range(5)]  # 5个时间步
h0 = np.zeros((20, 1))

xs, hs, ps = rnn.forward(inputs, h0)
print(f"时间步0的输出概率: {ps[0].flatten()[:3]}")
```

### 6.3 注意力机制

```python
def scaled_dot_product_attention(Q, K, V):
    """缩放点积注意力
    Q: Query矩阵 (seq_len, d_k)
    K: Key矩阵 (seq_len, d_k)
    V: Value矩阵 (seq_len, d_v)
    """
    d_k = K.shape[-1]
    
    # 计算注意力分数
    scores = np.dot(Q, K.T) / np.sqrt(d_k)
    
    # Softmax归一化
    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)
    
    # 加权求和
    output = np.dot(attention_weights, V)
    
    return output, attention_weights

# 示例
seq_len, d_model = 10, 64
Q = np.random.randn(seq_len, d_model)
K = np.random.randn(seq_len, d_model)
V = np.random.randn(seq_len, d_model)

output, weights = scaled_dot_product_attention(Q, K, V)
print(f"注意力输出形状: {output.shape}")
print(f"注意力权重形状: {weights.shape}")
```

---

**本章完**
