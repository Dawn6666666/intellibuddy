# 人工智能导论 - (五)自然语言处理

掌握NLP核心技术和深度学习应用。

---

## 5. 自然语言处理

### 5.1 词频统计（TF-IDF）

TF-IDF (Term Frequency-Inverse Document Frequency) 是一种统计方法，用于评估词语对文档集的重要程度。

```python
import math
from collections import Counter

class TFIDFVectorizer:
    def __init__(self):
        self.vocabulary = {}
        self.idf_values = {}
    
    def fit(self, documents):
        """训练TF-IDF模型"""
        # 构建词汇表
        all_words = set()
        for doc in documents:
            words = doc.lower().split()
            all_words.update(words)
        
        self.vocabulary = {word: i for i, word in enumerate(all_words)}
        
        # 计算IDF值
        n_docs = len(documents)
        for word in self.vocabulary:
            df = sum(1 for doc in documents if word in doc.lower().split())
            self.idf_values[word] = math.log(n_docs / df)
    
    def transform(self, documents):
        """转换文档为TF-IDF向量"""
        vectors = []
        
        for doc in documents:
            words = doc.lower().split()
            word_count = Counter(words)
            doc_length = len(words)
            
            vector = [0] * len(self.vocabulary)
            
            for word, count in word_count.items():
                if word in self.vocabulary:
                    tf = count / doc_length
                    tfidf = tf * self.idf_values[word]
                    vector[self.vocabulary[word]] = tfidf
            
            vectors.append(vector)
        
        return vectors

# 示例
documents = [
    "机器学习是人工智能的分支",
    "深度学习是机器学习的子集",
    "神经网络是深度学习的基础"
]

vectorizer = TFIDFVectorizer()
vectorizer.fit(documents)
vectors = vectorizer.transform(documents)

print(f"词汇表: {list(vectorizer.vocabulary.keys())}")
print(f"第一个文档向量: {vectors[0][:5]}")
```

**TF-IDF公式**:

$$
\text{TF-IDF}(t, d) = \text{TF}(t, d) \times \text{IDF}(t)
$$

其中：

$$
\text{TF}(t, d) = \frac{\text{词t在文档d中出现次数}}{\text{文档d的总词数}}
$$

$$
\text{IDF}(t) = \log \frac{\text{总文档数}}{\text{包含词t的文档数}}
$$

### 5.2 Word2Vec词向量

Word2Vec将词语映射到连续向量空间，捕捉语义关系。

#### 5.2.1 Skip-Gram模型

```python
import numpy as np

class SkipGram:
    """Skip-Gram Word2Vec模型"""
    
    def __init__(self, vocab_size, embedding_dim=100):
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        
        # 初始化嵌入矩阵
        self.W1 = np.random.randn(vocab_size, embedding_dim) * 0.01
        self.W2 = np.random.randn(embedding_dim, vocab_size) * 0.01
    
    def softmax(self, x):
        """Softmax函数"""
        exp_x = np.exp(x - np.max(x))
        return exp_x / exp_x.sum()
    
    def forward(self, target_word_idx):
        """前向传播
        
        target_word_idx: 目标词的索引
        """
        # 隐藏层（查找嵌入向量）
        h = self.W1[target_word_idx]
        
        # 输出层
        u = np.dot(h, self.W2)
        y_pred = self.softmax(u)
        
        return h, y_pred
    
    def backward(self, target_word_idx, context_word_idx, learning_rate=0.01):
        """反向传播
        
        target_word_idx: 目标词索引
        context_word_idx: 上下文词索引
        """
        # 前向传播
        h, y_pred = self.forward(target_word_idx)
        
        # 计算误差
        error = y_pred.copy()
        error[context_word_idx] -= 1
        
        # 更新W2
        dW2 = np.outer(h, error)
        self.W2 -= learning_rate * dW2
        
        # 更新W1
        dh = np.dot(error, self.W2.T)
        self.W1[target_word_idx] -= learning_rate * dh
    
    def train(self, corpus, window_size=2, epochs=100, learning_rate=0.01):
        """训练模型
        
        corpus: 词索引列表
        window_size: 上下文窗口大小
        """
        for epoch in range(epochs):
            loss = 0
            
            for i, target_word in enumerate(corpus):
                # 获取上下文词
                start = max(0, i - window_size)
                end = min(len(corpus), i + window_size + 1)
                
                for j in range(start, end):
                    if j != i:
                        context_word = corpus[j]
                        self.backward(target_word, context_word, learning_rate)
                        
                        # 计算损失
                        _, y_pred = self.forward(target_word)
                        loss -= np.log(y_pred[context_word] + 1e-10)
            
            if epoch % 10 == 0:
                print(f"Epoch {epoch}, Loss: {loss:.4f}")
    
    def get_embedding(self, word_idx):
        """获取词向量"""
        return self.W1[word_idx]

# 示例
sentences = ["我 爱 自然 语言 处理", "深度 学习 很 有趣", "词 向量 很 有用"]
words = []
for sent in sentences:
    words.extend(sent.split())

# 构建词汇表
vocab = {word: i for i, word in enumerate(set(words))}
vocab_size = len(vocab)

# 将句子转换为索引
corpus = [vocab[word] for sent in sentences for word in sent.split()]

# 训练模型
model = SkipGram(vocab_size, embedding_dim=10)
model.train(corpus, window_size=2, epochs=50, learning_rate=0.01)

# 获取词向量
word = "学习"
if word in vocab:
    embedding = model.get_embedding(vocab[word])
    print(f"\n词'{word}'的向量: {embedding[:5]}")
```

#### 5.2.2 词向量的语义运算

```python
class WordEmbeddings:
    """词向量语义运算"""
    
    def __init__(self, word_vectors, word2idx):
        self.word_vectors = word_vectors
        self.word2idx = word2idx
        self.idx2word = {v: k for k, v in word2idx.items()}
    
    def cosine_similarity(self, vec1, vec2):
        """余弦相似度"""
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        return dot_product / (norm1 * norm2 + 1e-10)
    
    def most_similar(self, word, top_n=5):
        """找到最相似的词"""
        if word not in self.word2idx:
            return []
        
        word_vec = self.word_vectors[self.word2idx[word]]
        similarities = []
        
        for idx, vec in enumerate(self.word_vectors):
            if idx != self.word2idx[word]:
                sim = self.cosine_similarity(word_vec, vec)
                similarities.append((self.idx2word[idx], sim))
        
        return sorted(similarities, key=lambda x: x[1], reverse=True)[:top_n]
    
    def analogy(self, word1, word2, word3):
        """词类比：word1 - word2 + word3
        例如：king - man + woman = queen
        """
        if any(w not in self.word2idx for w in [word1, word2, word3]):
            return None
        
        vec1 = self.word_vectors[self.word2idx[word1]]
        vec2 = self.word_vectors[self.word2idx[word2]]
        vec3 = self.word_vectors[self.word2idx[word3]]
        
        # 计算结果向量
        result_vec = vec1 - vec2 + vec3
        
        # 找到最相似的词
        similarities = []
        exclude = {self.word2idx[word1], self.word2idx[word2], self.word2idx[word3]}
        
        for idx, vec in enumerate(self.word_vectors):
            if idx not in exclude:
                sim = self.cosine_similarity(result_vec, vec)
                similarities.append((self.idx2word[idx], sim))
        
        return max(similarities, key=lambda x: x[1])[0]

# 演示
print("\nWord2Vec特性:")
print("="*60)
print("1. 语义相似性: '学习' 和 '教育' 的向量接近")
print("2. 词类比: king - man + woman ≈ queen")
print("3. 降维可视化: 词向量可用t-SNE降至2D/3D")
```

### 5.3 Transformer架构

Transformer是现代NLP的核心架构，使用自注意力机制。

#### 5.3.1 自注意力机制

```python
class MultiHeadAttention:
    """多头自注意力机制"""
    
    def __init__(self, d_model, num_heads):
        """
        d_model: 模型维度
        num_heads: 注意力头数
        """
        assert d_model % num_heads == 0
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        # Q, K, V的权重矩阵
        self.W_q = np.random.randn(d_model, d_model) * 0.01
        self.W_k = np.random.randn(d_model, d_model) * 0.01
        self.W_v = np.random.randn(d_model, d_model) * 0.01
        self.W_o = np.random.randn(d_model, d_model) * 0.01
    
    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        """缩放点积注意力
        
        公式: Attention(Q, K, V) = softmax(QK^T / √d_k) V
        """
        d_k = K.shape[-1]
        
        # 计算注意力分数
        scores = np.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)
        
        # 应用mask（可选）
        if mask is not None:
            scores = scores + (mask * -1e9)
        
        # Softmax归一化
        attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)
        
        # 加权求和
        output = np.matmul(attention_weights, V)
        
        return output, attention_weights
    
    def split_heads(self, x):
        """分割成多个注意力头
        
        (batch_size, seq_len, d_model) -> (batch_size, num_heads, seq_len, d_k)
        """
        batch_size, seq_len, d_model = x.shape
        x = x.reshape(batch_size, seq_len, self.num_heads, self.d_k)
        return x.transpose(0, 2, 1, 3)
    
    def forward(self, Q, K, V, mask=None):
        """前向传播"""
        batch_size = Q.shape[0]
        
        # 线性变换
        Q = np.matmul(Q, self.W_q)
        K = np.matmul(K, self.W_k)
        V = np.matmul(V, self.W_v)
        
        # 分割成多头
        Q = self.split_heads(Q)
        K = self.split_heads(K)
        V = self.split_heads(V)
        
        # 计算注意力
        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)
        
        # 合并多头
        attention_output = attention_output.transpose(0, 2, 1, 3)
        attention_output = attention_output.reshape(batch_size, -1, self.d_model)
        
        # 输出投影
        output = np.matmul(attention_output, self.W_o)
        
        return output, attention_weights

# 示例
batch_size, seq_len, d_model = 2, 10, 512
num_heads = 8

mha = MultiHeadAttention(d_model, num_heads)
X = np.random.randn(batch_size, seq_len, d_model)

output, weights = mha.forward(X, X, X)
print(f"注意力输出形状: {output.shape}")
print(f"注意力权重形状: {weights.shape}")
```

**自注意力公式**:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

其中：
+ $Q$ (Query): 查询矩阵
+ $K$ (Key): 键矩阵
+ $V$ (Value): 值矩阵
+ $d_k$: 键的维度

#### 5.3.2 位置编码

```python
class PositionalEncoding:
    """位置编码"""
    
    @staticmethod
    def get_positional_encoding(seq_len, d_model):
        """生成位置编码
        
        PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
        PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
        """
        pe = np.zeros((seq_len, d_model))
        
        position = np.arange(0, seq_len).reshape(-1, 1)
        div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))
        
        pe[:, 0::2] = np.sin(position * div_term)
        pe[:, 1::2] = np.cos(position * div_term)
        
        return pe
    
    @staticmethod
    def visualize_encoding(seq_len=100, d_model=128):
        """可视化位置编码"""
        pe = PositionalEncoding.get_positional_encoding(seq_len, d_model)
        
        print("位置编码特性:")
        print("="*60)
        print("1. 每个位置有唯一的编码")
        print("2. 相对位置关系可以通过内积计算")
        print(f"3. 编码形状: {pe.shape}")
        print(f"4. 位置0的编码前5维: {pe[0, :5]}")
        print(f"5. 位置1的编码前5维: {pe[1, :5]}")

PositionalEncoding.visualize_encoding()
```

### 5.4 文本分类实战

```python
class TextClassifier:
    """基于TF-IDF和逻辑回归的文本分类器"""
    
    def __init__(self):
        self.vectorizer = TFIDFVectorizer()
        self.weights = None
        self.bias = None
    
    def sigmoid(self, x):
        """Sigmoid激活函数"""
        return 1 / (1 + np.exp(-np.clip(x, -250, 250)))
    
    def fit(self, texts, labels, epochs=100, learning_rate=0.01):
        """训练分类器
        
        texts: 文本列表
        labels: 标签列表（0或1）
        """
        # 提取特征
        self.vectorizer.fit(texts)
        X = np.array(self.vectorizer.transform(texts))
        y = np.array(labels).reshape(-1, 1)
        
        # 初始化参数
        n_features = X.shape[1]
        self.weights = np.random.randn(n_features, 1) * 0.01
        self.bias = 0
        
        # 梯度下降
        for epoch in range(epochs):
            # 前向传播
            z = np.dot(X, self.weights) + self.bias
            predictions = self.sigmoid(z)
            
            # 计算损失
            loss = -np.mean(y * np.log(predictions + 1e-10) + 
                           (1 - y) * np.log(1 - predictions + 1e-10))
            
            # 反向传播
            dw = np.dot(X.T, (predictions - y)) / len(y)
            db = np.mean(predictions - y)
            
            # 更新参数
            self.weights -= learning_rate * dw
            self.bias -= learning_rate * db
            
            if epoch % 20 == 0:
                print(f"Epoch {epoch}, Loss: {loss:.4f}")
    
    def predict(self, texts):
        """预测"""
        X = np.array(self.vectorizer.transform(texts))
        z = np.dot(X, self.weights) + self.bias
        predictions = self.sigmoid(z)
        return (predictions > 0.5).astype(int).flatten()
    
    def predict_proba(self, texts):
        """预测概率"""
        X = np.array(self.vectorizer.transform(texts))
        z = np.dot(X, self.weights) + self.bias
        return self.sigmoid(z).flatten()

# 示例：情感分析
positive_texts = [
    "这部电影太棒了非常好看",
    "演员演技出色剧情精彩",
    "强烈推荐大家去看"
]

negative_texts = [
    "这部电影太差了浪费时间",
    "演技僵硬剧情无聊",
    "不推荐观看很失望"
]

# 准备数据
texts = positive_texts + negative_texts
labels = [1] * len(positive_texts) + [0] * len(negative_texts)

# 训练分类器
classifier = TextClassifier()
classifier.fit(texts, labels, epochs=100, learning_rate=0.1)

# 测试
test_texts = ["这部电影很精彩", "太无聊了"]
predictions = classifier.predict(test_texts)
probas = classifier.predict_proba(test_texts)

print("\n预测结果:")
for text, pred, prob in zip(test_texts, predictions, probas):
    sentiment = "正面" if pred == 1 else "负面"
    print(f"文本: {text}")
    print(f"情感: {sentiment}, 置信度: {prob:.2f}\n")
```

### 5.5 NLP任务总览

```python
class NLPTasksOverview:
    """NLP常见任务概览"""
    
    @staticmethod
    def summarize_tasks():
        """总结NLP任务"""
        
        tasks = [
            {
                'task': '文本分类',
                'description': '将文本分配到预定义类别',
                'examples': '情感分析、垃圾邮件检测',
                'methods': 'TF-IDF+LR, BERT'
            },
            {
                'task': '命名实体识别',
                'description': '识别文本中的实体（人名、地名等）',
                'examples': '\"张三在北京工作\" -> 张三(人名), 北京(地名)',
                'methods': 'CRF, BiLSTM-CRF, BERT'
            },
            {
                'task': '机器翻译',
                'description': '将文本从一种语言翻译成另一种',
                'examples': '英文 -> 中文',
                'methods': 'Seq2Seq, Transformer'
            },
            {
                'task': '文本生成',
                'description': '根据输入生成文本',
                'examples': '对话系统、文章续写',
                'methods': 'GPT, BART'
            },
            {
                'task': '问答系统',
                'description': '根据问题从文本中提取答案',
                'examples': 'SQuAD数据集',
                'methods': 'BERT, RoBERTa'
            },
            {
                'task': '文本摘要',
                'description': '生成文本的简短摘要',
                'examples': '新闻摘要',
                'methods': 'Extractive, Abstractive'
            }
        ]
        
        print("NLP常见任务")
        print("="*100)
        print(f"{'任务':<12} {'描述':<30} {'示例':<30} {'方法':<25}")
        print("-"*100)
        
        for task in tasks:
            print(f"{task['task']:<12} {task['description']:<30} "
                  f"{task['examples']:<30} {task['methods']:<25}")
        
        print("\n" + "="*100)
        print("NLP技术演进")
        print("="*100)
        
        evolution = [
            ("早期", "规则+统计", "TF-IDF, N-gram"),
            ("2010s", "深度学习", "Word2Vec, RNN, LSTM"),
            ("2017+", "Transformer时代", "BERT, GPT, T5"),
            ("2020+", "大模型", "GPT-3, ChatGPT, LLaMA")
        ]
        
        print(f"{'时期':<10} {'范式':<15} {'代表技术':<40}")
        print("-"*100)
        
        for period, paradigm, tech in evolution:
            print(f"{period:<10} {paradigm:<15} {tech:<40}")

NLPTasksOverview.summarize_tasks()
```

### 5.6 预训练语言模型

```python
class PretrainedModels:
    """预训练语言模型概述"""
    
    @staticmethod
    def compare_models():
        """对比主流预训练模型"""
        
        models = [
            {
                'name': 'BERT',
                'year': 2018,
                'architecture': 'Transformer Encoder',
                'pretraining': 'MLM + NSP',
                'params': '110M-340M',
                'use_case': '文本分类、NER、问答'
            },
            {
                'name': 'GPT-2',
                'year': 2019,
                'architecture': 'Transformer Decoder',
                'pretraining': 'Language Modeling',
                'params': '117M-1.5B',
                'use_case': '文本生成、对话'
            },
            {
                'name': 'RoBERTa',
                'year': 2019,
                'architecture': 'Transformer Encoder',
                'pretraining': 'MLM (优化)',
                'params': '125M-355M',
                'use_case': 'BERT改进版'
            },
            {
                'name': 'T5',
                'year': 2020,
                'architecture': 'Encoder-Decoder',
                'pretraining': 'Text-to-Text',
                'params': '60M-11B',
                'use_case': '统一框架（所有任务）'
            },
            {
                'name': 'GPT-3',
                'year': 2020,
                'architecture': 'Transformer Decoder',
                'pretraining': 'Language Modeling',
                'params': '175B',
                'use_case': 'Few-shot学习'
            }
        ]
        
        print("预训练语言模型对比")
        print("="*120)
        print(f"{'模型':<10} {'年份':<6} {'架构':<20} {'预训练任务':<20} {'参数量':<15} {'应用场景':<30}")
        print("-"*120)
        
        for model in models:
            print(f"{model['name']:<10} {model['year']:<6} {model['architecture']:<20} "
                  f"{model['pretraining']:<20} {model['params']:<15} {model['use_case']:<30}")
        
        print("\n" + "="*120)
        print("关键概念")
        print("="*120)
        
        concepts = {
            'MLM (Masked Language Model)': '随机mask部分词，让模型预测',
            'NSP (Next Sentence Prediction)': '预测两句话是否连续',
            'Fine-tuning': '在下游任务上微调预训练模型',
            'Few-shot Learning': '少样本学习，仅需少量示例',
            'Zero-shot Learning': '零样本学习，无需示例直接推理'
        }
        
        for concept, desc in concepts.items():
            print(f"\n【{concept}】")
            print(f"  {desc}")

PretrainedModels.compare_models()
```

---

**本章完**
