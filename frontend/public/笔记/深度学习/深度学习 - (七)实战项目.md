# 深度学习 - (七)实战项目

深度学习实战应用。

---

## 7. 实战项目

### 7.1 手写数字识别（CNN）

```python
"""
项目：MNIST手写数字识别
模型：LeNet-5 CNN
数据集：28x28灰度图像，10个类别（0-9）
"""

class MNISTClassifier:
    def __init__(self):
        # 卷积层
        self.conv1 = Conv2D(filters=6, kernel_size=5)
        self.pool1 = MaxPool2D(pool_size=2)
        self.conv2 = Conv2D(filters=16, kernel_size=5)
        self.pool2 = MaxPool2D(pool_size=2)
        
        # 全连接层
        self.fc1_w = np.random.randn(16 * 4 * 4, 120) * 0.01
        self.fc1_b = np.zeros((1, 120))
        
        self.fc2_w = np.random.randn(120, 84) * 0.01
        self.fc2_b = np.zeros((1, 84))
        
        self.fc3_w = np.random.randn(84, 10) * 0.01
        self.fc3_b = np.zeros((1, 10))
    
    def relu(self, x):
        return np.maximum(0, x)
    
    def softmax(self, x):
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)
    
    def forward(self, x):
        """前向传播"""
        # 卷积层1
        x = self.conv1.forward(x)
        x = self.relu(x)
        x = self.pool1.forward(x)
        
        # 卷积层2
        x = self.conv2.forward(x)
        x = self.relu(x)
        x = self.pool2.forward(x)
        
        # 展平
        batch_size = x.shape[0]
        x = x.reshape(batch_size, -1)
        
        # 全连接层1
        x = np.dot(x, self.fc1_w) + self.fc1_b
        x = self.relu(x)
        
        # 全连接层2
        x = np.dot(x, self.fc2_w) + self.fc2_b
        x = self.relu(x)
        
        # 输出层
        x = np.dot(x, self.fc3_w) + self.fc3_b
        x = self.softmax(x)
        
        return x
    
    def predict(self, x):
        """预测"""
        probs = self.forward(x)
        return np.argmax(probs, axis=1)

# 模拟MNIST数据
X_train = np.random.randn(100, 1, 28, 28)  # 100张28x28图像
y_train = np.random.randint(0, 10, 100)    # 标签0-9

# 训练模型（简化版）
model = MNISTClassifier()
predictions = model.predict(X_train[:5])

print("=== MNIST分类器 ===")
print(f"预测结果: {predictions}")
print(f"真实标签: {y_train[:5]}")
```

### 7.2 情感分析（RNN）

```python
"""
项目：电影评论情感分析
模型：LSTM
任务：二分类（正面/负面）
"""

class SentimentAnalyzer:
    def __init__(self, vocab_size, embedding_dim=128, hidden_size=256):
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.hidden_size = hidden_size
        
        # 词嵌入层
        self.embedding = np.random.randn(vocab_size, embedding_dim) * 0.01
        
        # LSTM
        self.lstm = LSTM(embedding_dim, hidden_size, hidden_size)
        
        # 输出层
        self.fc_w = np.random.randn(hidden_size, 2) * 0.01
        self.fc_b = np.zeros((1, 2))
    
    def embed(self, word_indices):
        """词嵌入"""
        return [self.embedding[idx] for idx in word_indices]
    
    def forward(self, word_indices):
        """前向传播"""
        # 词嵌入
        embedded = self.embed(word_indices)
        
        # LSTM
        outputs = self.lstm.forward(embedded)
        
        # 取最后一个时间步的输出
        last_output = outputs[-1]
        
        # 全连接层
        logits = np.dot(last_output, self.fc_w) + self.fc_b
        
        # Softmax
        exp_logits = np.exp(logits - np.max(logits))
        probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)
        
        return probs
    
    def predict(self, word_indices):
        """预测情感"""
        probs = self.forward(word_indices)
        return np.argmax(probs, axis=1)[0]

# 模拟数据
vocab_size = 10000
review = [45, 123, 456, 789, 234, 567, 890]  # 单词索引序列

analyzer = SentimentAnalyzer(vocab_size)
sentiment = analyzer.predict(review)

print("=== 情感分析 ===")
print(f"评论: {review}")
print(f"预测情感: {'正面' if sentiment == 1 else '负面'}")
```

### 7.3 图像生成（VAE）

```python
"""
项目：变分自编码器（VAE）
任务：图像生成
"""

class VAE:
    def __init__(self, input_dim, latent_dim):
        self.input_dim = input_dim
        self.latent_dim = latent_dim
        
        # 编码器
        self.enc_w1 = np.random.randn(input_dim, 512) * 0.01
        self.enc_b1 = np.zeros((1, 512))
        
        # 均值和方差
        self.mu_w = np.random.randn(512, latent_dim) * 0.01
        self.mu_b = np.zeros((1, latent_dim))
        
        self.logvar_w = np.random.randn(512, latent_dim) * 0.01
        self.logvar_b = np.zeros((1, latent_dim))
        
        # 解码器
        self.dec_w1 = np.random.randn(latent_dim, 512) * 0.01
        self.dec_b1 = np.zeros((1, 512))
        
        self.dec_w2 = np.random.randn(512, input_dim) * 0.01
        self.dec_b2 = np.zeros((1, input_dim))
    
    def relu(self, x):
        return np.maximum(0, x)
    
    def sigmoid(self, x):
        return 1 / (1 + np.exp(-np.clip(x, -250, 250)))
    
    def reparameterize(self, mu, logvar):
        """重参数化技巧"""
        std = np.exp(0.5 * logvar)
        eps = np.random.randn(*mu.shape)
        return mu + eps * std
    
    def encode(self, x):
        """编码"""
        h = np.dot(x, self.enc_w1) + self.enc_b1
        h = self.relu(h)
        
        mu = np.dot(h, self.mu_w) + self.mu_b
        logvar = np.dot(h, self.logvar_w) + self.logvar_b
        
        return mu, logvar
    
    def decode(self, z):
        """解码"""
        h = np.dot(z, self.dec_w1) + self.dec_b1
        h = self.relu(h)
        
        recon = np.dot(h, self.dec_w2) + self.dec_b2
        recon = self.sigmoid(recon)
        
        return recon
    
    def forward(self, x):
        """前向传播"""
        # 编码
        mu, logvar = self.encode(x)
        
        # 采样
        z = self.reparameterize(mu, logvar)
        
        # 解码
        recon = self.decode(z)
        
        return recon, mu, logvar
    
    def generate(self, n_samples):
        """生成新样本"""
        z = np.random.randn(n_samples, self.latent_dim)
        return self.decode(z)
    
    def loss(self, x, recon, mu, logvar):
        """VAE损失函数"""
        # 重构损失（二元交叉熵）
        recon_loss = -np.sum(x * np.log(recon + 1e-8) + (1 - x) * np.log(1 - recon + 1e-8))
        
        # KL散度
        kl_loss = -0.5 * np.sum(1 + logvar - mu ** 2 - np.exp(logvar))
        
        return recon_loss + kl_loss

# 示例
vae = VAE(input_dim=784, latent_dim=20)  # 28x28图像展平

# 训练数据
x = np.random.rand(32, 784)  # batch_size=32

# 前向传播
recon, mu, logvar = vae.forward(x)
loss = vae.loss(x, recon, mu, logvar)

print("=== VAE图像生成 ===")
print(f"重构误差: {loss:.2f}")

# 生成新图像
generated = vae.generate(5)
print(f"生成图像形状: {generated.shape}")
```

### 7.4 机器翻译（Seq2Seq）

```python
"""
项目：序列到序列翻译
模型：Encoder-Decoder + Attention
任务：英译中
"""

class Encoder:
    """编码器（LSTM）"""
    def __init__(self, vocab_size, embed_dim, hidden_dim):
        self.embedding = np.random.randn(vocab_size, embed_dim) * 0.01
        self.lstm = LSTM(embed_dim, hidden_dim, hidden_dim)
    
    def forward(self, input_seq):
        """编码"""
        # 词嵌入
        embedded = [self.embedding[idx] for idx in input_seq]
        
        # LSTM编码
        outputs = self.lstm.forward(embedded)
        
        return outputs

class AttentionDecoder:
    """解码器（带注意力机制）"""
    def __init__(self, vocab_size, embed_dim, hidden_dim):
        self.vocab_size = vocab_size
        self.embedding = np.random.randn(vocab_size, embed_dim) * 0.01
        self.lstm = LSTM(embed_dim + hidden_dim, hidden_dim, hidden_dim)
        
        # 注意力权重
        self.attn_w = np.random.randn(hidden_dim * 2, hidden_dim) * 0.01
        self.attn_v = np.random.randn(hidden_dim, 1) * 0.01
        
        # 输出层
        self.out_w = np.random.randn(hidden_dim, vocab_size) * 0.01
        self.out_b = np.zeros((1, vocab_size))
    
    def attention(self, decoder_hidden, encoder_outputs):
        """计算注意力分数"""
        scores = []
        
        for enc_out in encoder_outputs:
            # 拼接
            combined = np.hstack([decoder_hidden, enc_out])
            
            # 计算得分
            energy = np.tanh(np.dot(combined, self.attn_w))
            score = np.dot(energy, self.attn_v)
            scores.append(score[0, 0])
        
        # Softmax
        scores = np.array(scores)
        exp_scores = np.exp(scores - np.max(scores))
        attn_weights = exp_scores / np.sum(exp_scores)
        
        # 加权求和
        context = np.zeros_like(encoder_outputs[0])
        for i, weight in enumerate(attn_weights):
            context += weight * encoder_outputs[i]
        
        return context, attn_weights
    
    def forward(self, target_seq, encoder_outputs):
        """解码"""
        outputs = []
        hidden = np.zeros((1, self.lstm.hidden_size))
        
        for t in range(len(target_seq)):
            # 注意力
            context, attn_weights = self.attention(hidden, encoder_outputs)
            
            # 词嵌入
            embedded = self.embedding[target_seq[t]].reshape(1, -1)
            
            # 拼接上下文
            lstm_input = [np.hstack([embedded, context])]
            
            # LSTM解码
            lstm_out = self.lstm.forward(lstm_input)
            hidden = lstm_out[-1]
            
            # 输出预测
            logits = np.dot(hidden, self.out_w) + self.out_b
            outputs.append(logits)
        
        return outputs

class Seq2Seq:
    """序列到序列模型"""
    def __init__(self, src_vocab_size, tgt_vocab_size, embed_dim=256, hidden_dim=512):
        self.encoder = Encoder(src_vocab_size, embed_dim, hidden_dim)
        self.decoder = AttentionDecoder(tgt_vocab_size, embed_dim, hidden_dim)
    
    def translate(self, src_seq, max_len=50):
        """翻译"""
        # 编码
        encoder_outputs = self.encoder.forward(src_seq)
        
        # 解码（贪心搜索）
        translated = [1]  # <START> token
        
        for _ in range(max_len):
            outputs = self.decoder.forward(translated, encoder_outputs)
            last_output = outputs[-1]
            
            # 取概率最大的词
            next_word = np.argmax(last_output)
            translated.append(next_word)
            
            if next_word == 2:  # <END> token
                break
        
        return translated

# 示例
src_vocab_size = 10000  # 英文词汇表
tgt_vocab_size = 8000   # 中文词汇表

seq2seq = Seq2Seq(src_vocab_size, tgt_vocab_size)

# 翻译句子
src_sentence = [45, 123, 456, 789]  # "Hello world"
translation = seq2seq.translate(src_sentence)

print("=== 机器翻译 ===")
print(f"源句子（词索引）: {src_sentence}")
print(f"翻译结果（词索引）: {translation}")
```

---

**本章完**
