# 概率论与数理统计 - (一)概率论基础

学习概率的基本概念和公理。

---

## 1. 概率论基础

### 1.1 样本空间与事件

#### 基本概念

**随机试验（Random Experiment）**：满足以下条件的试验
1. 可重复进行
2. 结果具有多种可能性
3. 试验前无法预知确切结果

**样本空间** $\Omega$（Sample Space）：所有可能结果的集合

**样本点** $\omega$（Sample Point）：样本空间中的每个元素

**随机事件** $A$（Event）：样本空间的子集

**示例：**

**例1：抛硬币**
- 样本空间：$\Omega = \{正面, 反面\}$ 或 $\Omega = \{H, T\}$
- 事件 $A$：出现正面，即 $A = \{H\}$

**例2：掷骰子**
- 样本空间：$\Omega = \{1, 2, 3, 4, 5, 6\}$
- 事件 $A$：出现偶数，即 $A = \{2, 4, 6\}$

**例3：灯泡寿命（连续）**
- 样本空间：$\Omega = [0, +\infty)$
- 事件 $A$：寿命超过1000小时，即 $A = (1000, +\infty)$

#### 事件的运算

**事件的关系：**

| 关系 | 符号 | 含义 |
|------|------|------|
| 包含 | $A \subset B$ | $A$ 发生必然导致 $B$ 发生 |
| 相等 | $A = B$ | $A \subset B$ 且 $B \subset A$ |
| 互斥 | $A \cap B = \emptyset$ | $A$ 和 $B$ 不能同时发生 |
| 对立 | $\overline{A}$ 或 $A^c$ | $A$ 不发生 |

**事件的运算：**

| 运算 | 符号 | 含义 |
|------|------|------|
| 并（和） | $A \cup B$ | $A$ 或 $B$ 至少有一个发生 |
| 交（积） | $A \cap B$ 或 $AB$ | $A$ 和 $B$ 同时发生 |
| 差 | $A - B$ | $A$ 发生但 $B$ 不发生 |
| 补（对立） | $\overline{A}$ | $A$ 不发生 |

**运算律：**

$$
A \cup B = B \cup A \quad （交换律）
$$

$$
(A \cup B) \cup C = A \cup (B \cup C) \quad （结合律）
$$

$$
A \cap (B \cup C) = (A \cap B) \cup (A \cap C) \quad （分配律）
$$

**德摩根定律（De Morgan's Laws）：**

$$
\overline{A \cup B} = \overline{A} \cap \overline{B}
$$

$$
\overline{A \cap B} = \overline{A} \cup \overline{B}
$$

推广：

$$
\overline{\bigcup_{i=1}^{n} A_i} = \bigcap_{i=1}^{n} \overline{A_i}
$$

$$
\overline{\bigcap_{i=1}^{n} A_i} = \bigcup_{i=1}^{n} \overline{A_i}
$$

### 1.2 概率的定义

#### 古典概型（Classical Probability）

**定义**：若试验满足
1. 样本空间有限：$\Omega = \{\omega_1, \omega_2, \ldots, \omega_n\}$
2. 每个样本点等可能

则事件 $A$ 的概率为：

$$
P(A) = \frac{A \text{包含的样本点数}}{\Omega \text{中样本点总数}} = \frac{|A|}{|\Omega|}
$$

**示例：**

**例4：从52张扑克牌中抽1张，求抽到红心的概率**

**解**：

$$
P(\text{红心}) = \frac{13}{52} = \frac{1}{4}
$$

**例5：从10个产品中（其中2个次品）随机抽3个，求恰有1个次品的概率**

**解**：

样本空间大小：$|\Omega| = C_{10}^3 = 120$

恰有1个次品：从2个次品中选1个，从8个正品中选2个

$$
|A| = C_2^1 \cdot C_8^2 = 2 \times 28 = 56
$$

$$
P(A) = \frac{56}{120} = \frac{7}{15}
$$

#### 几何概型（Geometric Probability）

当样本空间是某个区域，事件的概率与区域的"度量"（长度、面积、体积）成正比。

$$
P(A) = \frac{A \text{的度量}}{\Omega \text{的度量}}
$$

**示例：**

**例6：会面问题**

甲、乙两人约定在12:00-13:00之间在某地会面，先到者等20分钟后离开。设两人到达时间相互独立且均匀分布，求两人能会面的概率。

**解**：

设甲、乙到达时刻分别为 $x, y$（以12:00为原点，单位：分钟）

样本空间：$\Omega = \{(x, y) : 0 \leq x \leq 60, 0 \leq y \leq 60\}$，面积 $= 60^2 = 3600$

会面条件：$|x - y| \leq 20$

$$
A = \{(x, y) : |x - y| \leq 20\}
$$

![几何概型示意](不需要图片)

计算 $A$ 的面积：

$$
S_A = 60^2 - 2 \times \frac{1}{2} \times 40^2 = 3600 - 1600 = 2000
$$

$$
P(A) = \frac{2000}{3600} = \frac{5}{9}
$$

#### 概率的公理化定义（Kolmogorov公理）

**定义**：设 $\Omega$ 为样本空间，$P$ 为定义在事件域上的实值函数，若满足：

**公理1（非负性）**：对任意事件 $A$，$P(A) \geq 0$

**公理2（规范性）**：$P(\Omega) = 1$

**公理3（可列可加性）**：若 $A_1, A_2, \ldots$ 两两互斥，则

$$
P\left(\bigcup_{i=1}^{\infty} A_i\right) = \sum_{i=1}^{\infty} P(A_i)
$$

则称 $P(A)$ 为事件 $A$ 的概率。

### 1.3 概率的性质

#### 基本性质

**性质1**：$P(\emptyset) = 0$（不可能事件概率为0）

**性质2**：$P(\overline{A}) = 1 - P(A)$（对立事件概率）

**性质3**：若 $A \subset B$，则 $P(A) \leq P(B)$（单调性）

**性质4**：对任意事件 $A$，$0 \leq P(A) \leq 1$（有界性）

**性质5**（加法公式）：

$$
P(A \cup B) = P(A) + P(B) - P(A \cap B)
$$

推广（容斥原理）：

$$
\begin{aligned}
P(A \cup B \cup C) = &P(A) + P(B) + P(C) \\
&- P(AB) - P(AC) - P(BC) \\
&+ P(ABC)
\end{aligned}
$$

一般地：

$$
P\left(\bigcup_{i=1}^{n} A_i\right) = \sum_{i} P(A_i) - \sum_{i<j} P(A_i A_j) + \sum_{i<j<k} P(A_i A_j A_k) - \cdots + (-1)^{n-1} P(A_1 A_2 \cdots A_n)
$$

**示例：**

**例7**：某班级有60%的学生会Python，50%会Java，30%两者都会。随机选一名学生，求他至少会一门语言的概率。

**解**：

设 $A$ = {会Python}，$B$ = {会Java}

$$
P(A) = 0.6, \quad P(B) = 0.5, \quad P(AB) = 0.3
$$

$$
P(A \cup B) = P(A) + P(B) - P(AB) = 0.6 + 0.5 - 0.3 = 0.8
$$

### 1.4 条件概率

#### 定义

**条件概率**：在事件 $B$ 发生的条件下，事件 $A$ 发生的概率，记作 $P(A|B)$

$$
P(A|B) = \frac{P(A \cap B)}{P(B)}, \quad P(B) > 0
$$

**直观理解**：已知 $B$ 发生后，样本空间从 $\Omega$ 缩小到 $B$，$A$ 发生相当于 $AB$ 发生。

**示例：**

**例8**：掷两颗骰子，已知点数之和为7，求第一颗骰子为3的概率。

**解**：

设 $A$ = {第一颗为3}，$B$ = {和为7}

$$
B = \{(1,6), (2,5), (3,4), (4,3), (5,2), (6,1)\}, \quad P(B) = \frac{6}{36} = \frac{1}{6}
$$

$$
AB = \{(3,4)\}, \quad P(AB) = \frac{1}{36}
$$

$$
P(A|B) = \frac{P(AB)}{P(B)} = \frac{1/36}{1/6} = \frac{1}{6}
$$

#### 条件概率的性质

条件概率 $P(\cdot | B)$ 满足概率的所有公理和性质：

1. $P(A|B) \geq 0$
2. $P(\Omega|B) = 1$
3. $P(A_1 \cup A_2 | B) = P(A_1|B) + P(A_2|B)$（$A_1, A_2$ 互斥）

**乘法公式：**

$$
P(AB) = P(B) \cdot P(A|B) = P(A) \cdot P(B|A)
$$

推广：

$$
P(A_1 A_2 \cdots A_n) = P(A_1) P(A_2|A_1) P(A_3|A_1 A_2) \cdots P(A_n|A_1 A_2 \cdots A_{n-1})
$$

**例9**：从52张牌中依次不放回抽3张，求都是红心的概率。

**解**：

设 $A_i$ = {第 $i$ 张是红心}

$$
\begin{aligned}
P(A_1 A_2 A_3) &= P(A_1) P(A_2|A_1) P(A_3|A_1 A_2) \\
&= \frac{13}{52} \times \frac{12}{51} \times \frac{11}{50} \\
&= \frac{1}{4} \times \frac{12}{51} \times \frac{11}{50} \\
&\approx 0.0129
\end{aligned}
$$

### 1.5 全概率公式与贝叶斯定理

#### 完备事件组

**定义**：若事件组 $A_1, A_2, \ldots, A_n$ 满足：
1. 两两互斥：$A_i \cap A_j = \emptyset$（$i \neq j$）
2. 完备：$\bigcup_{i=1}^{n} A_i = \Omega$

则称 $\{A_1, A_2, \ldots, A_n\}$ 为样本空间的一个**划分**或**完备事件组**。

#### 全概率公式（Law of Total Probability）

**定理**：若 $\{A_1, A_2, \ldots, A_n\}$ 是完备事件组，且 $P(A_i) > 0$，则对任意事件 $B$：

$$
P(B) = \sum_{i=1}^{n} P(B|A_i) P(A_i)
$$

**直观理解**："分情况讨论"的数学表达。将复杂事件 $B$ 分解为在各种情况 $A_i$ 下的条件概率。

**应用场景**：

1. **由"原因"推"结果"**：已知各种原因（$A_i$）发生的概率，以及在各原因下结果（$B$）发生的概率，求结果的总概率。

**例10**：某工厂有3条生产线，产量分别占总产量的30%、45%、25%，次品率分别为1%、2%、1.5%。随机抽取一件产品，求它是次品的概率。

**解**：

设 $A_1, A_2, A_3$ 分别表示产品来自生产线1、2、3，$B$ = {次品}

$$
P(A_1) = 0.30, \quad P(A_2) = 0.45, \quad P(A_3) = 0.25
$$

$$
P(B|A_1) = 0.01, \quad P(B|A_2) = 0.02, \quad P(B|A_3) = 0.015
$$

由全概率公式：

$$
\begin{aligned}
P(B) &= P(B|A_1)P(A_1) + P(B|A_2)P(A_2) + P(B|A_3)P(A_3) \\
&= 0.01 \times 0.30 + 0.02 \times 0.45 + 0.015 \times 0.25 \\
&= 0.003 + 0.009 + 0.00375 \\
&= 0.01575
\end{aligned}
$$

答：次品概率为1.575%。

#### 贝叶斯定理（Bayes' Theorem）

**定理**：若 $\{A_1, A_2, \ldots, A_n\}$ 是完备事件组，$P(A_i) > 0$，$P(B) > 0$，则：

$$
P(A_i|B) = \frac{P(B|A_i) P(A_i)}{P(B)} = \frac{P(B|A_i) P(A_i)}{\sum_{j=1}^{n} P(B|A_j) P(A_j)}
$$

**直观理解**："由果溯因"。已知结果 $B$ 发生，反推各原因 $A_i$ 的概率。

**术语**：
- $P(A_i)$：**先验概率**（Prior Probability），观察前对原因的估计
- $P(A_i|B)$：**后验概率**（Posterior Probability），观察到结果后更新的概率
- $P(B|A_i)$：**似然**（Likelihood）

**贝叶斯公式的意义**：

$$
\text{后验概率} = \frac{\text{似然} \times \text{先验概率}}{\text{边际概率}}
$$

**例11**（接例10）：若抽到次品，求它来自生产线2的概率。

**解**：

$$
\begin{aligned}
P(A_2|B) &= \frac{P(B|A_2) P(A_2)}{P(B)} \\
&= \frac{0.02 \times 0.45}{0.01575} \\
&= \frac{0.009}{0.01575} \\
&\approx 0.571
\end{aligned}
$$

答：约57.1%。

**例12：医学诊断**

某疾病患病率为0.1%（$P(D) = 0.001$）。检测的敏感性（真阳性率）为99%（$P(+|D) = 0.99$），特异性（真阴性率）为95%（$P(-|\overline{D}) = 0.95$）。

若检测结果为阳性，求实际患病的概率 $P(D|+)$。

**解**：

已知：
- $P(D) = 0.001$，$P(\overline{D}) = 0.999$
- $P(+|D) = 0.99$（敏感性）
- $P(+|\overline{D}) = 1 - 0.95 = 0.05$（假阳性率）

由贝叶斯公式：

$$
\begin{aligned}
P(D|+) &= \frac{P(+|D) P(D)}{P(+|D) P(D) + P(+|\overline{D}) P(\overline{D})} \\
&= \frac{0.99 \times 0.001}{0.99 \times 0.001 + 0.05 \times 0.999} \\
&= \frac{0.00099}{0.00099 + 0.04995} \\
&= \frac{0.00099}{0.05094} \\
&\approx 0.0194
\end{aligned}
$$

答：即使检测阳性，实际患病概率仅约1.94%。

**结论**：当疾病患病率极低时，即使检测准确度很高，阳性结果的阳性预测值（PPV）仍然很低。这是**基率谬误**的经典例子。

### 1.6 事件的独立性

#### 独立性定义

**定义**：若事件 $A$ 和 $B$ 满足

$$
P(AB) = P(A) \cdot P(B)
$$

则称 $A$ 和 $B$ **相互独立**（Independent）。

**等价定义**：

如果 $P(B) > 0$，则 $A$ 和 $B$ 独立等价于：

$$
P(A|B) = P(A)
$$

即"$B$ 的发生不影响 $A$ 发生的概率"。

#### 独立性的性质

**性质1**：若 $A$ 和 $B$ 独立，则：
- $A$ 和 $\overline{B}$ 独立
- $\overline{A}$ 和 $B$ 独立
- $\overline{A}$ 和 $\overline{B}$ 独立

**性质2**（三事件独立）：$A, B, C$ 相互独立，当且仅当：

$$
\begin{cases}
P(AB) = P(A)P(B) \\
P(AC) = P(A)P(C) \\
P(BC) = P(B)P(C) \\
P(ABC) = P(A)P(B)P(C)
\end{cases}
$$

**注意**：前三式成立时称**两两独立**，只有四式都成立才是**相互独立**。

**例13**：掷两颗骰子，设
- $A$ = {第一颗为偶数}
- $B$ = {第二颗为偶数}
- $C$ = {两颗和为偶数}

证明 $A, B, C$ 两两独立但不相互独立。

**证明**：

$$
P(A) = P(B) = P(C) = \frac{1}{2}
$$

$$
P(AB) = \frac{1}{4} = P(A)P(B) \checkmark
$$

$$
P(AC) = \frac{1}{4} = P(A)P(C) \checkmark
$$

$$
P(BC) = \frac{1}{4} = P(B)P(C) \checkmark
$$

但：

$$
P(ABC) = \frac{1}{4} \neq P(A)P(B)P(C) = \frac{1}{8}
$$

所以两两独立但不相互独立。

#### 独立性的应用

**例14：系统可靠性**

某系统由3个独立工作的部件组成，各部件正常工作的概率分别为0.9, 0.8, 0.85。

(1) 若并联（任一部件工作系统就工作），求系统可靠性
(2) 若串联（所有部件都工作系统才工作），求系统可靠性

**解**：

设 $A_i$ = {第 $i$ 个部件正常}，$i = 1, 2, 3$

**(1) 并联：**

系统失效当且仅当所有部件都失效：

$$
P(\text{系统工作}) = 1 - P(\overline{A_1} \overline{A_2} \overline{A_3})
$$

由独立性：

$$
P(\overline{A_1} \overline{A_2} \overline{A_3}) = P(\overline{A_1}) P(\overline{A_2}) P(\overline{A_3}) = 0.1 \times 0.2 \times 0.15 = 0.003
$$

$$
P(\text{系统工作}) = 1 - 0.003 = 0.997
$$

**(2) 串联：**

系统工作当且仅当所有部件都工作：

$$
P(\text{系统工作}) = P(A_1 A_2 A_3) = P(A_1) P(A_2) P(A_3) = 0.9 \times 0.8 \times 0.85 = 0.612
$$

### 1.7 应用实例

#### 应用1：朴素贝叶斯垃圾邮件过滤

基于贝叶斯定理和条件独立性假设的文本分类算法。

```python
import numpy as np

class NaiveBayesSpamFilter:
    def __init__(self):
        self.word_prob_spam = {}
        self.word_prob_ham = {}
        self.p_spam = 0.5
        self.p_ham = 0.5
    
    def train(self, emails, labels):
        """
        训练贝叶斯分类器
        emails: 邮件列表（每个邮件是单词列表）
        labels: 标签列表（1=spam, 0=ham）
        """
        spam_emails = [emails[i] for i in range(len(emails)) if labels[i] == 1]
        ham_emails = [emails[i] for i in range(len(emails)) if labels[i] == 0]
        
        self.p_spam = len(spam_emails) / len(emails)
        self.p_ham = len(ham_emails) / len(emails)
        
        # 统计词频
        spam_words = [word for email in spam_emails for word in email]
        ham_words = [word for email in ham_emails for word in email]
        
        vocab = set(spam_words + ham_words)
        
        # 使用拉普拉斯平滑
        for word in vocab:
            self.word_prob_spam[word] = (spam_words.count(word) + 1) / (len(spam_words) + len(vocab))
            self.word_prob_ham[word] = (ham_words.count(word) + 1) / (len(ham_words) + len(vocab))
    
    def predict(self, email):
        """
        预测邮件是否为垃圾邮件
        使用贝叶斯公式：P(spam|email) = P(email|spam) * P(spam) / P(email)
        """
        # 使用对数避免下溢
        log_p_spam = np.log(self.p_spam)
        log_p_ham = np.log(self.p_ham)
        
        for word in email:
            if word in self.word_prob_spam:
                log_p_spam += np.log(self.word_prob_spam[word])
                log_p_ham += np.log(self.word_prob_ham[word])
        
        return 1 if log_p_spam > log_p_ham else 0

# 示例使用
emails = [
    ['free', 'money', 'now'],
    ['meeting', 'schedule', 'tomorrow'],
    ['win', 'prize', 'free'],
    ['project', 'deadline', 'update']
]
labels = [1, 0, 1, 0]  # 1=spam, 0=ham

classifier = NaiveBayesSpamFilter()
classifier.train(emails, labels)

test_email = ['free', 'prize']
result = classifier.predict(test_email)
print(f"预测: {'垃圾邮件' if result == 1 else '正常邮件'}")
```

#### 应用2：随机算法期望分析

利用独立性分析快速排序的平均时间复杂度。

```python
def random_quicksort_analysis():
    """
    快速排序的期望复杂度分析
    基于独立性假设：每次选择的pivot是随机的
    """
    import random
    
    def quicksort_comparisons(arr, count=[0]):
        if len(arr) <= 1:
            return arr
        
        pivot = arr[0]
        count[0] += len(arr) - 1  # 与pivot比较次数
        
        left = [x for x in arr[1:] if x <= pivot]
        right = [x for x in arr[1:] if x > pivot]
        
        return quicksort_comparisons(left, count) + [pivot] + quicksort_comparisons(right, count)
    
    n = 100
    trials = 1000
    total_comparisons = 0
    
    for _ in range(trials):
        arr = list(range(n))
        random.shuffle(arr)
        count = [0]
        quicksort_comparisons(arr, count)
        total_comparisons += count[0]
    
    avg_comparisons = total_comparisons / trials
    theoretical = n * np.log(n) * 1.386  # ≈ 2n ln n
    
    print(f"样本数: {n}")
    print(f"平均比较次数（模拟）: {avg_comparisons:.0f}")
    print(f"理论值 (2n ln n): {theoretical:.0f}")
    print(f"误差: {abs(avg_comparisons - theoretical):.0f}")

# random_quicksort_analysis()  # 取消注释运行
```

### 1.8 本章小结

#### 核心概念

1. **样本空间与事件**：随机试验、样本空间、事件及其运算
2. **概率定义**：古典概型、几何概型、公理化定义
3. **概率性质**：加法公式、容斥原理
4. **条件概率**：定义、乘法公式
5. **全概率公式**：由"原因"推"结果"
6. **贝叶斯定理**：由"果"溯"因"，更新先验概率
7. **事件独立性**：$P(AB) = P(A)P(B)$

#### 重要公式速查

| 公式 | 名称 | 应用 |
|------|------|------|
| $P(A \cup B) = P(A) + P(B) - P(AB)$ | 加法公式 | 计算并事件概率 |
| $P(A|B) = \frac{P(AB)}{P(B)}$ | 条件概率 | 已知信息下的概率 |
| $P(AB) = P(A)P(B|A)$ | 乘法公式 | 计算交事件概率 |
| $P(B) = \sum P(B|A_i)P(A_i)$ | 全概率公式 | 分情况讨论 |
| $P(A_i|B) = \frac{P(B|A_i)P(A_i)}{\sum P(B|A_j)P(A_j)}$ | 贝叶斯公式 | 反向推理 |
| $P(AB) = P(A)P(B)$ | 独立性 | 简化计算 |

#### 常见题型

**类型1**：古典概型
- 方法：数清楚分子分母

**类型2**：全概率公式
- 识别：求某事件的总概率，可分多种情况
- 方法：找完备事件组，分情况计算

**类型3**：贝叶斯公式
- 识别：已知结果反推原因
- 方法：写出先验概率和似然，应用贝叶斯公式

**类型4**：独立性
- 识别：多个事件"互不影响"
- 方法：利用 $P(A_1 A_2 \cdots A_n) = P(A_1)P(A_2) \cdots P(A_n)$

---

**本章完**
