# 概率论与数理统计 - (七)回归分析

深入学习线性回归理论、诊断方法、模型评估与选择。

---

## 7. 回归分析

### 7.1 简单线性回归

#### 模型假设

**经典线性回归模型**（Classical Linear Regression Model, CLRM）：

$$
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i, \quad i = 1, 2, \ldots, n
$$

**基本假设**：
1. **线性性**：$E[Y|X] = \beta_0 + \beta_1 X$（真实关系是线性的）
2. **独立性**：$\epsilon_i$ 相互独立
3. **同方差性**：$\text{Var}(\epsilon_i) = \sigma^2$（常数方差）
4. **正态性**：$\epsilon_i \sim N(0, \sigma^2)$（用于推断）
5. **无测量误差**：$X$ 是确定性变量或测量无误差

#### 最小二乘法（OLS）

**目标**：最小化残差平方和（RSS）

$$
\min_{\beta_0, \beta_1} \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i)^2
$$

**正规方程**（Normal Equations）：

$$
\frac{\partial \text{RSS}}{\partial \beta_0} = -2\sum(y_i - \beta_0 - \beta_1 x_i) = 0
$$

$$
\frac{\partial \text{RSS}}{\partial \beta_1} = -2\sum x_i(y_i - \beta_0 - \beta_1 x_i) = 0
$$

**解**：

$$
\hat{\beta}_1 = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2} = \frac{S_{xy}}{S_{xx}}
$$

$$
\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}
$$

**重要性质**：
1. **无偏性**：$E[\hat{\beta}_0] = \beta_0$，$E[\hat{\beta}_1] = \beta_1$
2. **方差**：
   $$
   \text{Var}(\hat{\beta}_1) = \frac{\sigma^2}{S_{xx}}, \quad \text{Var}(\hat{\beta}_0) = \sigma^2 \left( \frac{1}{n} + \frac{\bar{x}^2}{S_{xx}} \right)
   $$
3. **Gauss-Markov定理**：在所有线性无偏估计中，OLS方差最小（BLUE）

#### 拟合优度（Goodness-of-Fit）

**总平方和分解**（ANOVA分解）：

$$
\underbrace{\sum (y_i - \bar{y})^2}_{\text{SST (Total)}} = \underbrace{\sum (\hat{y}_i - \bar{y})^2}_{\text{SSR (Regression)}} + \underbrace{\sum (y_i - \hat{y}_i)^2}_{\text{SSE (Error)}}
$$

**决定系数** $R^2$：

$$
R^2 = \frac{\text{SSR}}{\text{SST}} = 1 - \frac{\text{SSE}}{\text{SST}} = \frac{[\text{Cov}(X, Y)]^2}{\text{Var}(X) \cdot \text{Var}(Y)}
$$

**解释**：
- $R^2 \in [0, 1]$
- $R^2 = 0.8$ 表示80%的Y变异可由X解释
- 简单线性回归中，$R^2 = r_{XY}^2$（相关系数的平方）

**调整 $R^2$**（惩罚参数过多）：

$$
\bar{R}^2 = 1 - \frac{\text{SSE}/(n-p)}{\text{SST}/(n-1)} = 1 - (1 - R^2)\frac{n-1}{n-p}
$$

其中 $p$ 是参数个数（含截距）

**例1**：已知数据 $(1, 2), (2, 4), (3, 5), (4, 4), (5, 5)$，拟合线性回归。

**解**：

1. 计算均值：$\bar{x} = 3$, $\bar{y} = 4$
2. 计算：
   $$
   S_{xx} = \sum(x_i - \bar{x})^2 = 10
   $$
   $$
   S_{xy} = \sum(x_i - \bar{x})(y_i - \bar{y}) = 7
   $$
3. 系数：
   $$
   \hat{\beta}_1 = \frac{7}{10} = 0.7, \quad \hat{\beta}_0 = 4 - 0.7 \times 3 = 1.9
   $$
4. 回归方程：$\hat{y} = 1.9 + 0.7x$
5. $R^2$ 计算：
   $$
   \text{SST} = \sum(y_i - 4)^2 = 6, \quad \text{SSE} = \sum(y_i - \hat{y}_i)^2 = 1.1
   $$
   $$
   R^2 = 1 - \frac{1.1}{6} = 0.817
   $$

#### 假设检验与置信区间

**回归系数显著性检验**（t检验）：

$$
H_0: \beta_1 = 0 \quad \text{vs} \quad H_1: \beta_1 \neq 0
$$

**检验统计量**：

$$
t = \frac{\hat{\beta}_1}{\text{SE}(\hat{\beta}_1)} = \frac{\hat{\beta}_1}{s/\sqrt{S_{xx}}} \sim t(n-2)
$$

其中 $s = \sqrt{\frac{\text{SSE}}{n-2}}$ 是残差标准误

**置信区间**（$1-\alpha$ 置信度）：

$$
\hat{\beta}_1 \pm t_{\alpha/2, n-2} \cdot \text{SE}(\hat{\beta}_1)
$$

**整体显著性检验**（F检验）：

$$
F = \frac{\text{MSR}}{\text{MSE}} = \frac{\text{SSR}/1}{\text{SSE}/(n-2)} \sim F(1, n-2)
$$

（简单线性回归中，$F = t^2$）

**预测**：

1. **均值预测**（$X = x_0$ 时 $E[Y]$ 的置信区间）：
   $$
   \hat{y}_0 \pm t_{\alpha/2, n-2} \cdot s \sqrt{\frac{1}{n} + \frac{(x_0 - \bar{x})^2}{S_{xx}}}
   $$

2. **个体预测**（单个新观测值的预测区间）：
   $$
   \hat{y}_0 \pm t_{\alpha/2, n-2} \cdot s \sqrt{1 + \frac{1}{n} + \frac{(x_0 - \bar{x})^2}{S_{xx}}}
   $$

**注意**：个体预测区间更宽（多了"1"）

### 7.2 多元线性回归

#### 模型与矩阵形式

**多元线性回归模型**：

$$
Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \cdots + \beta_k X_{ik} + \epsilon_i
$$

**矩阵形式**：

$$
\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}
$$

其中：
- $\mathbf{Y} = (Y_1, \ldots, Y_n)^T$ 是 $n \times 1$ 响应向量
- $\mathbf{X} = \begin{bmatrix} 1 & X_{11} & \cdots & X_{1k} \\ \vdots & \vdots & \ddots & \vdots \\ 1 & X_{n1} & \cdots & X_{nk} \end{bmatrix}$ 是 $n \times (k+1)$ 设计矩阵
- $\boldsymbol{\beta} = (\beta_0, \beta_1, \ldots, \beta_k)^T$ 是 $(k+1) \times 1$ 参数向量
- $\boldsymbol{\epsilon} \sim N(\mathbf{0}, \sigma^2 \mathbf{I})$

#### OLS估计（矩阵推导）

**正规方程**：

$$
\mathbf{X}^T \mathbf{X} \hat{\boldsymbol{\beta}} = \mathbf{X}^T \mathbf{Y}
$$

**OLS解**：

$$
\hat{\boldsymbol{\beta}} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{Y}
$$

**预测值**：

$$
\hat{\mathbf{Y}} = \mathbf{X}\hat{\boldsymbol{\beta}} = \mathbf{X}(\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{Y} = \mathbf{H}\mathbf{Y}
$$

其中 $\mathbf{H} = \mathbf{X}(\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T$ 称为**帽子矩阵**（Hat Matrix）

**性质**：
1. $\mathbf{H}$ 是幂等矩阵：$\mathbf{H}^2 = \mathbf{H}$
2. $\mathbf{H}$ 是对称矩阵：$\mathbf{H}^T = \mathbf{H}$
3. $\text{tr}(\mathbf{H}) = k + 1$（自由度）

**残差**：

$$
\mathbf{e} = \mathbf{Y} - \hat{\mathbf{Y}} = (\mathbf{I} - \mathbf{H})\mathbf{Y}
$$

**估计方差**：

$$
\text{Var}(\hat{\boldsymbol{\beta}}) = \sigma^2 (\mathbf{X}^T \mathbf{X})^{-1}
$$

$$
\hat{\sigma}^2 = \frac{\mathbf{e}^T \mathbf{e}}{n - k - 1} = \frac{\text{SSE}}{n - k - 1}
$$

#### 假设检验

**单个系数检验**（t检验）：

$$
H_0: \beta_j = 0 \quad \text{vs} \quad H_1: \beta_j \neq 0
$$

$$
t_j = \frac{\hat{\beta}_j}{\text{SE}(\hat{\beta}_j)} \sim t(n - k - 1)
$$

其中 $\text{SE}(\hat{\beta}_j) = \hat{\sigma} \sqrt{[(\mathbf{X}^T \mathbf{X})^{-1}]_{jj}}$

**整体显著性检验**（F检验）：

$$
H_0: \beta_1 = \beta_2 = \cdots = \beta_k = 0
$$

$$
F = \frac{\text{MSR}}{\text{MSE}} = \frac{\text{SSR}/k}{\text{SSE}/(n-k-1)} \sim F(k, n-k-1)
$$

**部分F检验**（比较两个嵌套模型）：

检验是否可以去掉 $q$ 个变量：

$$
F = \frac{(\text{SSE}_{\text{reduced}} - \text{SSE}_{\text{full}})/q}{\text{SSE}_{\text{full}}/(n-k-1)} \sim F(q, n-k-1)
$$

#### 拟合优度与调整

**多元 $R^2$**：

$$
R^2 = \frac{\text{SSR}}{\text{SST}} = 1 - \frac{\text{SSE}}{\text{SST}}
$$

**问题**：添加任何变量都会增加 $R^2$（即使是无关变量）

**调整 $R^2$**：

$$
\bar{R}^2 = 1 - \frac{\text{SSE}/(n-k-1)}{\text{SST}/(n-1)} = 1 - (1 - R^2)\frac{n-1}{n-k-1}
$$

**比较**：
- 添加有意义的变量：$\bar{R}^2$ 增加
- 添加无意义的变量：$\bar{R}^2$ 可能减少

**例2**：用 $X_1$（学习时间）和 $X_2$（智商）预测考试成绩 $Y$

| $Y$ | $X_1$ | $X_2$ |
|-----|-------|-------|
| 80  | 5     | 110   |
| 85  | 7     | 120   |
| 75  | 4     | 105   |
| 90  | 8     | 125   |

矩阵形式：

$$
\mathbf{X} = \begin{bmatrix} 1 & 5 & 110 \\ 1 & 7 & 120 \\ 1 & 4 & 105 \\ 1 & 8 & 125 \end{bmatrix}, \quad \mathbf{Y} = \begin{bmatrix} 80 \\ 85 \\ 75 \\ 90 \end{bmatrix}
$$

使用 $\hat{\boldsymbol{\beta}} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{Y}$ 求解。

### 7.3 残差分析与诊断

#### 残差的定义与性质

**普通残差**：

$$
e_i = y_i - \hat{y}_i
$$

**标准化残差**（Standardized Residuals）：

$$
r_i = \frac{e_i}{s\sqrt{1 - h_{ii}}}
$$

其中 $h_{ii}$ 是帽子矩阵 $\mathbf{H}$ 的第 $i$ 个对角元

**学生化残差**（Studentized Residuals）：

$$
t_i = \frac{e_i}{s_{(i)}\sqrt{1 - h_{ii}}}
$$

其中 $s_{(i)}$ 是去掉第 $i$ 个观测后的残差标准差

**性质**：
- $\sum e_i = 0$（OLS保证）
- $\text{Var}(e_i) = \sigma^2(1 - h_{ii})$（不同观测残差方差不同！）
- $|r_i| > 2$ 或 $|t_i| > 2$ 提示可能是异常值

#### 模型假设检验

**1. 线性性检验**

**残差 vs 拟合值图**：
- 理想：随机散布，无明显模式
- 违反：呈现曲线或其他模式

**解决**：
- 添加多项式项（如 $X^2$）
- 变量变换（如取对数）

**2. 正态性检验**

**Q-Q图**（Quantile-Quantile Plot）：
- 理想：点接近直线
- 违反：S形、重尾、偏态

**Shapiro-Wilk检验**：
$$
H_0: \text{残差服从正态分布}
$$

**解决**：
- Box-Cox变换
- 使用稳健回归方法

**3. 同方差性检验**

**残差绝对值 vs 拟合值图**：
- 理想：水平带状，宽度恒定
- 违反：喇叭形（异方差）

**Breusch-Pagan检验**：

$$
H_0: \text{Var}(\epsilon_i) = \sigma^2 \quad (\text{同方差})
$$

步骤：
1. 拟合原模型，得残差 $e_i$
2. 用 $e_i^2$ 对 $\mathbf{X}$ 回归，得 $R^2_{e^2}$
3. 统计量：$\chi^2 = n R^2_{e^2} \sim \chi^2(k)$

**解决**：
- 加权最小二乘（WLS）
- 稳健标准误（Robust SE）

**4. 独立性检验**

**Durbin-Watson检验**（检验序列相关）：

$$
DW = \frac{\sum_{i=2}^{n} (e_i - e_{i-1})^2}{\sum_{i=1}^{n} e_i^2} \approx 2(1 - \hat{\rho})
$$

**解释**：
- $DW \approx 2$：无自相关
- $DW < 2$：正相关
- $DW > 2$：负相关

**解决**：
- 添加滞后项
- 广义最小二乘（GLS）

#### 影响诊断（Influence Diagnostics）

**1. 杠杆值**（Leverage）：$h_{ii}$

**定义**：帽子矩阵对角元，衡量 $X_i$ 的极端程度

**阈值**：$h_{ii} > \frac{2(k+1)}{n}$ 为高杠杆点

**2. Cook's Distance**：

$$
D_i = \frac{(\hat{\mathbf{Y}} - \hat{\mathbf{Y}}_{(i)})^T (\hat{\mathbf{Y}} - \hat{\mathbf{Y}}_{(i)})}{(k+1)\hat{\sigma}^2} = \frac{r_i^2}{k+1} \cdot \frac{h_{ii}}{1 - h_{ii}}
$$

**解释**：去掉第 $i$ 个观测对所有拟合值的影响

**阈值**：$D_i > \frac{4}{n}$ 或 $D_i > 1$ 为强影响点

**3. DFFITS**：

$$
\text{DFFITS}_i = \frac{\hat{y}_i - \hat{y}_{i(i)}}{s_{(i)}\sqrt{h_{ii}}}
$$

**解释**：去掉第 $i$ 个观测对自身拟合值的影响

**阈值**：$|\text{DFFITS}_i| > 2\sqrt{\frac{k+1}{n}}$

**4. DFBETAS**：

$$
\text{DFBETAS}_{j,i} = \frac{\hat{\beta}_j - \hat{\beta}_{j(i)}}{s_{(i)}\sqrt{[(\mathbf{X}^T \mathbf{X})^{-1}]_{jj}}}
$$

**解释**：去掉第 $i$ 个观测对 $\hat{\beta}_j$ 的影响

**阈值**：$|\text{DFBETAS}_{j,i}| > \frac{2}{\sqrt{n}}$

**诊断流程**：

```
1. 识别高杠杆点（h_ii大）
   ├─ 如果残差小 → 好的杠杆点（保留）
   └─ 如果残差大 → 坏的杠杆点（影响点）

2. 检查Cook's D
   ├─ D_i大 → 强影响点
   └─ 仔细检查是否为数据错误或异常情况

3. 决定
   ├─ 数据错误 → 修正或删除
   ├─ 真实异常 → 报告并单独分析
   └─ 正常波动 → 保留，考虑稳健方法
```

### 7.4 多重共线性

#### 定义与检测

**多重共线性**：解释变量之间高度线性相关

**后果**：
1. $(\mathbf{X}^T \mathbf{X})$ 接近奇异，估计不稳定
2. $\text{SE}(\hat{\beta}_j)$ 很大，单个检验不显著
3. 整体 $F$ 检验显著，但各个 $t$ 检验不显著
4. 系数符号可能违背经济意义

**检测方法**：

**1. 方差膨胀因子**（VIF, Variance Inflation Factor）：

$$
\text{VIF}_j = \frac{1}{1 - R_j^2}
$$

其中 $R_j^2$ 是 $X_j$ 对其他所有自变量回归的决定系数

**判断标准**：
- $\text{VIF}_j < 5$：轻微共线性
- $5 \leq \text{VIF}_j < 10$：中等共线性
- $\text{VIF}_j \geq 10$：严重共线性

**2. 条件数**（Condition Number）：

$$
\kappa = \sqrt{\frac{\lambda_{\max}}{\lambda_{\min}}}
$$

其中 $\lambda$ 是 $\mathbf{X}^T \mathbf{X}$ 的特征值

**判断**：$\kappa > 30$ 表明严重共线性

**3. 相关矩阵**：

检查自变量间的相关系数，$|r_{XiXj}| > 0.8$ 提示共线性

#### 解决方法

**1. 删除变量**

删除一个或多个高度相关的变量（基于理论或VIF）

**优点**：简单、解释性好
**缺点**：可能丢失信息、遗漏变量偏差

**2. 岭回归**（Ridge Regression）

$$
\hat{\boldsymbol{\beta}}_{\text{ridge}} = (\mathbf{X}^T \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}^T \mathbf{Y}
$$

**作用**：添加 $\lambda \mathbf{I}$ 使矩阵稳定，牺牲少量偏差换取方差大幅下降

**选择 $\lambda$**：交叉验证

**3. 主成分回归**（PCR）

步骤：
1. 对 $\mathbf{X}$ 做主成分分析（PCA）
2. 选取前 $m$ 个主成分 $\mathbf{Z}$（$m < k$）
3. 用 $\mathbf{Z}$ 代替 $\mathbf{X}$ 做回归

**优点**：完全消除共线性
**缺点**：解释性差

**4. 偏最小二乘**（PLS）

类似PCR，但主成分同时考虑 $\mathbf{X}$ 和 $\mathbf{Y}$

**5. 增加样本量**

更多数据可能降低共线性的影响

### 7.5 异方差问题

#### 后果

**1. OLS仍无偏一致**，但不再是最小方差估计（失去BLUE性质）

**2. 标准误估计有偏**：
- 真实：$\text{Var}(\hat{\boldsymbol{\beta}}) = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \boldsymbol{\Omega} \mathbf{X} (\mathbf{X}^T \mathbf{X})^{-1}$
- OLS估计：$\hat{\text{Var}}(\hat{\boldsymbol{\beta}}) = \hat{\sigma}^2 (\mathbf{X}^T \mathbf{X})^{-1}$
- 其中 $\boldsymbol{\Omega} = \text{diag}(\sigma_1^2, \ldots, \sigma_n^2)$

**3. t检验和F检验失效**

#### 检测

**1. 图形法**：

- **残差 vs 拟合值**：喇叭形
- **残差平方 vs 自变量**：上升或下降趋势

**2. Breusch-Pagan检验**（前面已介绍）

**3. White检验**（更一般）：

假设 $\text{Var}(\epsilon_i) = h(\mathbf{X}_i)$（任意函数关系）

步骤：
1. 拟合原模型，得 $e_i$
2. $e_i^2$ 对 $\mathbf{X}$、$\mathbf{X}^2$、交叉项 $X_iX_j$ 回归，得 $R^2$
3. $n R^2 \sim \chi^2(p-1)$（$p$ 是辅助回归的参数个数）

#### 解决方法

**1. 加权最小二乘**（WLS, Weighted Least Squares）

假设 $\text{Var}(\epsilon_i) = \sigma^2 / w_i$（已知权重 $w_i$）

**目标**：

$$
\min \sum_{i=1}^{n} w_i (y_i - \beta_0 - \beta_1 x_{i1} - \cdots - \beta_k x_{ik})^2
$$

**矩阵形式**：

$$
\hat{\boldsymbol{\beta}}_{\text{WLS}} = (\mathbf{X}^T \mathbf{W} \mathbf{X})^{-1} \mathbf{X}^T \mathbf{W} \mathbf{Y}
$$

其中 $\mathbf{W} = \text{diag}(w_1, \ldots, w_n)$

**性质**：WLS是BLUE

**实践**：
- 如果 $\text{Var}(\epsilon_i) \propto x_i$，设 $w_i = 1/x_i$
- 如果 $\text{Var}(\epsilon_i) \propto x_i^2$，设 $w_i = 1/x_i^2$

**可行广义最小二乘**（FGLS）：

当权重未知时：
1. 用OLS估计模型，得 $\hat{e}_i$
2. 用 $\ln(\hat{e}_i^2)$ 对 $\mathbf{X}$ 回归，预测 $\hat{\sigma}_i^2$
3. 设 $w_i = 1/\hat{\sigma}_i^2$，用WLS重新估计

**2. 稳健标准误**（Robust Standard Errors）

**HC0（White稳健标准误）**：

$$
\widehat{\text{Var}}(\hat{\boldsymbol{\beta}}) = (\mathbf{X}^T \mathbf{X})^{-1} \left( \sum_{i=1}^{n} e_i^2 \mathbf{x}_i \mathbf{x}_i^T \right) (\mathbf{X}^T \mathbf{X})^{-1}
$$

**HC1（修正）**：

$$
\widehat{\text{Var}}(\hat{\boldsymbol{\beta}}) = \frac{n}{n-k-1} (\mathbf{X}^T \mathbf{X})^{-1} \left( \sum_{i=1}^{n} e_i^2 \mathbf{x}_i \mathbf{x}_i^T \right) (\mathbf{X}^T \mathbf{X})^{-1}
$$

**HC3（更稳健）**：

$$
\widehat{\text{Var}}(\hat{\boldsymbol{\beta}}) = (\mathbf{X}^T \mathbf{X})^{-1} \left( \sum_{i=1}^{n} \frac{e_i^2}{(1-h_{ii})^2} \mathbf{x}_i \mathbf{x}_i^T \right) (\mathbf{X}^T \mathbf{X})^{-1}
$$

**优点**：
- 不需要指定异方差形式
- 系数估计不变，只改变标准误

**缺点**：
- 大样本下才渐近有效
- 小样本可能不准确

**3. 变量变换**

**常见变换**：
- **对数变换**：$\ln(Y)$ 对 $\ln(X)$（稳定方差，适合比例数据）
- **平方根变换**：$\sqrt{Y}$（适合计数数据）
- **Box-Cox变换**：
  $$
  Y^{(\lambda)} = \begin{cases} \frac{Y^\lambda - 1}{\lambda}, & \lambda \neq 0 \\ \ln(Y), & \lambda = 0 \end{cases}
  $$
  选择 $\lambda$ 使残差方差最小

### 7.6 模型选择

#### 选择标准

**1. AIC（Akaike Information Criterion）**：

$$
\text{AIC} = n \ln(\text{SSE}/n) + 2(k+1)
$$

**越小越好**（平衡拟合优度与复杂度）

**2. BIC（Bayesian Information Criterion）**：

$$
\text{BIC} = n \ln(\text{SSE}/n) + (k+1) \ln(n)
$$

**比AIC更惩罚复杂模型**（$\ln(n) > 2$ 当 $n > 7$）

**3. Adjusted $R^2$**（前面已介绍）

**4. Mallows' $C_p$**：

$$
C_p = \frac{\text{SSE}_p}{\hat{\sigma}^2_{\text{full}}} - n + 2p
$$

**理想**：$C_p \approx p$

**5. 交叉验证**（Cross-Validation）：

**K-fold CV**：
1. 数据分成 $K$ 份
2. 每次用 $K-1$ 份训练，1份测试
3. 平均测试误差：$\text{CV} = \frac{1}{K} \sum_{i=1}^{K} \text{MSE}_i$

**LOOCV**（Leave-One-Out）：$K = n$

#### 变量选择方法

**1. 全子集回归**（All Subsets Regression）

枚举所有 $2^k$ 个模型，选AIC/BIC最小者

**优点**：保证全局最优
**缺点**：$k$ 大时计算不可行

**2. 逐步回归**（Stepwise Regression）

**前向选择**（Forward Selection）：
1. 从零模型开始（只有截距）
2. 每次加入使AIC下降最多的变量
3. 直到无法改善

**后向消除**（Backward Elimination）：
1. 从全模型开始
2. 每次删除最不显著的变量（p值最大且 > 0.05）
3. 直到所有变量显著

**双向逐步**（Stepwise）：
- 结合前向和后向
- 每步可以加入或删除变量

**优点**：计算快
**缺点**：
- 局部最优
- p值不再准确（因为多次检验）
- 不稳定（数据微小变化导致不同模型）

**3. 正则化方法**

**Lasso回归**（$L_1$ 惩罚）：

$$
\min \left\{ \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{k} \beta_j x_{ij})^2 + \lambda \sum_{j=1}^{k} |\beta_j| \right\}
$$

**特点**：某些 $\hat{\beta}_j$ 精确为0（自动变量选择）

**Elastic Net**（$L_1 + L_2$）：

$$
\min \left\{ \text{RSS} + \lambda \left[ \alpha \sum |\beta_j| + (1-\alpha) \sum \beta_j^2 \right] \right\}
$$

**参数**：
- $\lambda$：惩罚强度（交叉验证选择）
- $\alpha \in [0, 1]$：$L_1$ vs $L_2$ 权重

#### 模型选择原则

**1. 理论优先**：基于领域知识选择变量

**2. 简约原则**（Parsimony）：能用简单模型就不用复杂模型

**3. 稳定性**：
- 检查不同样本下模型是否稳定
- Bootstrap评估系数稳定性

**4. 预测能力**：
- 用测试集评估
- 避免过拟合

**5. 诊断检查**：
- 残差分析
- 影响诊断
- 假设检验

**警告**：
- **不要数据挖掘**（data dredging）：尝试大量模型直到p < 0.05
- **不要p-hacking**：选择性报告结果
- **报告完整过程**：说明模型选择步骤

**例3**：比较三个模型

| 模型 | $k$ | $\text{SSE}$ | $R^2$ | $\bar{R}^2$ | AIC | BIC |
|------|-----|--------------|-------|-------------|-----|-----|
| M1   | 2   | 150          | 0.70  | 0.68        | 250 | 258 |
| M2   | 5   | 120          | 0.76  | 0.71        | 248 | 264 |
| M3   | 10  | 100          | 0.80  | 0.70        | 252 | 280 |

**分析**：
- M3的 $R^2$ 最高，但 $\bar{R}^2$ 与M2相同
- M2的AIC最低
- M1的BIC最低（最简约）
- **建议**：选M2（平衡性能与复杂度）

### 7.7 应用实例

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.stats.diagnostic import het_breuschpagan, het_white

def comprehensive_regression_analysis(X, y, feature_names=None, alpha=0.05):
    """
    完整的回归分析流程（拟合、诊断、可视化）
    
    参数:
    - X: n×k 设计矩阵（不含截距列）
    - y: n×1 响应变量
    - feature_names: 特征名称列表
    """
    import pandas as pd
    from sklearn.linear_model import LinearRegression
    from sklearn.preprocessing import StandardScaler
    
    n, k = X.shape
    if feature_names is None:
        feature_names = [f'X{i+1}' for i in range(k)]
    
    print("=" * 70)
    print("多元线性回归完整分析报告")
    print("=" * 70)
    
    # ===== 1. 模型拟合 =====
    print("\n【1. 模型拟合】")
    
    # 添加截距列
    X_with_intercept = np.column_stack([np.ones(n), X])
    
    # OLS估计
    beta_hat = np.linalg.inv(X_with_intercept.T @ X_with_intercept) @ X_with_intercept.T @ y
    y_pred = X_with_intercept @ beta_hat
    residuals = y - y_pred
    
    # 计算统计量
    y_mean = np.mean(y)
    SST = np.sum((y - y_mean)**2)
    SSR = np.sum((y_pred - y_mean)**2)
    SSE = np.sum(residuals**2)
    
    MSR = SSR / k
    MSE = SSE / (n - k - 1)
    s = np.sqrt(MSE)
    
    R2 = SSR / SST
    adj_R2 = 1 - (SSE / (n - k - 1)) / (SST / (n - 1))
    
    # F检验
    F_stat = MSR / MSE
    F_pval = 1 - stats.f.cdf(F_stat, k, n - k - 1)
    
    print(f"样本量: n = {n}")
    print(f"特征数: k = {k}")
    print(f"\n回归方程:")
    print(f"  y = {beta_hat[0]:.4f}", end="")
    for i, name in enumerate(feature_names):
        sign = "+" if beta_hat[i+1] >= 0 else "-"
        print(f" {sign} {abs(beta_hat[i+1]):.4f}*{name}", end="")
    print()
    
    print(f"\n拟合优度:")
    print(f"  R² = {R2:.4f}")
    print(f"  Adjusted R² = {adj_R2:.4f}")
    print(f"  残差标准误: s = {s:.4f}")
    
    print(f"\n整体显著性检验 (F检验):")
    print(f"  F({k}, {n-k-1}) = {F_stat:.4f}")
    print(f"  p值 = {F_pval:.6f}")
    if F_pval < alpha:
        print(f"  ✓ 模型整体显著 (p < {alpha})")
    else:
        print(f"  ✗ 模型整体不显著 (p ≥ {alpha})")
    
    # ===== 2. 系数检验 =====
    print("\n【2. 回归系数检验】")
    
    # 计算标准误
    var_beta = MSE * np.linalg.inv(X_with_intercept.T @ X_with_intercept)
    se_beta = np.sqrt(np.diag(var_beta))
    
    # t检验
    t_stats = beta_hat / se_beta
    t_pvals = 2 * (1 - stats.t.cdf(np.abs(t_stats), n - k - 1))
    
    # 置信区间
    t_critical = stats.t.ppf(1 - alpha/2, n - k - 1)
    ci_lower = beta_hat - t_critical * se_beta
    ci_upper = beta_hat + t_critical * se_beta
    
    coef_table = pd.DataFrame({
        '系数': beta_hat,
        '标准误': se_beta,
        't统计量': t_stats,
        'p值': t_pvals,
        f'{int((1-alpha)*100)}% CI下限': ci_lower,
        f'{int((1-alpha)*100)}% CI上限': ci_upper
    }, index=['截距'] + feature_names)
    
    print(coef_table.to_string())
    
    # ===== 3. 多重共线性检测 =====
    print("\n【3. 多重共线性诊断】")
    
    # VIF计算
    vif_data = []
    for i in range(k):
        vif = variance_inflation_factor(X, i)
        vif_data.append({'特征': feature_names[i], 'VIF': vif})
    
    vif_df = pd.DataFrame(vif_data)
    print(vif_df.to_string(index=False))
    
    max_vif = vif_df['VIF'].max()
    if max_vif < 5:
        print("✓ 无严重多重共线性 (所有VIF < 5)")
    elif max_vif < 10:
        print("⚠ 存在中等多重共线性 (5 ≤ VIF < 10)")
    else:
        print("✗ 严重多重共线性 (VIF ≥ 10)")
    
    # ===== 4. 异方差检验 =====
    print("\n【4. 异方差性检验】")
    
    # Breusch-Pagan检验
    bp_stat, bp_pval, _, _ = het_breuschpagan(residuals, X_with_intercept)
    print(f"Breusch-Pagan检验:")
    print(f"  统计量 = {bp_stat:.4f}, p值 = {bp_pval:.4f}")
    if bp_pval > alpha:
        print(f"  ✓ 满足同方差假设 (p ≥ {alpha})")
    else:
        print(f"  ✗ 存在异方差 (p < {alpha})")
    
    # White检验
    white_stat, white_pval, _, _ = het_white(residuals, X_with_intercept)
    print(f"\nWhite检验:")
    print(f"  统计量 = {white_stat:.4f}, p值 = {white_pval:.4f}")
    
    # ===== 5. 正态性检验 =====
    print("\n【5. 残差正态性检验】")
    
    shapiro_stat, shapiro_pval = stats.shapiro(residuals)
    print(f"Shapiro-Wilk检验:")
    print(f"  统计量 = {shapiro_stat:.4f}, p值 = {shapiro_pval:.4f}")
    if shapiro_pval > alpha:
        print(f"  ✓ 残差近似正态 (p ≥ {alpha})")
    else:
        print(f"  ✗ 残差偏离正态 (p < {alpha})")
    
    # ===== 6. 影响诊断 =====
    print("\n【6. 影响点诊断】")
    
    # 帽子矩阵
    H = X_with_intercept @ np.linalg.inv(X_with_intercept.T @ X_with_intercept) @ X_with_intercept.T
    leverage = np.diag(H)
    
    # 标准化残差
    std_residuals = residuals / (s * np.sqrt(1 - leverage))
    
    # Cook's D
    cooks_d = (std_residuals**2 / (k + 1)) * (leverage / (1 - leverage))
    
    # 阈值
    leverage_threshold = 2 * (k + 1) / n
    cooks_threshold = 4 / n
    
    high_leverage = np.where(leverage > leverage_threshold)[0]
    high_influence = np.where(cooks_d > cooks_threshold)[0]
    outliers = np.where(np.abs(std_residuals) > 2)[0]
    
    print(f"高杠杆点 (h_ii > {leverage_threshold:.3f}): {len(high_leverage)} 个")
    if len(high_leverage) > 0 and len(high_leverage) <= 10:
        print(f"  索引: {high_leverage.tolist()}")
    
    print(f"强影响点 (Cook's D > {cooks_threshold:.3f}): {len(high_influence)} 个")
    if len(high_influence) > 0 and len(high_influence) <= 10:
        print(f"  索引: {high_influence.tolist()}")
    
    print(f"异常值 (|std_res| > 2): {len(outliers)} 个")
    if len(outliers) > 0 and len(outliers) <= 10:
        print(f"  索引: {outliers.tolist()}")
    
    # ===== 7. 可视化 =====
    fig = plt.figure(figsize=(16, 12))
    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)
    
    # (1) 残差 vs 拟合值
    ax1 = fig.add_subplot(gs[0, 0])
    ax1.scatter(y_pred, residuals, alpha=0.6, edgecolors='k', linewidth=0.5)
    ax1.axhline(0, color='red', linestyle='--', linewidth=2)
    ax1.set_xlabel('拟合值')
    ax1.set_ylabel('残差')
    ax1.set_title('残差 vs 拟合值图')
    ax1.grid(alpha=0.3)
    
    # (2) Q-Q图
    ax2 = fig.add_subplot(gs[0, 1])
    stats.probplot(residuals, dist="norm", plot=ax2)
    ax2.set_title('正态Q-Q图')
    ax2.grid(alpha=0.3)
    
    # (3) Scale-Location图
    ax3 = fig.add_subplot(gs[0, 2])
    ax3.scatter(y_pred, np.sqrt(np.abs(std_residuals)), alpha=0.6, edgecolors='k', linewidth=0.5)
    ax3.set_xlabel('拟合值')
    ax3.set_ylabel('√|标准化残差|')
    ax3.set_title('Scale-Location图（检验同方差性）')
    ax3.grid(alpha=0.3)
    
    # (4) 残差直方图
    ax4 = fig.add_subplot(gs[1, 0])
    ax4.hist(residuals, bins=20, density=True, alpha=0.7, edgecolor='black')
    # 叠加正态曲线
    xmin, xmax = ax4.get_xlim()
    x_norm = np.linspace(xmin, xmax, 100)
    ax4.plot(x_norm, stats.norm.pdf(x_norm, 0, s), 'r-', linewidth=2, label='理论正态')
    ax4.set_xlabel('残差')
    ax4.set_ylabel('密度')
    ax4.set_title('残差分布')
    ax4.legend()
    ax4.grid(alpha=0.3)
    
    # (5) Cook's D图
    ax5 = fig.add_subplot(gs[1, 1])
    ax5.stem(range(n), cooks_d, basefmt=" ", markerfmt='o')
    ax5.axhline(cooks_threshold, color='red', linestyle='--', linewidth=2, label=f'阈值={cooks_threshold:.3f}')
    ax5.set_xlabel('观测序号')
    ax5.set_ylabel("Cook's D")
    ax5.set_title("Cook's Distance")
    ax5.legend()
    ax5.grid(alpha=0.3)
    
    # (6) 杠杆值 vs 标准化残差
    ax6 = fig.add_subplot(gs[1, 2])
    ax6.scatter(leverage, std_residuals, alpha=0.6, edgecolors='k', linewidth=0.5)
    ax6.axhline(0, color='gray', linestyle='-', linewidth=1)
    ax6.axhline(2, color='red', linestyle='--', linewidth=1.5, label='±2')
    ax6.axhline(-2, color='red', linestyle='--', linewidth=1.5)
    ax6.axvline(leverage_threshold, color='orange', linestyle='--', linewidth=1.5, label=f'h={leverage_threshold:.3f}')
    ax6.set_xlabel('杠杆值')
    ax6.set_ylabel('标准化残差')
    ax6.set_title('杠杆值 vs 标准化残差')
    ax6.legend()
    ax6.grid(alpha=0.3)
    
    # (7) VIF柱状图
    ax7 = fig.add_subplot(gs[2, 0])
    colors = ['red' if v >= 10 else 'orange' if v >= 5 else 'green' for v in vif_df['VIF']]
    ax7.barh(feature_names, vif_df['VIF'], color=colors, alpha=0.7, edgecolor='black')
    ax7.axvline(5, color='orange', linestyle='--', linewidth=1.5, label='VIF=5')
    ax7.axvline(10, color='red', linestyle='--', linewidth=1.5, label='VIF=10')
    ax7.set_xlabel('VIF')
    ax7.set_title('方差膨胀因子')
    ax7.legend()
    ax7.grid(alpha=0.3, axis='x')
    
    # (8) 系数图
    ax8 = fig.add_subplot(gs[2, 1])
    coef_values = beta_hat[1:]  # 不含截距
    coef_errors = se_beta[1:] * t_critical
    y_pos = np.arange(len(feature_names))
    colors_coef = ['green' if p < alpha else 'red' for p in t_pvals[1:]]
    ax8.barh(y_pos, coef_values, xerr=coef_errors, color=colors_coef, alpha=0.7, 
             capsize=5, edgecolor='black')
    ax8.set_yticks(y_pos)
    ax8.set_yticklabels(feature_names)
    ax8.axvline(0, color='black', linestyle='-', linewidth=1)
    ax8.set_xlabel('系数估计值 (±95% CI)')
    ax8.set_title('回归系数及置信区间')
    ax8.grid(alpha=0.3, axis='x')
    
    # (9) 实际 vs 预测
    ax9 = fig.add_subplot(gs[2, 2])
    ax9.scatter(y, y_pred, alpha=0.6, edgecolors='k', linewidth=0.5)
    min_val = min(y.min(), y_pred.min())
    max_val = max(y.max(), y_pred.max())
    ax9.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='理想拟合')
    ax9.set_xlabel('实际值')
    ax9.set_ylabel('预测值')
    ax9.set_title(f'实际 vs 预测 (R²={R2:.3f})')
    ax9.legend()
    ax9.grid(alpha=0.3)
    
    plt.suptitle('多元线性回归诊断图', fontsize=16, fontweight='bold', y=0.995)
    # plt.show()
    
    return {
        'coefficients': beta_hat,
        'se': se_beta,
        't_stats': t_stats,
        'p_values': t_pvals,
        'R2': R2,
        'adj_R2': adj_R2,
        'F_stat': F_stat,
        'F_pval': F_pval,
        'residuals': residuals,
        'predictions': y_pred,
        'vif': vif_df,
        'leverage': leverage,
        'cooks_d': cooks_d
    }

# # 示例：波士顿房价数据
# np.random.seed(42)
# n = 100
# X1 = np.random.uniform(1, 10, n)  # 房间数
# X2 = np.random.uniform(0, 100, n)  # 犯罪率
# X3 = np.random.uniform(1, 5, n)  # 距离市中心
# 
# y = 50 + 10*X1 - 0.5*X2 - 3*X3 + np.random.normal(0, 5, n)
# 
# X = np.column_stack([X1, X2, X3])
# 
# result = comprehensive_regression_analysis(
#     X, y,
#     feature_names=['房间数', '犯罪率', '距离']
# )
```

### 7.8 本章小结

#### 核心公式总结

| 内容 | 公式 |
|------|------|
| **OLS估计** | $\hat{\boldsymbol{\beta}} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{Y}$ |
| **决定系数** | $R^2 = 1 - \frac{\text{SSE}}{\text{SST}}$ |
| **调整R²** | $\bar{R}^2 = 1 - (1 - R^2)\frac{n-1}{n-k-1}$ |
| **F检验** | $F = \frac{\text{MSR}}{\text{MSE}} \sim F(k, n-k-1)$ |
| **VIF** | $\text{VIF}_j = \frac{1}{1 - R_j^2}$ |
| **Cook's D** | $D_i = \frac{r_i^2}{k+1} \cdot \frac{h_{ii}}{1 - h_{ii}}$ |
| **AIC** | $n \ln(\text{SSE}/n) + 2(k+1)$ |
| **BIC** | $n \ln(\text{SSE}/n) + (k+1) \ln(n)$ |

#### 假设检验总结

| 假设 | 检验方法 | 违反后果 | 解决方案 |
|------|---------|---------|---------|
| **线性性** | 残差图、部分回归图 | 估计有偏 | 变换、多项式 |
| **独立性** | Durbin-Watson | SE有偏、推断失效 | GLS、时间序列模型 |
| **同方差性** | BP/White检验 | SE有偏、效率低 | WLS、稳健SE |
| **正态性** | Q-Q图、S-W检验 | 小样本推断不准 | 变换、Bootstrap |

#### 诊断流程图

```
数据准备
  ↓
拟合OLS
  ↓
检查整体显著性（F检验）
  ├─ 不显著 → 重新考虑模型
  └─ 显著 → 继续
       ↓
检查系数显著性（t检验）
  ├─ 有不显著 → 考虑删除或保留（理论）
  └─ 全显著 → 继续
       ↓
残差诊断（4个假设）
  ├─ 违反 → 变换/加权/稳健方法
  └─ 满足 → 继续
       ↓
多重共线性诊断（VIF）
  ├─ 严重 → 岭回归/删除变量/PCA
  └─ 轻微 → 继续
       ↓
影响诊断（Cook's D）
  ├─ 有强影响点 → 检查数据质量
  └─ 无异常 → 继续
       ↓
模型比较（AIC/BIC/CV）
  ↓
最终模型 + 解释
```

#### 常见错误与建议

| 错误 | 后果 | 正确做法 |
|------|------|---------|
| 忽略残差诊断 | 推断失效 | 必做4个假设检验 |
| 只看R² | 过拟合、忽略显著性 | 看adj-R²、p值、诊断图 |
| 机械删除不显著变量 | 遗漏偏差 | 结合理论判断 |
| 不检查共线性 | 系数不稳定 | 必算VIF |
| 忽略影响点 | 结果被异常值主导 | Cook's D诊断 |
| 过度拟合 | 泛化差 | 交叉验证、惩罚回归 |

#### 实战建议

1. **探索性分析**：
   - 散点图矩阵（看关系）
   - 相关系数矩阵（看共线性）
   - 箱线图（看异常值）

2. **模型构建**：
   - 从简单到复杂
   - 基于理论选变量
   - 避免数据驱动的过度拟合

3. **诊断验证**：
   - 残差图是最重要的诊断工具
   - 影响诊断识别问题数据
   - 检验假设，违反时采取措施

4. **结果报告**：
   - 报告完整的回归表（系数、SE、p值、CI）
   - 说明模型选择过程
   - 展示关键诊断图
   - 解释实际意义（不只是统计显著性）

5. **预测应用**：
   - 区分均值预测和个体预测
   - 提供置信区间/预测区间
   - 警告外推风险（$x$ 超出样本范围）

---

**本章完**
