# 概率论与数理统计 - (四)大数定律与中心极限定理

理解大数定律和中心极限定理。

---

## 4. 大数定律与中心极限定理

### 4.1 随机变量序列的收敛性

在介绍大数定律之前，需要理解几种收敛概念。

#### 收敛类型

设 $\{X_n\}$ 为随机变量序列。

##### (1) 依概率收敛（Convergence in Probability）

**定义**：若对任意 $\epsilon > 0$，

$$
\lim_{n \to \infty} P(|X_n - X| > \epsilon) = 0
$$

则称 $X_n$ **依概率收敛**于 $X$，记作 $X_n \xrightarrow{P} X$

**直观理解**：当 $n$ 很大时，$X_n$ 以很大概率接近 $X$

##### (2) 依分布收敛（Convergence in Distribution）

**定义**：若对 $F(x)$ 的每个连续点 $x$，

$$
\lim_{n \to \infty} F_n(x) = F(x)
$$

则称 $X_n$ **依分布收敛**于 $X$，记作 $X_n \xrightarrow{d} X$

**直观理解**：$X_n$ 的分布函数逐点收敛到 $X$ 的分布函数

##### (3) 几乎必然收敛（Almost Sure Convergence）

**定义**：若

$$
P\left(\lim_{n \to \infty} X_n = X\right) = 1
$$

则称 $X_n$ **几乎必然收敛**于 $X$，记作 $X_n \xrightarrow{a.s.} X$

**直观理解**：除了概率为0的事件外，$X_n$ 的每个实现都收敛到 $X$

##### (4) 均方收敛（Mean Square Convergence）

**定义**：若

$$
\lim_{n \to \infty} E[(X_n - X)^2] = 0
$$

则称 $X_n$ **均方收敛**于 $X$，记作 $X_n \xrightarrow{L^2} X$

#### 收敛性之间的关系

$$
\begin{aligned}
&\text{几乎必然收敛} \Rightarrow \text{依概率收敛} \Rightarrow \text{依分布收敛} \\
&\text{均方收敛} \Rightarrow \text{依概率收敛}
\end{aligned}
$$

**注意**：反向推理一般不成立。

### 4.2 大数定律

大数定律描述：当样本量足够大时，样本均值会趋近于总体均值。

#### 切比雪夫不等式

**定理**（Chebyshev不等式）：设随机变量 $X$ 有期望 $\mu$ 和方差 $\sigma^2$，则对任意 $\epsilon > 0$，

$$
P(|X - \mu| \geq \epsilon) \leq \frac{\sigma^2}{\epsilon^2}
$$

或等价地：

$$
P(|X - \mu| < \epsilon) \geq 1 - \frac{\sigma^2}{\epsilon^2}
$$

**应用**：给出了偏离均值的概率上界，无需知道分布的具体形式。

**例1**：设 $X$ 的期望为10，方差为4，求 $P(|X - 10| \geq 4)$ 的上界。

**解**：

$$
P(|X - 10| \geq 4) \leq \frac{4}{4^2} = \frac{1}{4} = 0.25
$$

#### 大数定律的各种形式

##### (1) 弱大数定律（Khinchin）

**定理**：设 $X_1, X_2, \ldots$ 独立同分布，$E[X_i] = \mu$，则

$$
\frac{1}{n} \sum_{i=1}^{n} X_i \xrightarrow{P} \mu
$$

即对任意 $\epsilon > 0$，

$$
\lim_{n \to \infty} P\left( \left| \frac{1}{n} \sum_{i=1}^{n} X_i - \mu \right| > \epsilon \right) = 0
$$

**直观意义**："频率趋于概率"的数学表达

**证明思路**（需要方差存在 $\sigma^2 < \infty$）：

设 $\overline{X}_n = \frac{1}{n}\sum_{i=1}^{n} X_i$

$$
E[\overline{X}_n] = \mu, \quad \text{Var}(\overline{X}_n) = \frac{\sigma^2}{n}
$$

由切比雪夫不等式：

$$
P(|\overline{X}_n - \mu| \geq \epsilon) \leq \frac{\text{Var}(\overline{X}_n)}{\epsilon^2} = \frac{\sigma^2}{n\epsilon^2} \xrightarrow{n \to \infty} 0
$$

##### (2) 伯努利大数定律

**定理**：设 $\mu_n$ 为 $n$ 次独立重复试验中事件 $A$ 发生的次数，$p = P(A)$，则

$$
\frac{\mu_n}{n} \xrightarrow{P} p
$$

**意义**：频率的稳定性，"频率趋于概率"

##### (3) 强大数定律

**定理**：设 $X_1, X_2, \ldots$ 独立同分布，$E[X_i] = \mu$，则

$$
\frac{1}{n} \sum_{i=1}^{n} X_i \xrightarrow{a.s.} \mu
$$

即：

$$
P\left(\lim_{n \to \infty} \frac{1}{n} \sum_{i=1}^{n} X_i = \mu\right) = 1
$$

**意义**：比弱大数定律更强的结论（几乎必然收敛）

#### 大数定律的意义

1. **理论意义**：建立了频率与概率的联系
2. **实践意义**：样本均值是总体均值的良好估计
3. **应用**：蒙特卡洛方法、统计推断的理论基础

### 4.3 中心极限定理

中心极限定理（Central Limit Theorem, CLT）是概率论中最重要的定理之一，解释了为什么正态分布在自然界中如此普遍。

#### 林德伯格-列维中心极限定理

**定理**：设 $X_1, X_2, \ldots, X_n$ 独立同分布，$E[X_i] = \mu$，$\text{Var}(X_i) = \sigma^2 < \infty$，则

$$
\frac{\sum_{i=1}^{n} X_i - n\mu}{\sqrt{n}\sigma} \xrightarrow{d} N(0, 1)
$$

或等价地（样本均值形式）：

$$
\frac{\overline{X}_n - \mu}{\sigma / \sqrt{n}} \xrightarrow{d} N(0, 1)
$$

即：

$$
\overline{X}_n \approx N\left(\mu, \frac{\sigma^2}{n}\right)
$$

**直观意义**：

1. 无论原分布是什么，大量独立随机变量的**和**（或均值）近似服从正态分布
2. 样本量越大，近似越好
3. 解释了为什么很多实际数据呈正态分布（由大量微小独立因素累加）

**例2**：设 $X_1, \ldots, X_{100} \sim U(0, 1)$ 独立，求 $S = \sum_{i=1}^{100} X_i$ 的近似分布。

**解**：

$$
E[X_i] = 0.5, \quad \text{Var}(X_i) = \frac{1}{12}
$$

$$
E[S] = 100 \times 0.5 = 50, \quad \text{Var}(S) = 100 \times \frac{1}{12} = \frac{25}{3}
$$

由中心极限定理：

$$
S \approx N\left(50, \frac{25}{3}\right)
$$

求 $P(45 < S < 55)$：

$$
\begin{aligned}
P(45 < S < 55) &\approx P\left(\frac{45-50}{\sqrt{25/3}} < Z < \frac{55-50}{\sqrt{25/3}}\right) \\
&= P\left(\frac{-5}{2.887} < Z < \frac{5}{2.887}\right) \\
&= P(-1.732 < Z < 1.732) \\
&\approx \Phi(1.732) - \Phi(-1.732) \\
&\approx 2\Phi(1.732) - 1 \approx 0.917
\end{aligned}
$$

#### 棣莫弗-拉普拉斯定理（二项分布的正态近似）

**定理**：设 $X \sim B(n, p)$，当 $n$ 很大时，

$$
\frac{X - np}{\sqrt{np(1-p)}} \approx N(0, 1)
$$

即：

$$
X \approx N(np, np(1-p))
$$

**应用条件**：$np \geq 5$ 且 $n(1-p) \geq 5$

**连续性修正**：由于二项分布是离散的，使用正态近似时应用连续性修正：

$$
P(X = k) \approx P(k - 0.5 < Y < k + 0.5)
$$

其中 $Y \sim N(np, np(1-p))$

**例3**：抛硬币1000次，求正面出现480到520次之间的概率。

**解**：

$X \sim B(1000, 0.5)$，$E[X] = 500$，$\text{Var}(X) = 250$

$$
\begin{aligned}
P(480 \leq X \leq 520) &\approx P(479.5 < Y < 520.5) \quad \text{（连续性修正）} \\
&= P\left(\frac{479.5 - 500}{\sqrt{250}} < Z < \frac{520.5 - 500}{\sqrt{250}}\right) \\
&= P(-1.296 < Z < 1.296) \\
&\approx 2\Phi(1.296) - 1 \approx 0.805
\end{aligned}
$$

#### 李雅普诺夫中心极限定理

**定理**：设 $X_1, X_2, \ldots$ 独立（不要求同分布），$E[X_i] = \mu_i$，$\text{Var}(X_i) = \sigma_i^2$

令 $B_n^2 = \sum_{i=1}^{n} \sigma_i^2$，若存在 $\delta > 0$ 使得

$$
\lim_{n \to \infty} \frac{1}{B_n^{2+\delta}} \sum_{i=1}^{n} E[|X_i - \mu_i|^{2+\delta}] = 0
$$

则：

$$
\frac{\sum_{i=1}^{n} X_i - \sum_{i=1}^{n} \mu_i}{B_n} \xrightarrow{d} N(0, 1)
$$

**意义**：不需要同分布，只要每个变量的贡献都相对较小

#### CLT的应用场景

1. **误差分析**：测量误差通常是大量微小独立误差的累积
2. **采样理论**：样本均值的分布
3. **质量控制**：生产过程中的质量指标
4. **金融**：资产组合收益率
5. **物理**：粒子的布朗运动

### 4.4 蒙特卡洛方法

蒙特卡洛方法基于大数定律，通过随机采样来近似计算。

#### 蒙特卡洛积分

**目标**：计算 $I = \int_a^b g(x) \, dx$

**方法**：
1. 在 $[a, b]$ 上均匀采样 $X_1, \ldots, X_n \sim U(a, b)$
2. 令 $Y_i = (b-a) g(X_i)$
3. 由大数定律：

$$
I \approx \frac{1}{n} \sum_{i=1}^{n} Y_i = \frac{b-a}{n} \sum_{i=1}^{n} g(X_i)
$$

**误差估计**：

标准误差为 $\text{SE} = \frac{\sigma}{\sqrt{n}}$，其中 $\sigma^2 = \text{Var}(Y_i)$

#### 重要性采样

当 $g(x)$ 在某些区域变化剧烈时，使用重要性采样提高效率。

**方法**：从密度 $h(x)$ 采样，计算

$$
I = \int_a^b \frac{g(x)}{h(x)} h(x) \, dx \approx \frac{1}{n} \sum_{i=1}^{n} \frac{g(X_i)}{h(X_i)}
$$

其中 $X_i \sim h(x)$

**最优选择**：$h(x) \propto |g(x)|$（使方差最小）

### 4.5 应用实例

#### 应用1：估计π

利用大数定律通过几何概率估计π。

```python
import numpy as np
import matplotlib.pyplot as plt

def monte_carlo_pi(n_samples=1000000, visualize=False):
    """
    用蒙特卡洛方法估计π
    基于大数定律：在正方形内随机投点，落在内切圆的概率 = π/4
    """
    # 在正方形[-1,1]×[-1,1]中随机采样
    x = np.random.uniform(-1, 1, n_samples)
    y = np.random.uniform(-1, 1, n_samples)
    
    # 计算落在单位圆内的点
    inside_circle = (x**2 + y**2) <= 1
    
    # π估计 = 4 × (圆内点数 / 总点数)
    pi_estimate = 4 * np.sum(inside_circle) / n_samples
    
    # 计算标准误差（基于二项分布）
    p = np.sum(inside_circle) / n_samples
    std_error = 4 * np.sqrt(p * (1-p) / n_samples)
    
    print(f"=== 蒙特卡洛估计 π ===")
    print(f"样本数: {n_samples}")
    print(f"π估计值: {pi_estimate:.6f}")
    print(f"真实值: {np.pi:.6f}")
    print(f"绝对误差: {abs(pi_estimate - np.pi):.6f}")
    print(f"相对误差: {abs(pi_estimate - np.pi)/np.pi * 100:.3f}%")
    print(f"95%置信区间: [{pi_estimate - 1.96*std_error:.6f}, {pi_estimate + 1.96*std_error:.6f}]")
    
    if visualize and n_samples <= 10000:
        plt.figure(figsize=(8, 8))
        inside = inside_circle
        outside = ~inside_circle
        plt.scatter(x[inside], y[inside], c='red', s=1, alpha=0.5, label='圆内')
        plt.scatter(x[outside], y[outside], c='blue', s=1, alpha=0.5, label='圆外')
        circle = plt.Circle((0, 0), 1, fill=False, color='black', linewidth=2)
        plt.gca().add_patch(circle)
        plt.xlim(-1, 1)
        plt.ylim(-1, 1)
        plt.gca().set_aspect('equal')
        plt.title(f'蒙特卡洛估计π (n={n_samples})')
        plt.legend()
        plt.grid(alpha=0.3)
        # plt.show()
    
    return pi_estimate

# pi_est = monte_carlo_pi(100000, visualize=True)  # 取消注释运行
```

#### 应用2：中心极限定理演示

可视化不同分布下的CLT效果。

```python
def clt_demonstration(dist='uniform', n_samples=100, n_trials=10000):
    """
    演示中心极限定理
    展示不同原分布的样本均值如何趋于正态分布
    """
    from scipy import stats
    
    sample_means = []
    
    for _ in range(n_trials):
        if dist == 'uniform':
            samples = np.random.uniform(0, 1, n_samples)
            mu, sigma = 0.5, 1/np.sqrt(12)
        elif dist == 'exponential':
            samples = np.random.exponential(1, n_samples)
            mu, sigma = 1, 1
        elif dist == 'binomial':
            samples = np.random.binomial(10, 0.5, n_samples)
            mu, sigma = 5, np.sqrt(2.5)
        elif dist == 'poisson':
            samples = np.random.poisson(5, n_samples)
            mu, sigma = 5, np.sqrt(5)
        else:
            raise ValueError(f"Unknown distribution: {dist}")
        
        sample_means.append(np.mean(samples))
    
    sample_means = np.array(sample_means)
    
    theoretical_mean = mu
    theoretical_std = sigma / np.sqrt(n_samples)
    
    # 可视化
    plt.figure(figsize=(15, 5))
    
    # 原分布的一个样本
    plt.subplot(1, 3, 1)
    if dist == 'uniform':
        sample = np.random.uniform(0, 1, 1000)
    elif dist == 'exponential':
        sample = np.random.exponential(1, 1000)
    elif dist == 'binomial':
        sample = np.random.binomial(10, 0.5, 1000)
    elif dist == 'poisson':
        sample = np.random.poisson(5, 1000)
    
    plt.hist(sample, bins=30, density=True, alpha=0.7, edgecolor='black')
    plt.title(f'原分布 ({dist})')
    plt.xlabel('X')
    plt.ylabel('密度/频率')
    plt.grid(alpha=0.3)
    
    # 样本均值的分布
    plt.subplot(1, 3, 2)
    plt.hist(sample_means, bins=50, density=True, alpha=0.7, label='样本均值分布', edgecolor='black')
    x = np.linspace(sample_means.min(), sample_means.max(), 100)
    plt.plot(x, stats.norm.pdf(x, theoretical_mean, theoretical_std), 
             'r-', linewidth=2, label='理论正态分布')
    plt.xlabel('样本均值')
    plt.ylabel('密度')
    plt.title(f'CLT演示 (n={n_samples})')
    plt.legend()
    plt.grid(alpha=0.3)
    
    # Q-Q图
    plt.subplot(1, 3, 3)
    stats.probplot(sample_means, dist="norm", plot=plt)
    plt.title('Q-Q图（正态性检验）')
    plt.grid(alpha=0.3)
    
    plt.tight_layout()
    # plt.show()
    
    print(f"=== 中心极限定理演示 ===")
    print(f"原分布: {dist}")
    print(f"样本量: {n_samples}")
    print(f"试验次数: {n_trials}")
    print(f"\n样本均值统计:")
    print(f"  均值: {np.mean(sample_means):.6f} (理论: {theoretical_mean:.6f})")
    print(f"  标准差: {np.std(sample_means):.6f} (理论: {theoretical_std:.6f})")
    
    # Shapiro-Wilk正态性检验
    from scipy.stats import shapiro
    stat, p_value = shapiro(sample_means[:5000])  # 最多5000个样本
    print(f"\nShapiro-Wilk正态性检验:")
    print(f"  统计量: {stat:.6f}")
    print(f"  p值: {p_value:.6f}")
    print(f"  结论: {'接受正态性假设' if p_value > 0.05 else '拒绝正态性假设'}")

# clt_demonstration('exponential', n_samples=100)  # 取消注释运行
```

#### 应用3：蒙特卡洛积分

计算复杂积分。

```python
def monte_carlo_integration(func, a, b, n_samples=100000):
    """
    蒙特卡洛积分
    估计 ∫_a^b f(x) dx
    """
    # 均匀采样
    x = np.random.uniform(a, b, n_samples)
    
    # 函数值
    y = func(x)
    
    # 积分估计
    integral_estimate = (b - a) * np.mean(y)
    
    # 标准误差
    std_error = (b - a) * np.std(y) / np.sqrt(n_samples)
    
    print(f"=== 蒙特卡洛积分 ===")
    print(f"积分区间: [{a}, {b}]")
    print(f"样本数: {n_samples}")
    print(f"积分估计: {integral_estimate:.6f}")
    print(f"标准误差: {std_error:.6f}")
    print(f"95%置信区间: [{integral_estimate - 1.96*std_error:.6f}, {integral_estimate + 1.96*std_error:.6f}]")
    
    return integral_estimate, std_error

# 示例：计算 ∫_0^1 x^2 dx (真实值 = 1/3)
# def f(x):
#     return x**2
# 
# integral, error = monte_carlo_integration(f, 0, 1, n_samples=100000)
# print(f"真实值: {1/3:.6f}")
# print(f"误差: {abs(integral - 1/3):.6f}")
```

### 4.6 本章小结

#### 核心概念

1. **收敛性类型**：
   - 依概率收敛 $\xrightarrow{P}$
   - 依分布收敛 $\xrightarrow{d}$
   - 几乎必然收敛 $\xrightarrow{a.s.}$
   - 均方收敛 $\xrightarrow{L^2}$

2. **大数定律**：$\frac{1}{n}\sum X_i \xrightarrow{P} \mu$
   - 样本均值趋于总体均值
   - 频率趋于概率

3. **中心极限定理**：$\frac{\sum X_i - n\mu}{\sqrt{n}\sigma} \xrightarrow{d} N(0, 1)$
   - 大量独立随机变量的和近似正态
   - 解释正态分布的普遍性

#### 重要定理

**切比雪夫不等式**：

$$
P(|X - \mu| \geq \epsilon) \leq \frac{\sigma^2}{\epsilon^2}
$$

**中心极限定理（样本均值形式）**：

$$
\overline{X}_n \approx N\left(\mu, \frac{\sigma^2}{n}\right)
$$

**二项分布的正态近似**：

$$
B(n, p) \approx N(np, np(1-p)) \quad (n \text{大})
$$

#### 实际应用

1. **蒙特卡洛方法**：数值积分、概率估计
2. **统计推断**：置信区间、假设检验的理论基础
3. **质量控制**：样本均值的抽样分布
4. **风险管理**：VaR（风险价值）计算

#### 易错点

1. **CLT的前提**：独立同分布，且方差有限
2. **收敛速度**：$O(1/\sqrt{n})$，样本量增加4倍，误差才减半
3. **正态近似的条件**：二项分布需要 $np \geq 5$ 且 $n(1-p) \geq 5$
4. **连续性修正**：离散分布用正态近似时需要修正

---

**本章完**
