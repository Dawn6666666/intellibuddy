# 概率论与数理统计 - (九)统计分析实战

完整的统计分析项目案例，涵盖数据导入、清洗、探索、建模、诊断、可视化和报告撰写全流程。

---

## 9. 统计分析实战

### 📌 本章结构

| 项目 | 内容 | 技能点 |
|------|------|--------|
| **9.1 完整案例：房价预测** | 从原始数据到最终模型 | 回归、诊断、特征工程 |
| **9.2 数据清洗实战** | 缺失值、异常值、重复值处理 | Pandas高级技巧 |
| **9.3 探索性数据分析（EDA）** | 描述统计、可视化、关系探索 | Matplotlib/Seaborn |
| **9.4 A/B测试完整流程** | 实验设计到结论报告 | 假设检验、样本量计算 |
| **9.5 时间序列分析** | ARIMA建模与预测 | 平稳性检验、ACF/PACF |
| **9.6 分类问题：客户流失预测** | Logistic回归、模型评估 | 混淆矩阵、ROC曲线 |
| **9.7 分析报告撰写** | 专业报告模板 | 商业分析、可视化设计 |

---

## 9.1 完整案例：房价预测分析

### 项目背景

**目标**：建立房价预测模型，分析影响房价的关键因素

**数据集**：包含500套房产的13个特征
- 面积、卧室数、浴室数、楼层、车位
- 建造年份、学区评分、距市中心距离、犯罪率等

### Step 1：数据导入与初探

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# 设置中文字体和风格
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")

# 生成模拟数据集（实际项目中用pd.read_csv()）
np.random.seed(42)
n = 500

data = pd.DataFrame({
    '面积': np.random.normal(120, 40, n).clip(50, 300),
    '卧室数': np.random.choice([1, 2, 3, 4, 5], n, p=[0.1, 0.3, 0.35, 0.2, 0.05]),
    '浴室数': np.random.choice([1, 2, 3], n, p=[0.3, 0.5, 0.2]),
    '楼层': np.random.choice(range(1, 31), n),
    '车位': np.random.choice([0, 1, 2], n, p=[0.2, 0.5, 0.3]),
    '建造年份': np.random.choice(range(1980, 2024), n),
    '学区评分': np.random.normal(7.5, 1.5, n).clip(1, 10),
    '距市中心(km)': np.random.exponential(10, n).clip(0, 50),
    '犯罪率': np.random.gamma(2, 2, n).clip(0, 20),
    '绿化率': np.random.beta(5, 2, n) * 100
})

# 生成房价（基于多个特征的线性组合+噪声）
data['房价(万)'] = (
    50 +
    data['面积'] * 0.5 +
    data['卧室数'] * 15 +
    data['浴室数'] * 10 +
    data['楼层'] * 0.3 +
    data['车位'] * 20 +
    (2024 - data['建造年份']) * -0.5 +
    data['学区评分'] * 8 +
    data['距市中心(km)'] * -1.5 +
    data['犯罪率'] * -2 +
    data['绿化率'] * 0.5 +
    np.random.normal(0, 30, n)
).clip(100, 1000)

# 引入缺失值和异常值（模拟真实数据）
missing_idx = np.random.choice(n, 20, replace=False)
data.loc[missing_idx[:10], '绿化率'] = np.nan
data.loc[missing_idx[10:], '学区评分'] = np.nan

# 异常值（录入错误）
data.loc[np.random.choice(n, 3, replace=False), '房价(万)'] *= 3

print("=" * 70)
print("【1. 数据概览】")
print("=" * 70)
print(f"\n数据集形状: {data.shape[0]} 行 × {data.shape[1]} 列")
print("\n前5行数据:")
print(data.head())

print("\n数据类型:")
print(data.dtypes)

print("\n基本统计信息:")
print(data.describe())

print("\n缺失值统计:")
missing = data.isnull().sum()
missing_pct = 100 * missing / len(data)
missing_df = pd.DataFrame({
    '缺失数': missing[missing > 0],
    '缺失率(%)': missing_pct[missing > 0]
})
print(missing_df)
```

**输出示例**：
```
======================================================================
【1. 数据概览】
======================================================================

数据集形状: 500 行 × 11 列

前5行数据:
          面积  卧室数  浴室数  楼层  车位  建造年份   学区评分  距市中心(km)    犯罪率      绿化率   房价(万)
0   149.87     3      2     7   1     2003      8.22       8.93      3.45     75.31    312.56
1    99.46     2      2    15   1     1995      6.84      12.45      5.67     68.92    245.78
...

缺失值统计:
           缺失数  缺失率(%)
学区评分      10      2.00
绿化率        10      2.00
```

---

### Step 2：数据清洗

```python
print("\n" + "=" * 70)
print("【2. 数据清洗】")
print("=" * 70)

# 2.1 处理缺失值
print("\n2.1 缺失值处理策略:")

# 绿化率：用中位数填充（稳健）
median_green = data['绿化率'].median()
data['绿化率'].fillna(median_green, inplace=True)
print(f"  - 绿化率：填充中位数 {median_green:.2f}")

# 学区评分：用均值填充（正态分布）
mean_school = data['学区评分'].mean()
data['学区评分'].fillna(mean_school, inplace=True)
print(f"  - 学区评分：填充均值 {mean_school:.2f}")

# 2.2 异常值检测与处理
print("\n2.2 异常值检测（IQR方法）:")

def detect_outliers_iqr(series, name):
    Q1 = series.quantile(0.25)
    Q3 = series.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = series[(series < lower_bound) | (series > upper_bound)]
    print(f"  - {name}: 发现 {len(outliers)} 个异常值")
    print(f"    正常范围: [{lower_bound:.2f}, {upper_bound:.2f}]")
    return lower_bound, upper_bound

# 检测房价异常值
lower, upper = detect_outliers_iqr(data['房价(万)'], '房价')

# 可视化异常值
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# 箱线图
axes[0].boxplot(data['房价(万)'], vert=True, patch_artist=True)
axes[0].set_ylabel('房价(万)')
axes[0].set_title('房价箱线图（异常值检测）')
axes[0].axhline(upper, color='r', linestyle='--', label=f'上界={upper:.0f}')
axes[0].axhline(lower, color='r', linestyle='--', label=f'下界={lower:.0f}')
axes[0].legend()
axes[0].grid(alpha=0.3)

# 直方图
axes[1].hist(data['房价(万)'], bins=30, edgecolor='black', alpha=0.7)
axes[1].axvline(upper, color='r', linestyle='--', linewidth=2, label='上界')
axes[1].set_xlabel('房价(万)')
axes[1].set_ylabel('频数')
axes[1].set_title('房价分布直方图')
axes[1].legend()
axes[1].grid(alpha=0.3)

plt.tight_layout()
# plt.savefig('outlier_detection.png', dpi=300, bbox_inches='tight')
plt.show()

# 处理异常值：替换为上界（Winsorizing）
outlier_mask = (data['房价(万)'] < lower) | (data['房价(万)'] > upper)
n_outliers = outlier_mask.sum()
data.loc[data['房价(万)'] > upper, '房价(万)'] = upper
data.loc[data['房价(万)'] < lower, '房价(万)'] = lower

print(f"\n  ✓ 处理方法：Winsorizing（截尾），共处理 {n_outliers} 个异常值")

# 2.3 数据类型优化
print("\n2.3 数据类型优化:")
data['卧室数'] = data['卧室数'].astype('int8')
data['浴室数'] = data['浴室数'].astype('int8')
data['车位'] = data['车位'].astype('int8')
data['楼层'] = data['楼层'].astype('int8')
print(f"  ✓ 离散变量转换为int8，节省内存")

print(f"\n✅ 清洗后数据集: {data.shape[0]} 行 × {data.shape[1]} 列，无缺失值")
```

---

### Step 3：探索性数据分析（EDA）

```python
print("\n" + "=" * 70)
print("【3. 探索性数据分析（EDA）】")
print("=" * 70)

# 3.1 目标变量分布
print("\n3.1 目标变量（房价）分析:")

fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# 直方图
axes[0, 0].hist(data['房价(万)'], bins=40, edgecolor='black', alpha=0.7, color='skyblue')
axes[0, 0].set_xlabel('房价(万)')
axes[0, 0].set_ylabel('频数')
axes[0, 0].set_title('房价分布直方图')
axes[0, 0].axvline(data['房价(万)'].mean(), color='red', linestyle='--', 
                   label=f'均值={data["房价(万)"].mean():.1f}')
axes[0, 0].axvline(data['房价(万)'].median(), color='green', linestyle='--', 
                   label=f'中位数={data["房价(万)"].median():.1f}')
axes[0, 0].legend()
axes[0, 0].grid(alpha=0.3)

# Q-Q图（正态性检验）
stats.probplot(data['房价(万)'], dist="norm", plot=axes[0, 1])
axes[0, 1].set_title('Q-Q图（正态性检验）')
axes[0, 1].grid(alpha=0.3)

# 核密度估计
data['房价(万)'].plot(kind='kde', ax=axes[1, 0], linewidth=2)
axes[1, 0].set_xlabel('房价(万)')
axes[1, 0].set_ylabel('密度')
axes[1, 0].set_title('房价核密度估计')
axes[1, 0].grid(alpha=0.3)

# 描述统计
desc_stats = data['房价(万)'].describe()
table_data = [[k, f'{v:.2f}'] for k, v in desc_stats.items()]
table_data.append(['偏度', f'{data["房价(万)"].skew():.3f}'])
table_data.append(['峰度', f'{data["房价(万)"].kurt():.3f}'])

axes[1, 1].axis('off')
table = axes[1, 1].table(cellText=table_data, colLabels=['统计量', '值'],
                        loc='center', cellLoc='center')
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1, 2)
axes[1, 1].set_title('描述统计')

plt.tight_layout()
plt.show()

# Shapiro-Wilk正态性检验
stat, p_value = stats.shapiro(data['房价(万)'])
print(f"  Shapiro-Wilk检验: 统计量={stat:.4f}, p值={p_value:.6f}")
if p_value > 0.05:
    print("  ✓ 房价近似服从正态分布（p > 0.05）")
else:
    print("  ✗ 房价偏离正态分布（p < 0.05），考虑对数变换")

# 3.2 特征相关性分析
print("\n3.2 特征相关性分析:")

# 计算相关系数矩阵
corr_matrix = data.corr()

# 热力图
plt.figure(figsize=(12, 10))
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))  # 上三角遮罩
sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r',
            center=0, square=True, linewidths=1, cbar_kws={"shrink": 0.8})
plt.title('特征相关性热力图', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()

# 与房价相关性排序
price_corr = corr_matrix['房价(万)'].drop('房价(万)').sort_values(ascending=False)
print("\n与房价相关性排序:")
print(price_corr)

# 高相关特征可视化
top_features = price_corr.head(4).index.tolist()

fig, axes = plt.subplots(2, 2, figsize=(14, 10))
axes = axes.ravel()

for i, feat in enumerate(top_features):
    axes[i].scatter(data[feat], data['房价(万)'], alpha=0.5, edgecolors='k', linewidth=0.3)
    
    # 拟合回归线
    z = np.polyfit(data[feat], data['房价(万)'], 1)
    p = np.poly1d(z)
    axes[i].plot(data[feat], p(data[feat]), "r--", linewidth=2)
    
    # 显示相关系数
    r = price_corr[feat]
    axes[i].text(0.05, 0.95, f'r = {r:.3f}', transform=axes[i].transAxes,
                fontsize=12, verticalalignment='top', 
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
    
    axes[i].set_xlabel(feat)
    axes[i].set_ylabel('房价(万)')
    axes[i].set_title(f'{feat} vs 房价')
    axes[i].grid(alpha=0.3)

plt.tight_layout()
plt.show()

# 3.3 分类变量分析
print("\n3.3 分类变量分析:")

categorical_vars = ['卧室数', '浴室数', '车位']

fig, axes = plt.subplots(1, 3, figsize=(16, 5))

for i, var in enumerate(categorical_vars):
    # 按类别统计房价均值和标准差
    grouped = data.groupby(var)['房价(万)'].agg(['mean', 'std', 'count'])
    
    # 条形图（带误差线）
    x = grouped.index
    y = grouped['mean']
    yerr = grouped['std']
    
    axes[i].bar(x, y, yerr=yerr, capsize=5, alpha=0.7, edgecolor='black')
    axes[i].set_xlabel(var)
    axes[i].set_ylabel('平均房价(万)')
    axes[i].set_title(f'{var}与房价关系')
    axes[i].grid(axis='y', alpha=0.3)
    
    # 显示样本量
    for j, (idx, row) in enumerate(grouped.iterrows()):
        axes[i].text(idx, row['mean'] + row['std'] + 5, 
                    f"n={int(row['count'])}", 
                    ha='center', fontsize=9)

plt.tight_layout()
plt.show()
```

---

### Step 4：模型构建与评估

```python
print("\n" + "=" * 70)
print("【4. 多元线性回归建模】")
print("=" * 70)

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from statsmodels.stats.outliers_influence import variance_inflation_factor
import statsmodels.api as sm

# 4.1 划分数据集
X = data.drop('房价(万)', axis=1)
y = data['房价(万)']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print(f"\n训练集: {X_train.shape[0]} 样本")
print(f"测试集: {X_test.shape[0]} 样本")

# 4.2 使用statsmodels进行详细分析
X_train_sm = sm.add_constant(X_train)
model = sm.OLS(y_train, X_train_sm).fit()

print("\n" + "=" * 70)
print("【回归模型摘要】")
print("=" * 70)
print(model.summary())

# 4.3 VIF检测多重共线性
print("\n" + "=" * 70)
print("【多重共线性检测（VIF）】")
print("=" * 70)

vif_data = pd.DataFrame()
vif_data["特征"] = X_train.columns
vif_data["VIF"] = [variance_inflation_factor(X_train.values, i) 
                   for i in range(X_train.shape[1])]
vif_data = vif_data.sort_values('VIF', ascending=False)

print(vif_data.to_string(index=False))

if vif_data['VIF'].max() > 10:
    print("\n⚠️  存在严重多重共线性 (VIF > 10)，考虑删除或合并变量")
elif vif_data['VIF'].max() > 5:
    print("\n⚠️  存在中等多重共线性 (5 < VIF < 10)")
else:
    print("\n✅ 无严重多重共线性 (VIF < 5)")

# 4.4 模型预测
X_test_sm = sm.add_constant(X_test)
y_pred = model.predict(X_test_sm)

# 4.5 性能评估
print("\n" + "=" * 70)
print("【模型性能评估】")
print("=" * 70)

mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100

print(f"\n测试集表现:")
print(f"  R²         = {r2:.4f}  （解释了{r2*100:.1f}%的方差）")
print(f"  RMSE       = {rmse:.2f} 万")
print(f"  MAE        = {mae:.2f} 万")
print(f"  MAPE       = {mape:.2f}%")

# 可视化：实际vs预测
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# 散点图
axes[0].scatter(y_test, y_pred, alpha=0.6, edgecolors='k', linewidth=0.5)
min_val = min(y_test.min(), y_pred.min())
max_val = max(y_test.max(), y_pred.max())
axes[0].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, 
             label='理想拟合线')
axes[0].set_xlabel('实际房价(万)')
axes[0].set_ylabel('预测房价(万)')
axes[0].set_title(f'实际 vs 预测 (R²={r2:.3f})')
axes[0].legend()
axes[0].grid(alpha=0.3)

# 残差图
residuals = y_test - y_pred
axes[1].scatter(y_pred, residuals, alpha=0.6, edgecolors='k', linewidth=0.5)
axes[1].axhline(0, color='red', linestyle='--', linewidth=2)
axes[1].set_xlabel('预测房价(万)')
axes[1].set_ylabel('残差')
axes[1].set_title('残差图')
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()
```

---

### Step 5：残差诊断

```python
print("\n" + "=" * 70)
print("【5. 残差诊断】")
print("=" * 70)

residuals_train = model.resid
fitted_values = model.fittedvalues

fig = plt.figure(figsize=(16, 12))
gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)

# (1) 残差 vs 拟合值
ax1 = fig.add_subplot(gs[0, 0])
ax1.scatter(fitted_values, residuals_train, alpha=0.5, edgecolors='k', linewidth=0.3)
ax1.axhline(0, color='red', linestyle='--', linewidth=2)
ax1.set_xlabel('拟合值')
ax1.set_ylabel('残差')
ax1.set_title('残差 vs 拟合值')
ax1.grid(alpha=0.3)

# (2) Q-Q图
ax2 = fig.add_subplot(gs[0, 1])
stats.probplot(residuals_train, dist="norm", plot=ax2)
ax2.set_title('正态Q-Q图')
ax2.grid(alpha=0.3)

# (3) Scale-Location图
ax3 = fig.add_subplot(gs[0, 2])
std_resid = residuals_train / np.std(residuals_train)
ax3.scatter(fitted_values, np.sqrt(np.abs(std_resid)), alpha=0.5, 
           edgecolors='k', linewidth=0.3)
ax3.set_xlabel('拟合值')
ax3.set_ylabel('√|标准化残差|')
ax3.set_title('Scale-Location图')
ax3.grid(alpha=0.3)

# (4) 残差直方图
ax4 = fig.add_subplot(gs[1, 0])
ax4.hist(residuals_train, bins=30, edgecolor='black', alpha=0.7, density=True)
xmin, xmax = ax4.get_xlim()
x = np.linspace(xmin, xmax, 100)
ax4.plot(x, stats.norm.pdf(x, 0, np.std(residuals_train)), 'r-', 
        linewidth=2, label='理论正态分布')
ax4.set_xlabel('残差')
ax4.set_ylabel('密度')
ax4.set_title('残差分布')
ax4.legend()
ax4.grid(alpha=0.3)

# (5) Cook's Distance
ax5 = fig.add_subplot(gs[1, 1])
influence = model.get_influence()
cooks_d = influence.cooks_distance[0]
ax5.stem(range(len(cooks_d)), cooks_d, basefmt=" ", markerfmt='o')
threshold = 4 / len(X_train)
ax5.axhline(threshold, color='red', linestyle='--', linewidth=2, 
           label=f'阈值={threshold:.4f}')
ax5.set_xlabel('观测序号')
ax5.set_ylabel("Cook's Distance")
ax5.set_title("影响点诊断")
ax5.legend()
ax5.grid(alpha=0.3)

# (6) 杠杆值 vs 标准化残差
ax6 = fig.add_subplot(gs[1, 2])
leverage = influence.hat_matrix_diag
ax6.scatter(leverage, std_resid, alpha=0.5, edgecolors='k', linewidth=0.3)
ax6.axhline(0, color='gray', linestyle='-', linewidth=1)
ax6.axhline(2, color='red', linestyle='--', linewidth=1.5, label='±2σ')
ax6.axhline(-2, color='red', linestyle='--', linewidth=1.5)
ax6.axvline(2 * (X_train.shape[1] + 1) / len(X_train), color='orange', 
           linestyle='--', linewidth=1.5, label='高杠杆')
ax6.set_xlabel('杠杆值')
ax6.set_ylabel('标准化残差')
ax6.set_title('杠杆值 vs 标准化残差')
ax6.legend()
ax6.grid(alpha=0.3)

# (7) 异方差检验结果
ax7 = fig.add_subplot(gs[2, :])
ax7.axis('off')

# Breusch-Pagan检验
from statsmodels.stats.diagnostic import het_breuschpagan
bp_stat, bp_pval, _, _ = het_breuschpagan(residuals_train, X_train_sm)

# Shapiro-Wilk检验
sw_stat, sw_pval = stats.shapiro(residuals_train)

# Durbin-Watson检验
from statsmodels.stats.stattools import durbin_watson
dw_stat = durbin_watson(residuals_train)

diagnostic_text = f"""
【诊断检验结果】

1. 正态性检验 (Shapiro-Wilk):
   - 统计量 = {sw_stat:.4f}
   - p值 = {sw_pval:.6f}
   - 结论: {'✓ 残差近似正态分布' if sw_pval > 0.05 else '✗ 残差偏离正态分布'}

2. 同方差性检验 (Breusch-Pagan):
   - 统计量 = {bp_stat:.4f}
   - p值 = {bp_pval:.6f}
   - 结论: {'✓ 满足同方差假设' if bp_pval > 0.05 else '✗ 存在异方差'}

3. 独立性检验 (Durbin-Watson):
   - 统计量 = {dw_stat:.4f}
   - 结论: {'✓ 无明显自相关' if 1.5 < dw_stat < 2.5 else '⚠ 可能存在自相关'}
   - 注：DW≈2表示无自相关，<2表示正相关，>2表示负相关

4. 影响点:
   - 高Cook's D (>{threshold:.4f}): {np.sum(cooks_d > threshold)} 个
   - 高杠杆点: {np.sum(leverage > 2*(X_train.shape[1]+1)/len(X_train))} 个
   - 异常值 (|std_resid|>2): {np.sum(np.abs(std_resid) > 2)} 个
"""

ax7.text(0.05, 0.95, diagnostic_text, transform=ax7.transAxes,
        fontsize=11, verticalalignment='top', fontfamily='monospace',
        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

plt.suptitle('回归诊断图集', fontsize=16, fontweight='bold', y=0.995)
plt.show()

print(diagnostic_text)
```

---

### Step 6：特征重要性分析

```python
print("\n" + "=" * 70)
print("【6. 特征重要性分析】")
print("=" * 70)

# 提取系数（排除常数项）
coefficients = model.params.drop('const')
p_values = model.pvalues.drop('const')
conf_int = model.conf_int().drop('const')

# 创建特征重要性DataFrame
feature_importance = pd.DataFrame({
    '特征': coefficients.index,
    '系数': coefficients.values,
    'p值': p_values.values,
    '显著': ['***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else '' 
            for p in p_values.values],
    'CI下限': conf_int[0].values,
    'CI上限': conf_int[1].values
})

# 按系数绝对值排序
feature_importance['|系数|'] = np.abs(feature_importance['系数'])
feature_importance = feature_importance.sort_values('|系数|', ascending=False)

print("\n特征重要性排序（按系数绝对值）:")
print(feature_importance[['特征', '系数', 'p值', '显著', 'CI下限', 'CI上限']].to_string(index=False))

# 可视化特征重要性
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# 系数图（带置信区间）
y_pos = np.arange(len(feature_importance))
coeffs = feature_importance['系数'].values
errors = feature_importance['CI上限'].values - coeffs

colors = ['green' if p < 0.05 else 'gray' for p in feature_importance['p值']]

axes[0].barh(y_pos, coeffs, xerr=errors, color=colors, alpha=0.7, 
            capsize=5, edgecolor='black')
axes[0].set_yticks(y_pos)
axes[0].set_yticklabels(feature_importance['特征'])
axes[0].axvline(0, color='black', linestyle='-', linewidth=1)
axes[0].set_xlabel('回归系数（±95% CI）')
axes[0].set_title('特征系数及置信区间\n（绿色=显著，灰色=不显著）')
axes[0].grid(alpha=0.3, axis='x')

# p值图（-log10 scale）
neg_log_p = -np.log10(feature_importance['p值'] + 1e-10)
axes[1].barh(y_pos, neg_log_p, color=colors, alpha=0.7, edgecolor='black')
axes[1].axvline(-np.log10(0.05), color='red', linestyle='--', linewidth=2, 
               label='α=0.05')
axes[1].axvline(-np.log10(0.01), color='orange', linestyle='--', linewidth=2, 
               label='α=0.01')
axes[1].set_yticks(y_pos)
axes[1].set_yticklabels(feature_importance['特征'])
axes[1].set_xlabel('-log₁₀(p值)')
axes[1].set_title('特征显著性（值越大越显著）')
axes[1].legend()
axes[1].grid(alpha=0.3, axis='x')

plt.tight_layout()
plt.show()

# 关键发现
print("\n【关键发现】:")
significant_features = feature_importance[feature_importance['p值'] < 0.05]
print(f"\n✅ 显著特征 ({len(significant_features)} 个):")
for idx, row in significant_features.iterrows():
    direction = "正向" if row['系数'] > 0 else "负向"
    print(f"  - {row['特征']}: {direction}影响，系数={row['系数']:.3f} {row['显著']}")

non_significant = feature_importance[feature_importance['p值'] >= 0.05]
if len(non_significant) > 0:
    print(f"\n⚠️  不显著特征 ({len(non_significant)} 个):")
    for idx, row in non_significant.iterrows():
        print(f"  - {row['特征']}: p={row['p值']:.4f}")
```

---

### Step 7：业务解释与建议

```python
print("\n" + "=" * 70)
print("【7. 业务洞察与建议】")
print("=" * 70)

report = f"""
## 房价预测模型分析报告

### 一、模型表现总结

- **模型类型**: 多元线性回归
- **样本量**: {len(data)} 套房产（训练集{len(X_train)}，测试集{len(X_test)}）
- **R²**: {r2:.3f} （解释了{r2*100:.1f}%的房价变异）
- **RMSE**: {rmse:.2f}万 （平均预测误差）
- **MAPE**: {mape:.2f}% （平均百分比误差）

### 二、影响房价的关键因素（按重要性排序）

{chr(10).join([f"{i+1}. **{row['特征']}**: {'+' if row['系数']>0 else ''}{row['系数']:.2f}万/单位 {'（显著）' if row['p值']<0.05 else '（不显著）'}"
              for i, (idx, row) in enumerate(feature_importance.head(5).iterrows())])}

### 三、业务建议

#### 对购房者:
1. **面积**是最重要的定价因素，每平米约增加{feature_importance[feature_importance['特征']=='面积']['系数'].values[0]:.2f}万
2. **学区评分**显著影响房价，优质学区溢价明显
3. **距市中心距离**负相关，市中心房产具有位置溢价

#### 对开发商:
1. 重点投资学区房项目
2. 增加车位配套设施可提升房价
3. 控制犯罪率，提升社区安全感

#### 对政策制定者:
1. 加强学区资源均衡化，减少学区房炒作
2. 完善公共交通，降低距离对房价的负面影响
3. 提高社区绿化率和安全水平

### 四、模型局限性

1. **线性假设**: 实际关系可能非线性（考虑多项式或交互项）
2. **缺失变量**: 未考虑装修、朝向、配套设施等因素
3. **时间效应**: 未纳入市场周期、政策变化等时间因素
4. **异方差性**: {
   '检测到异方差，建议使用稳健标准误' if bp_pval < 0.05 
   else '未检测到显著异方差'
}

### 五、下一步工作

1. 收集更多特征数据（装修、朝向、楼盘品牌等）
2. 尝试非线性模型（随机森林、XGBoost、神经网络）
3. 进行时间序列分析，预测房价趋势
4. 细分市场（按区域、户型建立分层模型）
"""

print(report)

# 保存报告
# with open('房价预测分析报告.txt', 'w', encoding='utf-8') as f:
#     f.write(report)

print("\n✅ 完整分析流程结束！")
```

---

## 9.2 A/B测试完整流程

### 场景：电商推荐算法优化

**背景**：某电商平台开发了新的推荐算法（B版本），希望与当前算法（A版本）对比，看是否能提高点击率。

### Step 1：实验设计

```python
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns

print("=" * 70)
print("【A/B测试完整流程】")
print("=" * 70)

# 实验参数
alpha = 0.05  # 显著性水平
power = 0.8   # 统计功效
p_A = 0.05    # A组基准转化率
mde = 0.01    # 最小可检测效应（Minimum Detectable Effect）

print(f"\n实验设计:")
print(f"  - 显著性水平 α = {alpha}")
print(f"  - 统计功效 1-β = {power}")
print(f"  - A组基准转化率 = {p_A:.1%}")
print(f"  - 最小可检测效应 = {mde:.1%}")

# 样本量计算（双样本比例检验）
from statsmodels.stats.power import zt_ind_solve_power

effect_size = (p_A + mde - p_A) / np.sqrt(p_A * (1 - p_A))
n_required = zt_ind_solve_power(
    effect_size=effect_size,
    alpha=alpha,
    power=power,
    ratio=1.0,  # 1:1分配
    alternative='two-sided'
)

print(f"\n所需样本量: 每组至少 {int(np.ceil(n_required))} 人")
print(f"总样本量: {int(np.ceil(n_required * 2))} 人")

### Step 2：数据收集与初步分析

# 模拟实验数据
np.random.seed(42)
n_A, n_B = 10000, 10000

# A组：基准转化率5%
clicks_A = np.random.binomial(n_A, p_A)

# B组：转化率5.5%（真实提升0.5个百分点）
p_B_true = 0.055
clicks_B = np.random.binomial(n_B, p_B_true)

p_A_obs = clicks_A / n_A
p_B_obs = clicks_B / n_B

print("\n" + "=" * 70)
print("【数据收集结果】")
print("=" * 70)

results = pd.DataFrame({
    '组别': ['A组（对照组）', 'B组（实验组）'],
    '样本量': [n_A, n_B],
    '点击数': [clicks_A, clicks_B],
    '点击率': [p_A_obs, p_B_obs],
    '点击率(%)': [p_A_obs * 100, p_B_obs * 100]
})

print("\n" + results.to_string(index=False))

print(f"\n提升情况:")
absolute_lift = p_B_obs - p_A_obs
relative_lift = (p_B_obs - p_A_obs) / p_A_obs * 100
print(f"  绝对提升: {absolute_lift:.4f} ({absolute_lift*100:.2f}个百分点)")
print(f"  相对提升: {relative_lift:.2f}%")

### Step 3：假设检验

print("\n" + "=" * 70)
print("【假设检验】")
print("=" * 70)

# H0: p_A = p_B
# H1: p_A ≠ p_B

# 合并比例
p_pool = (clicks_A + clicks_B) / (n_A + n_B)

# Z检验统计量
se = np.sqrt(p_pool * (1 - p_pool) * (1/n_A + 1/n_B))
z = (p_B_obs - p_A_obs) / se

# p值（双侧）
p_value = 2 * (1 - stats.norm.cdf(abs(z)))

print(f"\n检验结果:")
print(f"  合并比例 p_pool = {p_pool:.4f}")
print(f"  Z统计量 = {z:.4f}")
print(f"  p值 = {p_value:.6f}")
print(f"  临界值 Z_{alpha/2} = {stats.norm.ppf(1-alpha/2):.4f}")

if p_value < alpha:
    print(f"\n✅ 拒绝H0 (p < {alpha})")
    print(f"   结论: B组转化率**显著高于**A组")
else:
    print(f"\n❌ 不能拒绝H0 (p ≥ {alpha})")
    print(f"   结论: 两组转化率**无显著差异**")

# 置信区间
ci_lower, ci_upper = stats.norm.interval(
    1 - alpha,
    loc=p_B_obs - p_A_obs,
    scale=np.sqrt(p_A_obs*(1-p_A_obs)/n_A + p_B_obs*(1-p_B_obs)/n_B)
)

print(f"\n差异的95%置信区间: [{ci_lower:.4f}, {ci_upper:.4f}]")
print(f"或: [{ci_lower*100:.2f}%, {ci_upper*100:.2f}%]")

### Step 4：可视化

fig, axes = plt.subplots(1, 3, figsize=(16, 5))

# (1) 转化率对比
groups = ['A组\n（对照组）', 'B组\n（实验组）']
rates = [p_A_obs * 100, p_B_obs * 100]
colors = ['#3498db', '#e74c3c']

bars = axes[0].bar(groups, rates, color=colors, alpha=0.7, edgecolor='black', linewidth=2)
axes[0].set_ylabel('点击率 (%)')
axes[0].set_title('A/B组点击率对比', fontweight='bold')
axes[0].grid(axis='y', alpha=0.3)

# 添加数值标签
for bar, rate in zip(bars, rates):
    height = bar.get_height()
    axes[0].text(bar.get_x() + bar.get_width()/2., height + 0.05,
                f'{rate:.2f}%\n(n={n_A})',
                ha='center', va='bottom', fontweight='bold', fontsize=11)

# (2) 置信区间可视化
diff = p_B_obs - p_A_obs
error = (ci_upper - ci_lower) / 2

axes[1].barh(['差异'], [diff * 100], xerr=[error * 100],
            capsize=10, color='green', alpha=0.7, edgecolor='black', linewidth=2)
axes[1].axvline(0, color='red', linestyle='--', linewidth=2, label='无差异线')
axes[1].set_xlabel('点击率差异 (百分点)')
axes[1].set_title(f'B-A差异及95%置信区间\np={p_value:.4f}', fontweight='bold')
axes[1].legend()
axes[1].grid(alpha=0.3, axis='x')

# (3) 统计功效曲线
effect_sizes = np.linspace(-0.02, 0.02, 100)
z_stats = effect_sizes / se

# 双侧检验的功效
power_curve = (
    stats.norm.cdf(z_stats - stats.norm.ppf(1-alpha/2)) +
    stats.norm.cdf(-z_stats - stats.norm.ppf(1-alpha/2))
)

axes[2].plot(effect_sizes * 100, power_curve, linewidth=2, label='功效曲线')
axes[2].axhline(0.8, color='orange', linestyle='--', linewidth=2, label='目标功效=0.8')
axes[2].axvline(0, color='red', linestyle='--', linewidth=1.5, label='H0')
axes[2].axvline(mde * 100, color='green', linestyle='--', linewidth=1.5, label=f'MDE={mde*100:.1f}%')
axes[2].scatter([absolute_lift * 100], [power], s=200, color='red', zorder=5, 
               label=f'观测值 (功效={power:.2f})')
axes[2].set_xlabel('真实效应 (百分点)')
axes[2].set_ylabel('统计功效 (1-β)')
axes[2].set_title('统计功效分析', fontweight='bold')
axes[2].legend()
axes[2].grid(alpha=0.3)

plt.tight_layout()
plt.show()

### Step 5：业务决策

print("\n" + "=" * 70)
print("【业务决策建议】")
print("=" * 70)

decision_report = f"""
## A/B测试结论报告

### 实验配置
- 实验时间: [实际填写]
- 流量分配: A组 50%, B组 50%
- 样本量: 每组 {n_A:,} 人

### 关键指标
- A组点击率: {p_A_obs:.2%}
- B组点击率: {p_B_obs:.2%}
- 绝对提升: {absolute_lift:.4f} ({absolute_lift*100:+.2f}个百分点)
- 相对提升: {relative_lift:+.2f}%

### 统计检验
- Z统计量: {z:.4f}
- p值: {p_value:.6f}
- 95% CI: [{ci_lower*100:.2f}%, {ci_upper*100:.2f}%]
- **结论**: {'✅ 显著差异' if p_value < alpha else '❌ 无显著差异'}

### 决策建议
"""

if p_value < alpha and ci_lower > 0:
    decision_report += f"""
✅ **建议全量上线B版本算法**

理由:
1. B组点击率显著高于A组 (p={p_value:.4f} < 0.05)
2. 95%置信区间 [{ci_lower*100:.2f}%, {ci_upper*100:.2f}%] 完全为正
3. 预期ROI:
   - 假设日均曝光1000万次
   - 提升点击 = 1000万 × {absolute_lift:.4f} = {int(10000000 * absolute_lift):,} 次
   - 若每次点击价值10元，日增收益 ≈ {int(10000000 * absolute_lift * 10):,} 元
"""
elif p_value < alpha:
    decision_report += f"""
⚠️  **需要进一步分析**

B组虽然显著不同，但置信区间跨越0，建议:
1. 延长实验时间，收集更多数据
2. 分层分析（新老用户、不同品类等）
3. 检查是否有Simpson悖论
"""
else:
    decision_report += f"""
❌ **不建议上线B版本**

理由:
1. 统计检验未发现显著差异 (p={p_value:.4f} ≥ 0.05)
2. 提升 {relative_lift:.2f}% 可能只是随机波动
3. 考虑:
   - 是否需要更长实验周期？
   - 是否对特定细分群体有效？
   - B版本是否有其他优势（如成本、延迟）？
"""

print(decision_report)
```

---

## 9.3 时间序列分析：销售预测

### 9.2 方差分析（ANOVA）

**场景**：比较3种教学方法的效果。

```python
from scipy import stats

# 3组学生成绩
method_A = [85, 88, 90, 87, 86]
method_B = [78, 80, 82, 79, 81]
method_C = [92, 94, 91, 93, 95]

# 单因素方差分析
f_stat, p_value = stats.f_oneway(method_A, method_B, method_C)

print(f"F统计量: {f_stat:.4f}")
print(f"p值: {p_value:.6f}")

if p_value < 0.05:
    print("结论：不同教学方法效果有显著差异")
    
    # 事后检验（Tukey HSD）
    from statsmodels.stats.multicomp import pairwise_tukeyhsd
    
    data = method_A + method_B + method_C
    groups = ['A']*5 + ['B']*5 + ['C']*5
    
    tukey = pairwise_tukeyhsd(data, groups, alpha=0.05)
    print("\nTukey事后检验:")
    print(tukey)
```

### 9.3 卡方独立性检验

**场景**：检验性别与购买偏好是否独立。

```python
import numpy as np
from scipy.stats import chi2_contingency

# 列联表
#        产品A  产品B  产品C
# 男性     30     20     10
# 女性     10     25     35

observed = np.array([
    [30, 20, 10],
    [10, 25, 35]
])

chi2, p_value, dof, expected = chi2_contingency(observed)

print("观测频数:")
print(observed)
print("\n期望频数:")
print(expected)
print(f"\nχ²统计量: {chi2:.4f}")
print(f"自由度: {dof}")
print(f"p值: {p_value:.6f}")

if p_value < 0.05:
    print("结论：性别与购买偏好显著相关")
else:
    print("结论：性别与购买偏好独立")
```

### 9.4 时间序列分析

**场景**：销售数据预测。

```python
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.holtwinters import ExponentialSmoothing

# 月度销售数据（24个月）
sales = [120, 135, 158, 171, 196, 210, 188, 165, 
         142, 125, 130, 145, 165, 180, 201, 225, 
         245, 268, 242, 215, 189, 165, 155, 170]

# 三次指数平滑（考虑趋势和季节性）
model = ExponentialSmoothing(
    sales, 
    seasonal_periods=12,  # 12个月季节周期
    trend='add',          # 加法趋势
    seasonal='add'        # 加法季节性
)
fitted = model.fit()

# 预测未来6个月
forecast = fitted.forecast(steps=6)

# 可视化
plt.figure(figsize=(12, 6))
plt.plot(sales, label='历史数据', marker='o')
plt.plot(range(24, 30), forecast, label='预测', marker='s', color='red')
plt.xlabel('月份')
plt.ylabel('销售额')
plt.title('销售预测')
plt.legend()
plt.grid()
plt.show()

print("未来6个月预测:")
for i, val in enumerate(forecast, 1):
    print(f"第{i}个月: {val:.2f}")
```

### 9.5 相关性分析

**场景**：分析多个变量之间的相关性。

```python
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# 生成示例数据
np.random.seed(42)
n = 100

data = pd.DataFrame({
    '学习时间': np.random.normal(5, 1.5, n),
    '睡眠时间': np.random.normal(7, 1, n),
    '运动时间': np.random.normal(1.5, 0.5, n)
})

# 生成成绩（与学习时间正相关，与其他负相关）
data['成绩'] = (
    60 + 
    data['学习时间'] * 5 + 
    data['睡眠时间'] * 2 + 
    data['运动时间'] * 1.5 + 
    np.random.normal(0, 5, n)
)

# 计算相关系数矩阵
corr_matrix = data.corr()

print("相关系数矩阵:")
print(corr_matrix)

# 可视化
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', 
            center=0, square=True, linewidths=1)
plt.title('变量相关性热力图')
plt.tight_layout()
plt.show()

# 显著性检验
from scipy.stats import pearsonr

for col in ['学习时间', '睡眠时间', '运动时间']:
    r, p = pearsonr(data[col], data['成绩'])
    print(f"\n{col}与成绩的相关性:")
    print(f"  相关系数 r = {r:.4f}")
    print(f"  p值 = {p:.6f}")
    if p < 0.05:
        print(f"  结论：显著相关")
    else:
        print(f"  结论：不显著")
```

---

**本章完**
