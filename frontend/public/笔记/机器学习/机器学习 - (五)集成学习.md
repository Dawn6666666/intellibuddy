# 机器学习 - (五)集成学习

学习集成学习技术。

---

## 5. 集成学习

### 5.1 梯度提升树（GBDT）

```python
class GradientBoostingClassifier:
    """梯度提升分类器"""
    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):
        self.n_estimators = n_estimators
        self.learning_rate = learning_rate
        self.max_depth = max_depth
        self.trees = []
    
    def sigmoid(self, x):
        """Sigmoid函数"""
        return 1 / (1 + np.exp(-np.clip(x, -250, 250)))
    
    def log_loss_gradient(self, y, y_pred):
        """对数损失的梯度"""
        return y - self.sigmoid(y_pred)
    
    def fit(self, X, y):
        """训练GBDT"""
        # 初始化预测值（对数几率）
        self.init_pred = np.log(np.mean(y) / (1 - np.mean(y)))
        F = np.full(len(y), self.init_pred)
        
        for i in range(self.n_estimators):
            # 计算残差（梯度）
            residuals = self.log_loss_gradient(y, F)
            
            # 训练一棵树拟合残差
            tree = DecisionTree(max_depth=self.max_depth)
            tree.fit(X, residuals)
            
            # 更新预测值
            predictions = np.array(tree.predict(X))
            F += self.learning_rate * predictions
            
            self.trees.append(tree)
            
            if i % 20 == 0:
                loss = -np.mean(y * np.log(self.sigmoid(F) + 1e-8) + 
                               (1 - y) * np.log(1 - self.sigmoid(F) + 1e-8))
                print(f"Iteration {i}, Loss: {loss:.4f}")
    
    def predict_proba(self, X):
        """预测概率"""
        F = np.full(len(X), self.init_pred)
        
        for tree in self.trees:
            predictions = np.array(tree.predict(X))
            F += self.learning_rate * predictions
        
        return self.sigmoid(F)
    
    def predict(self, X):
        """预测类别"""
        proba = self.predict_proba(X)
        return (proba >= 0.5).astype(int)

# 示例
X, y = make_classification(n_samples=500, n_features=10, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

gbdt = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1)
gbdt.fit(X_train, y_train)

y_pred = gbdt.predict(X_test)
accuracy = np.mean(y_pred == y_test)
print(f"GBDT准确率: {accuracy:.4f}")
```

### 5.2 AdaBoost算法

```python
class AdaBoost:
    """AdaBoost分类器"""
    def __init__(self, n_estimators=50):
        self.n_estimators = n_estimators
        self.alphas = []
        self.models = []
    
    def fit(self, X, y):
        """训练AdaBoost"""
        n_samples = len(y)
        
        # 初始化样本权重
        w = np.ones(n_samples) / n_samples
        
        for i in range(self.n_estimators):
            # 训练弱分类器（决策树桩）
            tree = DecisionTree(max_depth=1)
            tree.fit(X, y)
            
            # 预测
            y_pred = np.array(tree.predict(X))
            
            # 计算误差
            err = np.sum(w * (y_pred != y))
            
            # 计算分类器权重
            alpha = 0.5 * np.log((1 - err) / (err + 1e-10))
            
            # 更新样本权重
            w *= np.exp(-alpha * y * y_pred)
            w /= np.sum(w)
            
            self.alphas.append(alpha)
            self.models.append(tree)
            
            if i % 10 == 0:
                print(f"Estimator {i}, Error: {err:.4f}, Alpha: {alpha:.4f}")
    
    def predict(self, X):
        """预测"""
        # 加权投票
        predictions = np.zeros(len(X))
        
        for alpha, model in zip(self.alphas, self.models):
            predictions += alpha * np.array(model.predict(X))
        
        return np.sign(predictions)

# 示例
y_binary = np.where(y == 0, -1, 1)  # 转换为-1/1

adaboost = AdaBoost(n_estimators=50)
adaboost.fit(X_train, y_binary[:len(X_train)])

y_pred = adaboost.predict(X_test)
accuracy = np.mean(y_pred == y_binary[len(X_train):])
print(f"AdaBoost准确率: {accuracy:.4f}")
```

---

**本章完**
