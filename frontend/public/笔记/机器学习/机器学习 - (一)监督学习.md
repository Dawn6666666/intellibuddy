# 机器学习 - (一)监督学习

掌握监督学习经典算法。

---

## 📚 机器学习流程

```plain
数据收集 → 数据预处理 → 特征工程 → 模型选择 → 训练 → 评估 → 部署
```

---

## 1. 监督学习

### 1.1 决策树

```python
import numpy as np
from collections import Counter

class DecisionTree:
    def __init__(self, max_depth=10, min_samples_split=2):
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.tree = None
    
    def entropy(self, y):
        """计算熵"""
        counts = Counter(y)
        probs = [count / len(y) for count in counts.values()]
        return -sum(p * np.log2(p) for p in probs if p > 0)
    
    def information_gain(self, X, y, feature_idx, threshold):
        """计算信息增益"""
        left_mask = X[:, feature_idx] <= threshold
        right_mask = ~left_mask
        
        if len(y[left_mask]) == 0 or len(y[right_mask]) == 0:
            return 0
        
        n = len(y)
        left_entropy = self.entropy(y[left_mask])
        right_entropy = self.entropy(y[right_mask])
        
        weighted_entropy = (len(y[left_mask]) / n) * left_entropy + \
                          (len(y[right_mask]) / n) * right_entropy
        
        return self.entropy(y) - weighted_entropy
    
    def best_split(self, X, y):
        """找到最佳分割"""
        best_gain = 0
        best_feature = None
        best_threshold = None
        
        for feature_idx in range(X.shape[1]):
            thresholds = np.unique(X[:, feature_idx])
            
            for threshold in thresholds:
                gain = self.information_gain(X, y, feature_idx, threshold)
                
                if gain > best_gain:
                    best_gain = gain
                    best_feature = feature_idx
                    best_threshold = threshold
        
        return best_feature, best_threshold, best_gain
    
    def build_tree(self, X, y, depth=0):
        """构建决策树"""
        # 停止条件
        if (depth >= self.max_depth or 
            len(y) < self.min_samples_split or 
            len(np.unique(y)) == 1):
            return Counter(y).most_common(1)[0][0]
        
        # 找到最佳分割
        feature, threshold, gain = self.best_split(X, y)
        
        if gain == 0:
            return Counter(y).most_common(1)[0][0]
        
        # 分割数据
        left_mask = X[:, feature] <= threshold
        right_mask = ~left_mask
        
        # 递归构建子树
        left_subtree = self.build_tree(X[left_mask], y[left_mask], depth + 1)
        right_subtree = self.build_tree(X[right_mask], y[right_mask], depth + 1)
        
        return {
            'feature': feature,
            'threshold': threshold,
            'left': left_subtree,
            'right': right_subtree
        }
    
    def fit(self, X, y):
        """训练决策树"""
        self.tree = self.build_tree(X, y)
    
    def predict_sample(self, x, tree):
        """预测单个样本"""
        if not isinstance(tree, dict):
            return tree
        
        if x[tree['feature']] <= tree['threshold']:
            return self.predict_sample(x, tree['left'])
        else:
            return self.predict_sample(x, tree['right'])
    
    def predict(self, X):
        """预测"""
        return [self.predict_sample(x, self.tree) for x in X]

# 示例
from sklearn.datasets import make_classification
X, y = make_classification(n_samples=100, n_features=4, n_classes=2, random_state=42)

dt = DecisionTree(max_depth=5)
dt.fit(X, y)
predictions = dt.predict(X[:5])
print(f"预测结果: {predictions}")
```

### 1.2 支持向量机（SVM）

```python
import numpy as np

class SVM:
    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):
        self.lr = learning_rate
        self.lambda_param = lambda_param
        self.n_iters = n_iters
        self.w = None
        self.b = None
    
    def fit(self, X, y):
        """训练SVM"""
        n_samples, n_features = X.shape
        
        # 将标签转换为-1和1
        y_ = np.where(y <= 0, -1, 1)
        
        # 初始化权重
        self.w = np.zeros(n_features)
        self.b = 0
        
        for i in range(self.n_iters):
            for idx, x_i in enumerate(X):
                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1
                
                if condition:
                    # 正确分类
                    self.w -= self.lr * (2 * self.lambda_param * self.w)
                else:
                    # 错误分类或在边界上
                    self.w -= self.lr * (2 * self.lambda_param * self.w - np.dot(x_i, y_[idx]))
                    self.b -= self.lr * y_[idx]
    
    def predict(self, X):
        """预测"""
        approximation = np.dot(X, self.w) - self.b
        return np.sign(approximation)

# 示例
X = np.array([[3, 3], [4, 3], [1, 1]])
y = np.array([1, 1, -1])

svm = SVM()
svm.fit(X, y)
predictions = svm.predict(X)
print(f"SVM预测: {predictions}")
```

### 1.3 随机森林

```python
import random

class RandomForest:
    def __init__(self, n_trees=10, max_depth=10, min_samples_split=2, n_features=None):
        self.n_trees = n_trees
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.n_features = n_features
        self.trees = []
    
    def bootstrap_sample(self, X, y):
        """自助采样"""
        n_samples = X.shape[0]
        idxs = np.random.choice(n_samples, n_samples, replace=True)
        return X[idxs], y[idxs]
    
    def fit(self, X, y):
        """训练随机森林"""
        self.trees = []
        
        for _ in range(self.n_trees):
            tree = DecisionTree(
                max_depth=self.max_depth,
                min_samples_split=self.min_samples_split
            )
            
            X_sample, y_sample = self.bootstrap_sample(X, y)
            tree.fit(X_sample, y_sample)
            self.trees.append(tree)
    
    def predict(self, X):
        """预测（投票）"""
        predictions = np.array([tree.predict(X) for tree in self.trees])
        return [Counter(predictions[:, i]).most_common(1)[0][0] for i in range(len(X))]

# 示例
rf = RandomForest(n_trees=5)
rf.fit(X, y)
rf_predictions = rf.predict(X[:3])
print(f"随机森林预测: {rf_predictions}")
```

### 1.4 监督学习算法对比

#### 分类算法对比

| 算法 | 优点 | 缺点 | 适用场景 | 复杂度 |
|------|------|------|----------|--------|
| **逻辑回归** | 简单快速、可解释性强、概率输出 | 线性模型、特征工程要求高 | 二分类、基线模型 | $O(nd)$ |
| **决策树** | 可解释、处理非线性、无需归一化 | 容易过拟合、不稳定 | 规则提取、特征重要性 | $O(nd \log n)$ |
| **随机森林** | 准确率高、抗过拟合、特征重要性 | 模型大、速度慢、黑盒 | 表格数据、特征筛选 | $O(knd \log n)$ |
| **SVM** | 高维表现好、核技巧处理非线性 | 大数据慢、参数敏感 | 小样本、文本分类 | $O(n^2d)$ - $O(n^3d)$ |
| **KNN** | 简单、无训练、适应局部模式 | 预测慢、内存大、维度灾难 | 小数据、推荐系统 | $O(nd)$ 预测 |
| **朴素贝叶斯** | 速度极快、适合高维、在线学习 | 独立性假设强 | 文本分类、垃圾邮件 | $O(nd)$ |

#### 回归算法对比

| 算法 | 损失函数 | 正则化 | 优点 | 缺点 |
|------|----------|--------|------|------|
| **线性回归** | MSE | 无 | 简单、可解释、快速 | 线性假设、易过拟合 |
| **Ridge回归** | MSE + L2 | $\lambda \sum w_i^2$ | 解决多重共线性、防止过拟合 | 不能特征选择 |
| **Lasso回归** | MSE + L1 | $\lambda \sum \|w_i\|$ | 特征选择、稀疏解 | 不稳定（相关特征） |
| **ElasticNet** | MSE + L1 + L2 | $\alpha L1 + (1-\alpha) L2$ | 结合Lasso和Ridge优点 | 两个超参数 |
| **SVR** | ε-insensitive | 支持向量 | 处理非线性、鲁棒 | 大数据慢、参数多 |

### 1.5 模型正则化技术

#### L1与L2正则化

**L1正则化（Lasso）：**

$$
L(\mathbf{w}) = \sum_{i=1}^{n} (y_i - \mathbf{w}^T \mathbf{x}_i)^2 + \lambda \sum_{j=1}^{d} |w_j|
$$

**特点：**
- 稀疏解（自动特征选择）
- 某些权重变为0
- 不可导（在0处）

**L2正则化（Ridge）：**

$$
L(\mathbf{w}) = \sum_{i=1}^{n} (y_i - \mathbf{w}^T \mathbf{x}_i)^2 + \lambda \sum_{j=1}^{d} w_j^2
$$

**特点：**
- 权重趋向小值但不为0
- 处处可导
- 解决多重共线性

**ElasticNet（混合）：**

$$
L(\mathbf{w}) = \sum_{i=1}^{n} (y_i - \mathbf{w}^T \mathbf{x}_i)^2 + \lambda_1 \sum_{j=1}^{d} |w_j| + \lambda_2 \sum_{j=1}^{d} w_j^2
$$

#### 正则化实现

```python
class RegularizedRegression:
    """正则化回归"""
    
    def __init__(self, alpha=1.0, l1_ratio=0.5, max_iter=1000, lr=0.01):
        self.alpha = alpha        # 正则化强度
        self.l1_ratio = l1_ratio  # L1比例（0=Ridge, 1=Lasso, 0.5=ElasticNet）
        self.max_iter = max_iter
        self.lr = lr
        self.weights = None
        self.bias = None
    
    def fit(self, X, y):
        """训练（梯度下降）"""
        n_samples, n_features = X.shape
        
        # 初始化参数
        self.weights = np.zeros(n_features)
        self.bias = 0
        
        for _ in range(self.max_iter):
            # 预测
            y_pred = np.dot(X, self.weights) + self.bias
            
            # 计算梯度
            dw = -(2 / n_samples) * np.dot(X.T, (y - y_pred))
            db = -(2 / n_samples) * np.sum(y - y_pred)
            
            # 添加正则化梯度
            # L1正则化梯度：sign(w)
            # L2正则化梯度：2w
            l1_grad = self.alpha * self.l1_ratio * np.sign(self.weights)
            l2_grad = self.alpha * (1 - self.l1_ratio) * 2 * self.weights
            
            dw += l1_grad + l2_grad
            
            # 更新参数
            self.weights -= self.lr * dw
            self.bias -= self.lr * db
    
    def predict(self, X):
        """预测"""
        return np.dot(X, self.weights) + self.bias

# 示例：对比三种正则化
X = np.random.randn(100, 20)
y = np.random.randn(100)

# Ridge (L2)
ridge = RegularizedRegression(alpha=1.0, l1_ratio=0.0)
ridge.fit(X, y)
print(f"Ridge非零权重数: {np.sum(np.abs(ridge.weights) > 0.01)}")

# Lasso (L1)
lasso = RegularizedRegression(alpha=1.0, l1_ratio=1.0)
lasso.fit(X, y)
print(f"Lasso非零权重数: {np.sum(np.abs(lasso.weights) > 0.01)}")

# ElasticNet
elastic = RegularizedRegression(alpha=1.0, l1_ratio=0.5)
elastic.fit(X, y)
print(f"ElasticNet非零权重数: {np.sum(np.abs(elastic.weights) > 0.01)}")
```

### 1.6 超参数详解

#### 决策树超参数

| 参数 | 说明 | 典型值 | 影响 |
|------|------|--------|------|
| `max_depth` | 最大深度 | 3-10 | 深度↑ → 复杂度↑ → 过拟合风险↑ |
| `min_samples_split` | 分裂最小样本数 | 2-20 | 值↑ → 剪枝效果↑ → 欠拟合风险↑ |
| `min_samples_leaf` | 叶节点最小样本数 | 1-10 | 值↑ → 泛化能力↑ |
| `max_features` | 最大特征数 | sqrt(n) | 特征随机性 |
| `criterion` | 分裂标准 | gini/entropy | 信息增益/基尼不纯度 |

**信息增益（ID3）：**

$$
\text{Gain}(D, A) = H(D) - \sum_{v} \frac{|D_v|}{|D|} H(D_v)
$$

其中熵：

$$
H(D) = -\sum_{k=1}^{K} p_k \log_2 p_k
$$

**基尼不纯度（CART）：**

$$
\text{Gini}(D) = 1 - \sum_{k=1}^{K} p_k^2
$$

#### SVM超参数

| 参数 | 说明 | 典型值 | 影响 |
|------|------|--------|------|
| `C` | 惩罚参数 | 0.1-100 | C↑ → 容忍错误↓ → 过拟合风险↑ |
| `kernel` | 核函数 | linear/rbf/poly | 处理非线性能力 |
| `gamma` | RBF核参数 | 0.001-1 | γ↑ → 影响范围↓ → 过拟合风险↑ |
| `degree` | 多项式核次数 | 2-5 | 仅对poly核有效 |

**常用核函数：**

**线性核：**

$$
K(\mathbf{x}_i, \mathbf{x}_j) = \mathbf{x}_i^T \mathbf{x}_j
$$

**RBF核（高斯核）：**

$$
K(\mathbf{x}_i, \mathbf{x}_j) = \exp\left(-\gamma \|\mathbf{x}_i - \mathbf{x}_j\|^2\right)
$$

**多项式核：**

$$
K(\mathbf{x}_i, \mathbf{x}_j) = (\gamma \mathbf{x}_i^T \mathbf{x}_j + r)^d
$$

#### 随机森林超参数

| 参数 | 说明 | 典型值 | 调优策略 |
|------|------|--------|----------|
| `n_estimators` | 树的数量 | 100-500 | 越多越好（但有收益递减） |
| `max_depth` | 树的深度 | 10-50 | 根据数据复杂度调整 |
| `max_features` | 随机特征数 | sqrt(n) 或 log2(n) | 分类：sqrt，回归：n/3 |
| `min_samples_split` | 分裂最小样本 | 2-20 | 防止过拟合 |
| `bootstrap` | 是否自助采样 | True | 默认开启 |
| `oob_score` | 袋外评分 | False | 开启可用于验证 |

### 1.7 模型评估指标

#### 分类指标

**混淆矩阵：**

|  | 预测正 | 预测负 |
|--|--------|--------|
| **实际正** | TP（真正） | FN（假负） |
| **实际负** | FP（假正） | TN（真负） |

**核心指标：**

**准确率（Accuracy）：**

$$
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
$$

**精确率（Precision）：**

$$
\text{Precision} = \frac{TP}{TP + FP}
$$

**召回率（Recall）：**

$$
\text{Recall} = \frac{TP}{TP + FN}
$$

**F1分数：**

$$
F1 = \frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
$$

**指标选择：**

| 场景 | 关注指标 | 示例 |
|------|----------|------|
| 平衡数据 | Accuracy | 手写数字识别 |
| 关注误报 | Precision | 垃圾邮件检测（宁可漏检） |
| 关注漏报 | Recall | 疾病诊断（不能漏检） |
| 不平衡数据 | F1 / AUC | 欺诈检测 |

#### 回归指标

**均方误差（MSE）：**

$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

**均方根误差（RMSE）：**

$$
\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
$$

**平均绝对误差（MAE）：**

$$
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
$$

**R²决定系数：**

$$
R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
$$

**指标对比：**

| 指标 | 特点 | 优点 | 缺点 |
|------|------|------|------|
| **MSE** | 平方误差 | 可导、惩罚大误差 | 异常值敏感、单位不直观 |
| **RMSE** | MSE平方根 | 单位与y相同 | 异常值敏感 |
| **MAE** | 绝对误差 | 鲁棒、直观 | 不可导（在0处） |
| **R²** | 相对指标 | 0-1范围、可解释性 | 特征增加时会虚高 |

### 1.8 偏差-方差权衡

#### 理论分析

**误差分解：**

$$
\mathbb{E}[(y - \hat{f}(x))^2] = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}
$$

**偏差（Bias）：**

$$
\text{Bias} = \mathbb{E}[\hat{f}(x)] - f(x)
$$

- 模型假设与真实函数的差距
- 高偏差 → 欠拟合

**方差（Variance）：**

$$
\text{Variance} = \mathbb{E}[(\hat{f}(x) - \mathbb{E}[\hat{f}(x)])^2]
$$

- 模型对训练数据扰动的敏感度
- 高方差 → 过拟合

#### 模型复杂度对比

| 模型类型 | 偏差 | 方差 | 示例 |
|----------|------|------|------|
| **简单模型** | 高 | 低 | 线性回归、朴素贝叶斯 |
| **中等模型** | 中 | 中 | 决策树（限制深度）、KNN（适中k） |
| **复杂模型** | 低 | 高 | 深度决策树、KNN（k=1） |
| **集成模型** | 低 | 低 | 随机森林、GBDT |

#### 诊断与解决

**诊断方法：**

| 现象 | 训练误差 | 验证误差 | 诊断 |
|------|----------|----------|------|
| 正常 | 低 | 低 | ✅ 刚好 |
| 过拟合 | 极低 | 高 | 高方差 |
| 欠拟合 | 高 | 高 | 高偏差 |
| 数据问题 | 低 | 极高 | 数据分布不一致 |

**解决策略：**

**减少过拟合（降低方差）：**
- 增加训练数据
- 特征选择/降维
- 正则化（L1/L2）
- 早停（Early Stopping）
- Dropout（神经网络）
- 减小模型复杂度

**减少欠拟合（降低偏差）：**
- 增加模型复杂度
- 增加特征（多项式、交叉）
- 减小正则化强度
- 增加训练时间

---

**本章完**
