# 机器学习 - (二)无监督学习

学习聚类与降维技术。

---

## 2. 无监督学习

### 2.1 主成分分析（PCA）

```python
class PCA:
    def __init__(self, n_components):
        self.n_components = n_components
        self.components = None
        self.mean = None
    
    def fit(self, X):
        """训练PCA"""
        # 中心化数据
        self.mean = np.mean(X, axis=0)
        X_centered = X - self.mean
        
        # 计算协方差矩阵
        cov_matrix = np.cov(X_centered.T)
        
        # 特征值分解
        eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)
        
        # 按特征值排序
        idxs = np.argsort(eigenvalues)[::-1]
        eigenvalues = eigenvalues[idxs]
        eigenvectors = eigenvectors[:, idxs]
        
        # 选择前n_components个主成分
        self.components = eigenvectors[:, :self.n_components]
    
    def transform(self, X):
        """降维"""
        X_centered = X - self.mean
        return np.dot(X_centered, self.components)
    
    def fit_transform(self, X):
        """训练并降维"""
        self.fit(X)
        return self.transform(X)

# 示例
X = np.random.randn(100, 5)
pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X)
print(f"原始维度: {X.shape}, 降维后: {X_reduced.shape}")
```

### 2.2 DBSCAN聚类

```python
class DBSCAN:
    def __init__(self, eps=0.5, min_samples=5):
        self.eps = eps
        self.min_samples = min_samples
    
    def euclidean_distance(self, p1, p2):
        """欧几里得距离"""
        return np.sqrt(np.sum((p1 - p2) ** 2))
    
    def get_neighbors(self, X, point_idx):
        """获取邻居点"""
        neighbors = []
        for i, point in enumerate(X):
            if self.euclidean_distance(X[point_idx], point) <= self.eps:
                neighbors.append(i)
        return neighbors
    
    def fit_predict(self, X):
        """聚类"""
        n_points = len(X)
        labels = [-1] * n_points  # -1表示噪声点
        cluster_id = 0
        
        for i in range(n_points):
            if labels[i] != -1:  # 已处理
                continue
            
            neighbors = self.get_neighbors(X, i)
            
            if len(neighbors) < self.min_samples:
                labels[i] = -1  # 噪声点
            else:
                # 开始新聚类
                labels[i] = cluster_id
                
                # 扩展聚类
                seed_set = neighbors[:]
                j = 0
                while j < len(seed_set):
                    q = seed_set[j]
                    
                    if labels[q] == -1:
                        labels[q] = cluster_id
                    
                    if labels[q] != -1:
                        j += 1
                        continue
                    
                    labels[q] = cluster_id
                    new_neighbors = self.get_neighbors(X, q)
                    
                    if len(new_neighbors) >= self.min_samples:
                        seed_set.extend(new_neighbors)
                    
                    j += 1
                
                cluster_id += 1
        
        return np.array(labels)

# 示例
X = np.vstack([
    np.random.randn(20, 2) + [2, 2],
    np.random.randn(20, 2) + [-2, -2]
])

dbscan = DBSCAN(eps=1.0, min_samples=3)
labels = dbscan.fit_predict(X)
print(f"聚类标签: {np.unique(labels)}")
```

### 2.3 K-Means与改进算法

#### K-Means++初始化

```python
class KMeansPlusPlus:
    """K-Means++（改进初始化）"""
    
    def __init__(self, k=3, max_iters=100, random_state=None):
        self.k = k
        self.max_iters = max_iters
        self.random_state = random_state
        self.centroids = None
        self.labels = None
    
    def initialize_centroids(self, X):
        """K-Means++初始化"""
        np.random.seed(self.random_state)
        n_samples = X.shape[0]
        
        # 随机选择第一个中心
        centroids = [X[np.random.randint(n_samples)]]
        
        for _ in range(1, self.k):
            # 计算每个点到最近中心的距离
            distances = np.array([
                min([np.linalg.norm(x - c) for c in centroids])
                for x in X
            ])
            
            # 距离平方作为概率权重
            probabilities = distances ** 2
            probabilities /= probabilities.sum()
            
            # 按概率采样下一个中心
            cumulative_probs = np.cumsum(probabilities)
            r = np.random.random()
            
            for idx, cumprob in enumerate(cumulative_probs):
                if r < cumprob:
                    centroids.append(X[idx])
                    break
        
        return np.array(centroids)
    
    def fit(self, X):
        """训练"""
        # K-Means++初始化
        self.centroids = self.initialize_centroids(X)
        
        for iteration in range(self.max_iters):
            # 分配样本到最近的簇
            distances = np.sqrt(((X - self.centroids[:, np.newaxis])**2).sum(axis=2))
            self.labels = np.argmin(distances, axis=0)
            
            # 更新簇中心
            new_centroids = np.array([
                X[self.labels == i].mean(axis=0) if np.sum(self.labels == i) > 0
                else self.centroids[i]
                for i in range(self.k)
            ])
            
            # 检查收敛
            if np.allclose(self.centroids, new_centroids, atol=1e-4):
                print(f"收敛于第 {iteration} 轮")
                break
            
            self.centroids = new_centroids
        
        return self
    
    def predict(self, X):
        """预测"""
        distances = np.sqrt(((X - self.centroids[:, np.newaxis])**2).sum(axis=2))
        return np.argmin(distances, axis=0)
    
    def inertia(self, X):
        """簇内平方和（SSE）"""
        distances = np.sqrt(((X - self.centroids[:, np.newaxis])**2).sum(axis=2))
        min_distances = np.min(distances, axis=0)
        return np.sum(min_distances ** 2)

# 示例
X = np.vstack([
    np.random.randn(50, 2) + [2, 2],
    np.random.randn(50, 2) + [-2, -2],
    np.random.randn(50, 2) + [2, -2]
])

kmeans = KMeansPlusPlus(k=3, random_state=42)
kmeans.fit(X)
print(f"簇内平方和: {kmeans.inertia(X):.2f}")
```

### 2.4 层次聚类

#### 凝聚层次聚类

```python
class AgglomerativeClustering:
    """凝聚层次聚类"""
    
    def __init__(self, n_clusters=3, linkage='ward'):
        self.n_clusters = n_clusters
        self.linkage = linkage  # ward, complete, average, single
        self.labels = None
    
    def euclidean_distance(self, X1, X2):
        """欧几里得距离矩阵"""
        return np.sqrt(((X1[:, np.newaxis] - X2)**2).sum(axis=2))
    
    def ward_distance(self, X, clusters, i, j):
        """Ward距离（最小方差）"""
        cluster_i = X[list(clusters[i])]
        cluster_j = X[list(clusters[j])]
        cluster_merged = np.vstack([cluster_i, cluster_j])
        
        # 合并前后的方差差
        variance_before = (
            np.sum((cluster_i - cluster_i.mean(axis=0))**2) +
            np.sum((cluster_j - cluster_j.mean(axis=0))**2)
        )
        variance_after = np.sum((cluster_merged - cluster_merged.mean(axis=0))**2)
        
        return variance_after - variance_before
    
    def fit_predict(self, X):
        """训练并预测"""
        n_samples = X.shape[0]
        
        # 初始化：每个样本为一个簇
        clusters = {i: {i} for i in range(n_samples)}
        
        while len(clusters) > self.n_clusters:
            # 找到距离最小的两个簇
            min_distance = float('inf')
            merge_pair = None
            
            cluster_ids = list(clusters.keys())
            for i in range(len(cluster_ids)):
                for j in range(i + 1, len(cluster_ids)):
                    id_i, id_j = cluster_ids[i], cluster_ids[j]
                    
                    if self.linkage == 'ward':
                        distance = self.ward_distance(X, clusters, id_i, id_j)
                    elif self.linkage == 'single':
                        # 单链接：最小距离
                        X_i = X[list(clusters[id_i])]
                        X_j = X[list(clusters[id_j])]
                        distance = self.euclidean_distance(X_i, X_j).min()
                    elif self.linkage == 'complete':
                        # 全链接：最大距离
                        X_i = X[list(clusters[id_i])]
                        X_j = X[list(clusters[id_j])]
                        distance = self.euclidean_distance(X_i, X_j).max()
                    elif self.linkage == 'average':
                        # 平均链接
                        X_i = X[list(clusters[id_i])]
                        X_j = X[list(clusters[id_j])]
                        distance = self.euclidean_distance(X_i, X_j).mean()
                    
                    if distance < min_distance:
                        min_distance = distance
                        merge_pair = (id_i, id_j)
            
            # 合并簇
            id_i, id_j = merge_pair
            clusters[id_i] = clusters[id_i].union(clusters[id_j])
            del clusters[id_j]
        
        # 分配标签
        self.labels = np.zeros(n_samples, dtype=int)
        for label, cluster in enumerate(clusters.values()):
            for idx in cluster:
                self.labels[idx] = label
        
        return self.labels

# 示例
hc = AgglomerativeClustering(n_clusters=3, linkage='ward')
labels = hc.fit_predict(X)
print(f"层次聚类结果: {Counter(labels)}")
```

### 2.5 降维算法对比

#### 降维算法总览

| 算法 | 类型 | 线性/非线性 | 保留信息 | 适用场景 | 复杂度 |
|------|------|------------|----------|----------|--------|
| **PCA** | 无监督 | 线性 | 方差 | 数据压缩、去噪 | $O(nd^2)$ |
| **LDA** | 监督 | 线性 | 类别分离度 | 分类前处理 | $O(nd^2 + d^3)$ |
| **t-SNE** | 无监督 | 非线性 | 局部结构 | 数据可视化 | $O(n^2)$ |
| **UMAP** | 无监督 | 非线性 | 全局+局部 | 大规模可视化 | $O(n \log n)$ |
| **自编码器** | 无监督 | 非线性 | 重构误差 | 复杂特征 | $O(n \cdot \text{epoch})$ |

### 2.6 t-SNE可视化

```python
class TSNE:
    """t-SNE降维（简化版）"""
    
    def __init__(self, n_components=2, perplexity=30, learning_rate=200, n_iter=1000):
        self.n_components = n_components
        self.perplexity = perplexity
        self.learning_rate = learning_rate
        self.n_iter = n_iter
    
    def compute_pairwise_affinities(self, X):
        """计算高维空间中的相似度"""
        n = X.shape[0]
        P = np.zeros((n, n))
        
        for i in range(n):
            # 计算距离
            diff = X - X[i]
            distances = np.sum(diff ** 2, axis=1)
            
            # 高斯核
            beta = 1.0  # 简化：使用固定方差
            P[i] = np.exp(-distances * beta)
            P[i, i] = 0
            
            # 归一化
            sum_P = np.sum(P[i])
            if sum_P > 0:
                P[i] /= sum_P
        
        # 对称化
        P = (P + P.T) / (2 * n)
        P = np.maximum(P, 1e-12)
        
        return P
    
    def compute_low_dim_affinities(self, Y):
        """计算低维空间中的相似度（t分布）"""
        n = Y.shape[0]
        diff = Y[:, np.newaxis, :] - Y
        distances = np.sum(diff ** 2, axis=2)
        
        # t分布（自由度=1）
        Q = 1 / (1 + distances)
        np.fill_diagonal(Q, 0)
        
        # 归一化
        Q /= np.sum(Q)
        Q = np.maximum(Q, 1e-12)
        
        return Q
    
    def fit_transform(self, X):
        """训练并降维"""
        n = X.shape[0]
        
        # 计算高维相似度
        P = self.compute_pairwise_affinities(X)
        
        # 随机初始化低维表示
        Y = np.random.randn(n, self.n_components) * 0.01
        
        # 梯度下降优化
        for iteration in range(self.n_iter):
            # 计算低维相似度
            Q = self.compute_low_dim_affinities(Y)
            
            # 计算梯度
            PQ_diff = P - Q
            grad = np.zeros_like(Y)
            
            for i in range(n):
                diff = Y[i] - Y
                distances = np.sum(diff ** 2, axis=1)
                A = (1 + distances) ** -1
                grad[i] = 4 * np.sum((PQ_diff[i] * A)[:, np.newaxis] * diff, axis=0)
            
            # 更新
            Y -= self.learning_rate * grad
            
            if iteration % 100 == 0:
                # KL散度
                kl_div = np.sum(P * np.log(P / Q))
                print(f"Iteration {iteration}, KL散度: {kl_div:.4f}")
        
        return Y

# 示例
X_3d = np.random.randn(100, 10)
tsne = TSNE(n_components=2, n_iter=500)
X_2d = tsne.fit_transform(X_3d)
print(f"降维结果: {X_3d.shape} -> {X_2d.shape}")
```

### 2.7 自编码器

```python
class Autoencoder:
    """简单自编码器（降维）"""
    
    def __init__(self, input_dim, encoding_dim, learning_rate=0.01):
        self.input_dim = input_dim
        self.encoding_dim = encoding_dim
        self.lr = learning_rate
        
        # 编码器权重
        self.W_encoder = np.random.randn(input_dim, encoding_dim) * 0.01
        self.b_encoder = np.zeros((1, encoding_dim))
        
        # 解码器权重
        self.W_decoder = np.random.randn(encoding_dim, input_dim) * 0.01
        self.b_decoder = np.zeros((1, input_dim))
    
    def sigmoid(self, x):
        """Sigmoid激活函数"""
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))
    
    def encode(self, X):
        """编码"""
        return self.sigmoid(np.dot(X, self.W_encoder) + self.b_encoder)
    
    def decode(self, Z):
        """解码"""
        return self.sigmoid(np.dot(Z, self.W_decoder) + self.b_decoder)
    
    def fit(self, X, epochs=100):
        """训练"""
        n_samples = X.shape[0]
        
        for epoch in range(epochs):
            # 前向传播
            Z = self.encode(X)  # 编码
            X_reconstructed = self.decode(Z)  # 解码
            
            # 计算重构误差
            reconstruction_error = X - X_reconstructed
            loss = np.mean(reconstruction_error ** 2)
            
            # 反向传播
            # 解码器梯度
            delta_decoder = -2 * reconstruction_error * X_reconstructed * (1 - X_reconstructed)
            dW_decoder = np.dot(Z.T, delta_decoder) / n_samples
            db_decoder = np.sum(delta_decoder, axis=0, keepdims=True) / n_samples
            
            # 编码器梯度
            delta_encoder = np.dot(delta_decoder, self.W_decoder.T) * Z * (1 - Z)
            dW_encoder = np.dot(X.T, delta_encoder) / n_samples
            db_encoder = np.sum(delta_encoder, axis=0, keepdims=True) / n_samples
            
            # 更新参数
            self.W_decoder -= self.lr * dW_decoder
            self.b_decoder -= self.lr * db_decoder
            self.W_encoder -= self.lr * dW_encoder
            self.b_encoder -= self.lr * db_encoder
            
            if epoch % 20 == 0:
                print(f"Epoch {epoch}, Loss: {loss:.4f}")
    
    def transform(self, X):
        """降维（编码）"""
        return self.encode(X)

# 示例
X = np.random.rand(200, 20)
ae = Autoencoder(input_dim=20, encoding_dim=5)
ae.fit(X, epochs=100)
X_encoded = ae.transform(X)
print(f"自编码器降维: {X.shape} -> {X_encoded.shape}")
```

### 2.8 聚类评估指标

#### 内部指标（无标签）

**轮廓系数（Silhouette Score）：**

$$
s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}
$$

其中：
- $a(i)$：样本 $i$ 到同簇其他样本的平均距离
- $b(i)$：样本 $i$ 到最近簇的平均距离
- 取值范围：[-1, 1]，越接近1越好

**Davies-Bouldin指数：**

$$
DB = \frac{1}{k} \sum_{i=1}^{k} \max_{j \neq i} \frac{s_i + s_j}{d_{ij}}
$$

- $s_i$：簇 $i$ 内样本到中心的平均距离
- $d_{ij}$：簇 $i$ 和簇 $j$ 中心的距离
- 越小越好

**Calinski-Harabasz指数：**

$$
CH = \frac{\text{SSB} / (k-1)}{\text{SSW} / (n-k)}
$$

- SSB：簇间方差
- SSW：簇内方差
- 越大越好

#### 外部指标（有标签）

| 指标 | 公式 | 范围 | 说明 |
|------|------|------|------|
| **兰德指数（RI）** | $\frac{TP+TN}{TP+FP+FN+TN}$ | [0, 1] | 越大越好 |
| **调整兰德指数（ARI）** | $\frac{RI - E[RI]}{\max(RI) - E[RI]}$ | [-1, 1] | 修正随机性 |
| **互信息（MI）** | $\sum p(i,j) \log \frac{p(i,j)}{p(i)p(j)}$ | [0, ∞] | 越大越好 |
| **NMI（归一化MI）** | $\frac{MI}{\sqrt{H(C) \cdot H(K)}}$ | [0, 1] | 归一化版本 |

```python
class ClusteringMetrics:
    """聚类评估指标"""
    
    @staticmethod
    def silhouette_score(X, labels):
        """轮廓系数"""
        n_samples = len(X)
        scores = []
        
        for i in range(n_samples):
            # 同簇样本
            same_cluster = X[labels == labels[i]]
            a_i = np.mean([np.linalg.norm(X[i] - x) for x in same_cluster if not np.array_equal(x, X[i])])
            
            # 最近簇
            other_clusters = [c for c in np.unique(labels) if c != labels[i]]
            b_i = min([
                np.mean([np.linalg.norm(X[i] - x) for x in X[labels == c]])
                for c in other_clusters
            ]) if other_clusters else 0
            
            s_i = (b_i - a_i) / max(a_i, b_i) if max(a_i, b_i) > 0 else 0
            scores.append(s_i)
        
        return np.mean(scores)
    
    @staticmethod
    def davies_bouldin_index(X, labels):
        """Davies-Bouldin指数"""
        clusters = np.unique(labels)
        centroids = np.array([X[labels == c].mean(axis=0) for c in clusters])
        
        # 簇内平均距离
        s = np.array([
            np.mean([np.linalg.norm(x - centroids[c]) for x in X[labels == c]])
            for c in range(len(clusters))
        ])
        
        # 计算DB指数
        db = 0
        for i in range(len(clusters)):
            max_ratio = 0
            for j in range(len(clusters)):
                if i != j:
                    d_ij = np.linalg.norm(centroids[i] - centroids[j])
                    ratio = (s[i] + s[j]) / d_ij if d_ij > 0 else 0
                    max_ratio = max(max_ratio, ratio)
            db += max_ratio
        
        return db / len(clusters)

# 示例
metrics = ClusteringMetrics()
silhouette = metrics.silhouette_score(X, kmeans.labels)
db_index = metrics.davies_bouldin_index(X, kmeans.labels)
print(f"轮廓系数: {silhouette:.4f}")
print(f"DB指数: {db_index:.4f}")
```

### 2.9 聚类算法对比

| 算法 | 簇形状 | 噪声处理 | 需指定k | 可扩展性 | 适用场景 |
|------|--------|----------|---------|----------|----------|
| **K-Means** | 凸形（球形） | 敏感 | 是 | 好 | 大数据、球形簇 |
| **DBSCAN** | 任意形状 | 鲁棒 | 否 | 中等 | 任意形状、噪声数据 |
| **层次聚类** | 任意形状 | 一般 | 否（可后切） | 差 | 小数据、层次结构 |
| **GMM** | 椭圆形 | 一般 | 是 | 中等 | 软聚类、概率模型 |
| **Mean Shift** | 任意形状 | 鲁棒 | 否 | 差 | 图像分割 |

---

**本章完**
