# 线性代数 - (十)实战项目

用NumPy实现线性代数算法。

---

## 10. 实战项目

### 项目1：图像压缩（SVD应用）

```python
from PIL import Image
import matplotlib.pyplot as plt

def svd_image_compression(image_path, k_values):
    """使用SVD进行图像压缩"""
    # 读取图像
    img = Image.open(image_path).convert('L')
    A = np.array(img, dtype=float)
    
    # SVD分解
    U, s, Vt = np.linalg.svd(A, full_matrices=False)
    
    # 不同k值的压缩结果
    fig, axes = plt.subplots(1, len(k_values) + 1, figsize=(15, 4))
    
    axes[0].imshow(A, cmap='gray')
    axes[0].set_title('原图')
    axes[0].axis('off')
    
    for idx, k in enumerate(k_values):
        # 重构
        A_k = U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]
        
        # 压缩率
        original_size = A.shape[0] * A.shape[1]
        compressed_size = k * (A.shape[0] + A.shape[1] + 1)
        ratio = compressed_size / original_size * 100
        
        axes[idx + 1].imshow(A_k, cmap='gray')
        axes[idx + 1].set_title(f'k={k}\n压缩率:{ratio:.1f}%')
        axes[idx + 1].axis('off')
    
    plt.tight_layout()
    plt.show()
    
    # 能量保留率
    energy = np.cumsum(s**2) / np.sum(s**2)
    plt.figure(figsize=(8, 5))
    plt.plot(energy, linewidth=2)
    plt.xlabel('奇异值个数')
    plt.ylabel('能量保留率')
    plt.title('SVD能量曲线')
    plt.grid(True)
    plt.show()

# 使用示例
# svd_image_compression('lena.jpg', [10, 30, 50, 100])
```

### 项目2：推荐系统（协同过滤）

```python
def collaborative_filtering(ratings, k=10):
    """
    基于矩阵分解的推荐系统
    
    ratings: 用户-物品评分矩阵（缺失值为0）
    k: 潜在因子数量
    """
    # 填充缺失值为均值
    mask = ratings > 0
    user_mean = np.sum(ratings, axis=1, keepdims=True) / np.sum(mask, axis=1, keepdims=True)
    ratings_filled = np.where(mask, ratings, user_mean)
    
    # SVD分解
    U, s, Vt = np.linalg.svd(ratings_filled, full_matrices=False)
    
    # 降维
    U_k = U[:, :k]
    s_k = s[:k]
    Vt_k = Vt[:k, :]
    
    # 预测评分
    predictions = U_k @ np.diag(s_k) @ Vt_k
    
    # 只保留原本缺失的位置
    recommendations = np.where(mask, 0, predictions)
    
    return recommendations

# 示例数据（用户×电影评分）
ratings = np.array([
    [5, 3, 0, 1],
    [4, 0, 0, 1],
    [1, 1, 0, 5],
    [0, 0, 5, 4],
    [0, 1, 4, 0]
], dtype=float)

predicted = collaborative_filtering(ratings, k=2)
print("预测评分:\n", predicted.round(2))

# 为用户0推荐
user_id = 0
top_items = np.argsort(predicted[user_id])[::-1]
print(f"\n为用户{user_id}推荐物品: {top_items[:2]}")
```

### 项目3：主成分分析（降维可视化）

```python
from sklearn.datasets import load_digits

def pca_visualization():
    """PCA降维可视化手写数字"""
    # 加载数据
    digits = load_digits()
    X = digits.data  # 64维（8×8图像）
    y = digits.target
    
    # PCA降维到2D
    X_centered = X - np.mean(X, axis=0)
    cov = np.cov(X_centered.T)
    eigenvalues, eigenvectors = np.linalg.eig(cov)
    
    # 排序
    idx = eigenvalues.argsort()[::-1]
    eigenvectors = eigenvectors[:, idx]
    
    # 投影到前2个主成分
    X_pca = X_centered @ eigenvectors[:, :2]
    
    # 可视化
    plt.figure(figsize=(10, 8))
    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab10', alpha=0.6)
    plt.colorbar(scatter, label='数字')
    plt.xlabel(f'PC1 ({eigenvalues[0]/sum(eigenvalues)*100:.1f}%)')
    plt.ylabel(f'PC2 ({eigenvalues[1]/sum(eigenvalues)*100:.1f}%)')
    plt.title('PCA降维：手写数字可视化')
    plt.grid(True, alpha=0.3)
    plt.show()
    
    # 累计方差贡献率
    cumsum_ratio = np.cumsum(eigenvalues) / np.sum(eigenvalues)
    plt.figure(figsize=(8, 5))
    plt.plot(cumsum_ratio[:20], 'o-', linewidth=2)
    plt.xlabel('主成分数量')
    plt.ylabel('累计方差贡献率')
    plt.title('PCA方差解释')
    plt.grid(True)
    plt.axhline(y=0.95, color='r', linestyle='--', label='95%阈值')
    plt.legend()
    plt.show()

# 运行
# pca_visualization()
```

### 项目4：最小二乘法回归

```python
def polynomial_regression(x, y, degree=3):
    """多项式回归（最小二乘法）"""
    # 构造Vandermonde矩阵
    X = np.vander(x, degree + 1, increasing=True)
    
    # 最小二乘解: (X^T X)^{-1} X^T y
    coeffs = np.linalg.inv(X.T @ X) @ X.T @ y
    
    # 预测
    x_plot = np.linspace(x.min(), x.max(), 100)
    X_plot = np.vander(x_plot, degree + 1, increasing=True)
    y_plot = X_plot @ coeffs
    
    # 可视化
    plt.figure(figsize=(10, 6))
    plt.scatter(x, y, color='blue', label='数据点')
    plt.plot(x_plot, y_plot, 'r-', linewidth=2, label=f'{degree}次多项式拟合')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.title('多项式回归（最小二乘法）')
    plt.legend()
    plt.grid(True)
    plt.show()
    
    return coeffs

# 示例：噪声数据拟合
np.random.seed(42)
x = np.linspace(-3, 3, 30)
y = 0.5 * x**2 - 2*x + 1 + np.random.normal(0, 0.5, len(x))

coeffs = polynomial_regression(x, y, degree=2)
print(f"拟合系数: {coeffs}")
```

### 项目5：马尔可夫链（PageRank）

```python
def pagerank_algorithm(link_matrix, iterations=100, damping=0.85):
    """
    PageRank算法
    
    link_matrix: 链接矩阵（A[i,j]=1表示j链接到i）
    """
    n = len(link_matrix)
    
    # 构造转移矩阵（列归一化）
    col_sums = link_matrix.sum(axis=0)
    col_sums[col_sums == 0] = 1  # 避免除以0
    M = link_matrix / col_sums
    
    # 加入随机跳转
    M = damping * M + (1 - damping) / n * np.ones((n, n))
    
    # 幂迭代法求主特征向量
    r = np.ones(n) / n
    for _ in range(iterations):
        r = M @ r
    
    return r / r.sum()

# 示例：网页链接图
links = np.array([
    [0, 1, 1, 0],
    [0, 0, 1, 1],
    [1, 0, 0, 1],
    [0, 0, 1, 0]
])

ranks = pagerank_algorithm(links)
print("PageRank分数:")
for i, score in enumerate(ranks):
    print(f"  网页{i}: {score:.3f}")

# 可视化
import networkx as nx

G = nx.DiGraph(links)
pos = nx.spring_layout(G)

plt.figure(figsize=(8, 6))
node_sizes = [r * 5000 for r in ranks]
nx.draw(G, pos, with_labels=True, node_size=node_sizes, 
        node_color='lightblue', arrows=True, arrowsize=20)
plt.title('PageRank可视化（节点大小=重要性）')
plt.show()
```

---

## 📚 学习建议

### 理论与实践结合

**每个概念的学习路径：**

```
理论定义 → 手算示例 → NumPy实现 → 实际应用
```

### 几何直观理解

**核心概念的几何意义：**

| 概念 | 几何意义 |
|------|----------|
| 向量 | 空间中的箭头 |
| 矩阵 | 线性变换 |
| 特征向量 | 变换不改变方向的向量 |
| 特征值 | 沿特征向量的缩放因子 |
| 正交 | 垂直 |

### 重点难点

1. **矩阵乘法**：不满足交换律，注意顺序
2. **特征值计算**：掌握特征方程求解
3. **对角化**：理解可对角化条件
4. **Schmidt正交化**：记住递推公式

### 推荐资源

📖 **教材：**
- 《线性代数及其应用》- David C. Lay（应用导向）
- Gilbert Strang《线性代数导论》（MIT经典）
- 《线性代数应该这样学》- Sheldon Axler（理论深度）
- 《Matrix Cookbook》（公式速查手册）

🎥 **视频课程：**
- ⭐ **3Blue1Brown《线性代数的本质》**（强烈推荐，几何直观）
- MIT 18.06 Linear Algebra - Gilbert Strang
- Khan Academy线性代数课程
- Coursera《Mathematics for Machine Learning: Linear Algebra》

💻 **在线工具：**
- [Matrix Calculator](https://matrixcalc.org/) - 在线矩阵计算
- [Wolfram Alpha](https://www.wolframalpha.com/) - 符号计算
- [GeoGebra](https://www.geogebra.org/) - 可视化工具

📝 **练习平台：**
- Khan Academy习题
- MIT OCW作业
- LeetCode（矩阵类算法题）

---

## 附录A：NumPy速查表

### 创建矩阵

```python
import numpy as np

# 基本创建
A = np.array([[1, 2], [3, 4]])  # 从列表创建
I = np.eye(3)                    # 单位矩阵
Z = np.zeros((3, 3))             # 零矩阵
O = np.ones((2, 3))              # 全1矩阵
D = np.diag([1, 2, 3])           # 对角矩阵
R = np.random.rand(3, 3)         # 随机矩阵[0,1)
N = np.random.randn(3, 3)        # 标准正态分布

# 特殊构造
V = np.arange(10)                # 向量[0,1,...,9]
L = np.linspace(0, 1, 5)         # 等间隔[0, 0.25, 0.5, 0.75, 1]
M = np.meshgrid(...)             # 网格矩阵
```

### 基本运算

```python
# 算术运算
A + B          # 逐元素加法
A - B          # 逐元素减法
A * B          # 逐元素乘法
A / B          # 逐元素除法
A ** 2         # 逐元素平方

# 矩阵运算
A @ B          # 矩阵乘法（Python 3.5+）
np.dot(A, B)   # 矩阵乘法
A.T            # 转置
np.linalg.matrix_power(A, 3)  # A^3
```

### 线性代数函数

```python
# 行列式与逆
np.linalg.det(A)      # 行列式
np.linalg.inv(A)      # 逆矩阵
np.linalg.pinv(A)     # 伪逆（Moore-Penrose）

# 秩与迹
np.linalg.matrix_rank(A)  # 秩
np.trace(A)               # 迹（对角线和）

# 特征值分解
eigenvalues, eigenvectors = np.linalg.eig(A)
eigenvalues = np.linalg.eigvals(A)  # 仅特征值

# SVD分解
U, s, Vt = np.linalg.svd(A)
U, s, Vt = np.linalg.svd(A, full_matrices=False)  # 经济型

# QR分解
Q, R = np.linalg.qr(A)

# Cholesky分解（正定矩阵）
L = np.linalg.cholesky(A)

# 解方程组
x = np.linalg.solve(A, b)     # Ax = b
x = np.linalg.lstsq(A, b)[0]  # 最小二乘解
```

### 向量运算

```python
# 内积与范数
np.dot(u, v)             # 内积
np.inner(u, v)           # 同上
np.linalg.norm(v)        # L2范数
np.linalg.norm(v, 1)     # L1范数
np.linalg.norm(v, np.inf) # 无穷范数

# 外积与叉积
np.outer(u, v)           # 外积（u⊗v）
np.cross(u, v)           # 叉积（仅3D）
```

### 矩阵分析

```python
# 条件数
np.linalg.cond(A)        # 条件数（测量病态程度）

# 矩阵范数
np.linalg.norm(A)        # Frobenius范数
np.linalg.norm(A, 2)     # 谱范数（最大奇异值）
np.linalg.norm(A, 'fro') # Frobenius范数
```

---

## 附录B：常见矩阵公式

### 矩阵求导

$$
\frac{\partial (\mathbf{x}^T \mathbf{a})}{\partial \mathbf{x}} = \mathbf{a}
$$

$$
\frac{\partial (\mathbf{x}^T A \mathbf{x})}{\partial \mathbf{x}} = (A + A^T) \mathbf{x}
$$

$$
\frac{\partial (A\mathbf{x})}{\partial \mathbf{x}} = A^T
$$

### 矩阵恒等式

$$
(AB)^T = B^T A^T
$$

$$
(AB)^{-1} = B^{-1} A^{-1}
$$

$$
\det(AB) = \det(A) \det(B)
$$

$$
\text{tr}(AB) = \text{tr}(BA)
$$

$$
\text{tr}(A + B) = \text{tr}(A) + \text{tr}(B)
$$

### Sherman-Morrison公式

$$
(A + \mathbf{u}\mathbf{v}^T)^{-1} = A^{-1} - \frac{A^{-1}\mathbf{u}\mathbf{v}^T A^{-1}}{1 + \mathbf{v}^T A^{-1} \mathbf{u}}
$$

（用于快速更新逆矩阵）

---

## 附录C：面试/考试高频题

### 1. 证明题

**题目**：证明 $\text{rank}(A + B) \leq \text{rank}(A) + \text{rank}(B)$

<details>
<summary>点击查看答案</summary>

设 $A$ 的列空间为 $C(A)$，$B$ 的列空间为 $C(B)$

$(A+B)\mathbf{x} = A\mathbf{x} + B\mathbf{x} \in C(A) + C(B)$

因此 $C(A+B) \subseteq C(A) + C(B)$

所以 $\dim(C(A+B)) \leq \dim(C(A) + C(B)) \leq \dim(C(A)) + \dim(C(B))$

即 $\text{rank}(A+B) \leq \text{rank}(A) + \text{rank}(B)$ ∎
</details>

### 2. 计算题

**题目**：求矩阵 $A = \begin{bmatrix} 3 & 1 \\ 1 & 3 \end{bmatrix}$ 的对角化

<details>
<summary>点击查看答案</summary>

1. **特征值**：
   $$\det(A - \lambda I) = (3-\lambda)^2 - 1 = 0 \Rightarrow \lambda_1 = 4, \lambda_2 = 2$$

2. **特征向量**：
   - $\lambda_1 = 4$: $\mathbf{v}_1 = \begin{bmatrix} 1 \\ 1 \end{bmatrix}$
   - $\lambda_2 = 2$: $\mathbf{v}_2 = \begin{bmatrix} -1 \\ 1 \end{bmatrix}$

3. **对角化**：
   $$A = PDP^{-1} = \frac{1}{2}\begin{bmatrix} 1 & -1 \\ 1 & 1 \end{bmatrix}\begin{bmatrix} 4 & 0 \\ 0 & 2 \end{bmatrix}\begin{bmatrix} 1 & 1 \\ -1 & 1 \end{bmatrix}$$
</details>

### 3. 应用题

**题目**：用最小二乘法拟合数据点 $(0,1), (1,2), (2,4)$ 到直线 $y = ax + b$

<details>
<summary>点击查看答案</summary>

构造方程组：
$$X = \begin{bmatrix} 1 & 0 \\ 1 & 1 \\ 1 & 2 \end{bmatrix}, \quad \mathbf{y} = \begin{bmatrix} 1 \\ 2 \\ 4 \end{bmatrix}$$

最小二乘解：
$$\begin{bmatrix} b \\ a \end{bmatrix} = (X^T X)^{-1} X^T \mathbf{y} = \begin{bmatrix} 0.33 \\ 1.5 \end{bmatrix}$$

拟合直线：$y = 1.5x + 0.33$
</details>

---

## 附录D：Python实现完整线性代数库

```python
class LinearAlgebra:
    """从零实现线性代数库（教学用）"""
    
    @staticmethod
    def det(A):
        """计算行列式（递归）"""
        n = len(A)
        if n == 1:
            return A[0][0]
        if n == 2:
            return A[0][0]*A[1][1] - A[0][1]*A[1][0]
        
        det_val = 0
        for j in range(n):
            # 余子式
            minor = [row[:j] + row[j+1:] for row in A[1:]]
            det_val += ((-1)**j) * A[0][j] * LinearAlgebra.det(minor)
        return det_val
    
    @staticmethod
    def transpose(A):
        """转置"""
        return [[A[i][j] for i in range(len(A))] for j in range(len(A[0]))]
    
    @staticmethod
    def multiply(A, B):
        """矩阵乘法"""
        rows_A, cols_A = len(A), len(A[0])
        rows_B, cols_B = len(B), len(B[0])
        
        C = [[0 for _ in range(cols_B)] for _ in range(rows_A)]
        for i in range(rows_A):
            for j in range(cols_B):
                for k in range(cols_A):
                    C[i][j] += A[i][k] * B[k][j]
        return C
    
    @staticmethod
    def gauss_eliminate(A, b):
        """高斯消元法"""
        n = len(b)
        # 增广矩阵
        M = [A[i] + [b[i]] for i in range(n)]
        
        # 前向消元
        for i in range(n):
            # 选主元
            max_row = max(range(i, n), key=lambda r: abs(M[r][i]))
            M[i], M[max_row] = M[max_row], M[i]
            
            # 消元
            for j in range(i+1, n):
                factor = M[j][i] / M[i][i]
                for k in range(i, n+1):
                    M[j][k] -= factor * M[i][k]
        
        # 回代
        x = [0] * n
        for i in range(n-1, -1, -1):
            x[i] = M[i][n] - sum(M[i][j] * x[j] for j in range(i+1, n))
            x[i] /= M[i][i]
        
        return x

# 测试
A = [[2, 1], [1, -1]]
b = [5, 1]
x = LinearAlgebra.gauss_eliminate(A, b)
print(f"解: {x}")  # [2.0, 1.0]
```

---

> **🎓 学习完线性代数后，你应该能够**：
> 1. ✅ 理解向量空间、线性变换的本质
> 2. ✅ 熟练使用NumPy进行矩阵运算
> 3. ✅ 将线性代数应用到机器学习、图形学、优化等领域
> 4. ✅ 阅读论文中的矩阵公式
> 5. ✅ 独立实现PCA、SVD等算法
>
> **下一步学习方向**：
> - 📈 机器学习（梯度下降、神经网络）
> - 🎮 计算机图形学（3D变换、光照）
> - 🔢 数值优化（凸优化、非线性规划）
> - 🧮 高等线性代数（张量、泛函分析）
>
> **Good luck!** 🚀

---

**本章完**
