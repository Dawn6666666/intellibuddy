# çº¿æ€§ä»£æ•° - (å)å®æˆ˜é¡¹ç›®

ç”¨NumPyå®ç°çº¿æ€§ä»£æ•°ç®—æ³•ã€‚

---

## 10. å®æˆ˜é¡¹ç›®

### é¡¹ç›®1ï¼šå›¾åƒå‹ç¼©ï¼ˆSVDåº”ç”¨ï¼‰

```python
from PIL import Image
import matplotlib.pyplot as plt

def svd_image_compression(image_path, k_values):
    """ä½¿ç”¨SVDè¿›è¡Œå›¾åƒå‹ç¼©"""
    # è¯»å–å›¾åƒ
    img = Image.open(image_path).convert('L')
    A = np.array(img, dtype=float)
    
    # SVDåˆ†è§£
    U, s, Vt = np.linalg.svd(A, full_matrices=False)
    
    # ä¸åŒkå€¼çš„å‹ç¼©ç»“æœ
    fig, axes = plt.subplots(1, len(k_values) + 1, figsize=(15, 4))
    
    axes[0].imshow(A, cmap='gray')
    axes[0].set_title('åŸå›¾')
    axes[0].axis('off')
    
    for idx, k in enumerate(k_values):
        # é‡æ„
        A_k = U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]
        
        # å‹ç¼©ç‡
        original_size = A.shape[0] * A.shape[1]
        compressed_size = k * (A.shape[0] + A.shape[1] + 1)
        ratio = compressed_size / original_size * 100
        
        axes[idx + 1].imshow(A_k, cmap='gray')
        axes[idx + 1].set_title(f'k={k}\nå‹ç¼©ç‡:{ratio:.1f}%')
        axes[idx + 1].axis('off')
    
    plt.tight_layout()
    plt.show()
    
    # èƒ½é‡ä¿ç•™ç‡
    energy = np.cumsum(s**2) / np.sum(s**2)
    plt.figure(figsize=(8, 5))
    plt.plot(energy, linewidth=2)
    plt.xlabel('å¥‡å¼‚å€¼ä¸ªæ•°')
    plt.ylabel('èƒ½é‡ä¿ç•™ç‡')
    plt.title('SVDèƒ½é‡æ›²çº¿')
    plt.grid(True)
    plt.show()

# ä½¿ç”¨ç¤ºä¾‹
# svd_image_compression('lena.jpg', [10, 30, 50, 100])
```

### é¡¹ç›®2ï¼šæ¨èç³»ç»Ÿï¼ˆååŒè¿‡æ»¤ï¼‰

```python
def collaborative_filtering(ratings, k=10):
    """
    åŸºäºçŸ©é˜µåˆ†è§£çš„æ¨èç³»ç»Ÿ
    
    ratings: ç”¨æˆ·-ç‰©å“è¯„åˆ†çŸ©é˜µï¼ˆç¼ºå¤±å€¼ä¸º0ï¼‰
    k: æ½œåœ¨å› å­æ•°é‡
    """
    # å¡«å……ç¼ºå¤±å€¼ä¸ºå‡å€¼
    mask = ratings > 0
    user_mean = np.sum(ratings, axis=1, keepdims=True) / np.sum(mask, axis=1, keepdims=True)
    ratings_filled = np.where(mask, ratings, user_mean)
    
    # SVDåˆ†è§£
    U, s, Vt = np.linalg.svd(ratings_filled, full_matrices=False)
    
    # é™ç»´
    U_k = U[:, :k]
    s_k = s[:k]
    Vt_k = Vt[:k, :]
    
    # é¢„æµ‹è¯„åˆ†
    predictions = U_k @ np.diag(s_k) @ Vt_k
    
    # åªä¿ç•™åŸæœ¬ç¼ºå¤±çš„ä½ç½®
    recommendations = np.where(mask, 0, predictions)
    
    return recommendations

# ç¤ºä¾‹æ•°æ®ï¼ˆç”¨æˆ·Ã—ç”µå½±è¯„åˆ†ï¼‰
ratings = np.array([
    [5, 3, 0, 1],
    [4, 0, 0, 1],
    [1, 1, 0, 5],
    [0, 0, 5, 4],
    [0, 1, 4, 0]
], dtype=float)

predicted = collaborative_filtering(ratings, k=2)
print("é¢„æµ‹è¯„åˆ†:\n", predicted.round(2))

# ä¸ºç”¨æˆ·0æ¨è
user_id = 0
top_items = np.argsort(predicted[user_id])[::-1]
print(f"\nä¸ºç”¨æˆ·{user_id}æ¨èç‰©å“: {top_items[:2]}")
```

### é¡¹ç›®3ï¼šä¸»æˆåˆ†åˆ†æï¼ˆé™ç»´å¯è§†åŒ–ï¼‰

```python
from sklearn.datasets import load_digits

def pca_visualization():
    """PCAé™ç»´å¯è§†åŒ–æ‰‹å†™æ•°å­—"""
    # åŠ è½½æ•°æ®
    digits = load_digits()
    X = digits.data  # 64ç»´ï¼ˆ8Ã—8å›¾åƒï¼‰
    y = digits.target
    
    # PCAé™ç»´åˆ°2D
    X_centered = X - np.mean(X, axis=0)
    cov = np.cov(X_centered.T)
    eigenvalues, eigenvectors = np.linalg.eig(cov)
    
    # æ’åº
    idx = eigenvalues.argsort()[::-1]
    eigenvectors = eigenvectors[:, idx]
    
    # æŠ•å½±åˆ°å‰2ä¸ªä¸»æˆåˆ†
    X_pca = X_centered @ eigenvectors[:, :2]
    
    # å¯è§†åŒ–
    plt.figure(figsize=(10, 8))
    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab10', alpha=0.6)
    plt.colorbar(scatter, label='æ•°å­—')
    plt.xlabel(f'PC1 ({eigenvalues[0]/sum(eigenvalues)*100:.1f}%)')
    plt.ylabel(f'PC2 ({eigenvalues[1]/sum(eigenvalues)*100:.1f}%)')
    plt.title('PCAé™ç»´ï¼šæ‰‹å†™æ•°å­—å¯è§†åŒ–')
    plt.grid(True, alpha=0.3)
    plt.show()
    
    # ç´¯è®¡æ–¹å·®è´¡çŒ®ç‡
    cumsum_ratio = np.cumsum(eigenvalues) / np.sum(eigenvalues)
    plt.figure(figsize=(8, 5))
    plt.plot(cumsum_ratio[:20], 'o-', linewidth=2)
    plt.xlabel('ä¸»æˆåˆ†æ•°é‡')
    plt.ylabel('ç´¯è®¡æ–¹å·®è´¡çŒ®ç‡')
    plt.title('PCAæ–¹å·®è§£é‡Š')
    plt.grid(True)
    plt.axhline(y=0.95, color='r', linestyle='--', label='95%é˜ˆå€¼')
    plt.legend()
    plt.show()

# è¿è¡Œ
# pca_visualization()
```

### é¡¹ç›®4ï¼šæœ€å°äºŒä¹˜æ³•å›å½’

```python
def polynomial_regression(x, y, degree=3):
    """å¤šé¡¹å¼å›å½’ï¼ˆæœ€å°äºŒä¹˜æ³•ï¼‰"""
    # æ„é€ VandermondeçŸ©é˜µ
    X = np.vander(x, degree + 1, increasing=True)
    
    # æœ€å°äºŒä¹˜è§£: (X^T X)^{-1} X^T y
    coeffs = np.linalg.inv(X.T @ X) @ X.T @ y
    
    # é¢„æµ‹
    x_plot = np.linspace(x.min(), x.max(), 100)
    X_plot = np.vander(x_plot, degree + 1, increasing=True)
    y_plot = X_plot @ coeffs
    
    # å¯è§†åŒ–
    plt.figure(figsize=(10, 6))
    plt.scatter(x, y, color='blue', label='æ•°æ®ç‚¹')
    plt.plot(x_plot, y_plot, 'r-', linewidth=2, label=f'{degree}æ¬¡å¤šé¡¹å¼æ‹Ÿåˆ')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.title('å¤šé¡¹å¼å›å½’ï¼ˆæœ€å°äºŒä¹˜æ³•ï¼‰')
    plt.legend()
    plt.grid(True)
    plt.show()
    
    return coeffs

# ç¤ºä¾‹ï¼šå™ªå£°æ•°æ®æ‹Ÿåˆ
np.random.seed(42)
x = np.linspace(-3, 3, 30)
y = 0.5 * x**2 - 2*x + 1 + np.random.normal(0, 0.5, len(x))

coeffs = polynomial_regression(x, y, degree=2)
print(f"æ‹Ÿåˆç³»æ•°: {coeffs}")
```

### é¡¹ç›®5ï¼šé©¬å°”å¯å¤«é“¾ï¼ˆPageRankï¼‰

```python
def pagerank_algorithm(link_matrix, iterations=100, damping=0.85):
    """
    PageRankç®—æ³•
    
    link_matrix: é“¾æ¥çŸ©é˜µï¼ˆA[i,j]=1è¡¨ç¤ºjé“¾æ¥åˆ°iï¼‰
    """
    n = len(link_matrix)
    
    # æ„é€ è½¬ç§»çŸ©é˜µï¼ˆåˆ—å½’ä¸€åŒ–ï¼‰
    col_sums = link_matrix.sum(axis=0)
    col_sums[col_sums == 0] = 1  # é¿å…é™¤ä»¥0
    M = link_matrix / col_sums
    
    # åŠ å…¥éšæœºè·³è½¬
    M = damping * M + (1 - damping) / n * np.ones((n, n))
    
    # å¹‚è¿­ä»£æ³•æ±‚ä¸»ç‰¹å¾å‘é‡
    r = np.ones(n) / n
    for _ in range(iterations):
        r = M @ r
    
    return r / r.sum()

# ç¤ºä¾‹ï¼šç½‘é¡µé“¾æ¥å›¾
links = np.array([
    [0, 1, 1, 0],
    [0, 0, 1, 1],
    [1, 0, 0, 1],
    [0, 0, 1, 0]
])

ranks = pagerank_algorithm(links)
print("PageRankåˆ†æ•°:")
for i, score in enumerate(ranks):
    print(f"  ç½‘é¡µ{i}: {score:.3f}")

# å¯è§†åŒ–
import networkx as nx

G = nx.DiGraph(links)
pos = nx.spring_layout(G)

plt.figure(figsize=(8, 6))
node_sizes = [r * 5000 for r in ranks]
nx.draw(G, pos, with_labels=True, node_size=node_sizes, 
        node_color='lightblue', arrows=True, arrowsize=20)
plt.title('PageRankå¯è§†åŒ–ï¼ˆèŠ‚ç‚¹å¤§å°=é‡è¦æ€§ï¼‰')
plt.show()
```

---

## ğŸ“š å­¦ä¹ å»ºè®®

### ç†è®ºä¸å®è·µç»“åˆ

**æ¯ä¸ªæ¦‚å¿µçš„å­¦ä¹ è·¯å¾„ï¼š**

```
ç†è®ºå®šä¹‰ â†’ æ‰‹ç®—ç¤ºä¾‹ â†’ NumPyå®ç° â†’ å®é™…åº”ç”¨
```

### å‡ ä½•ç›´è§‚ç†è§£

**æ ¸å¿ƒæ¦‚å¿µçš„å‡ ä½•æ„ä¹‰ï¼š**

| æ¦‚å¿µ | å‡ ä½•æ„ä¹‰ |
|------|----------|
| å‘é‡ | ç©ºé—´ä¸­çš„ç®­å¤´ |
| çŸ©é˜µ | çº¿æ€§å˜æ¢ |
| ç‰¹å¾å‘é‡ | å˜æ¢ä¸æ”¹å˜æ–¹å‘çš„å‘é‡ |
| ç‰¹å¾å€¼ | æ²¿ç‰¹å¾å‘é‡çš„ç¼©æ”¾å› å­ |
| æ­£äº¤ | å‚ç›´ |

### é‡ç‚¹éš¾ç‚¹

1. **çŸ©é˜µä¹˜æ³•**ï¼šä¸æ»¡è¶³äº¤æ¢å¾‹ï¼Œæ³¨æ„é¡ºåº
2. **ç‰¹å¾å€¼è®¡ç®—**ï¼šæŒæ¡ç‰¹å¾æ–¹ç¨‹æ±‚è§£
3. **å¯¹è§’åŒ–**ï¼šç†è§£å¯å¯¹è§’åŒ–æ¡ä»¶
4. **Schmidtæ­£äº¤åŒ–**ï¼šè®°ä½é€’æ¨å…¬å¼

### æ¨èèµ„æº

ğŸ“– **æ•™æï¼š**
- ã€Šçº¿æ€§ä»£æ•°åŠå…¶åº”ç”¨ã€‹- David C. Layï¼ˆåº”ç”¨å¯¼å‘ï¼‰
- Gilbert Strangã€Šçº¿æ€§ä»£æ•°å¯¼è®ºã€‹ï¼ˆMITç»å…¸ï¼‰
- ã€Šçº¿æ€§ä»£æ•°åº”è¯¥è¿™æ ·å­¦ã€‹- Sheldon Axlerï¼ˆç†è®ºæ·±åº¦ï¼‰
- ã€ŠMatrix Cookbookã€‹ï¼ˆå…¬å¼é€ŸæŸ¥æ‰‹å†Œï¼‰

ğŸ¥ **è§†é¢‘è¯¾ç¨‹ï¼š**
- â­ **3Blue1Brownã€Šçº¿æ€§ä»£æ•°çš„æœ¬è´¨ã€‹**ï¼ˆå¼ºçƒˆæ¨èï¼Œå‡ ä½•ç›´è§‚ï¼‰
- MIT 18.06 Linear Algebra - Gilbert Strang
- Khan Academyçº¿æ€§ä»£æ•°è¯¾ç¨‹
- Courseraã€ŠMathematics for Machine Learning: Linear Algebraã€‹

ğŸ’» **åœ¨çº¿å·¥å…·ï¼š**
- [Matrix Calculator](https://matrixcalc.org/) - åœ¨çº¿çŸ©é˜µè®¡ç®—
- [Wolfram Alpha](https://www.wolframalpha.com/) - ç¬¦å·è®¡ç®—
- [GeoGebra](https://www.geogebra.org/) - å¯è§†åŒ–å·¥å…·

ğŸ“ **ç»ƒä¹ å¹³å°ï¼š**
- Khan Academyä¹ é¢˜
- MIT OCWä½œä¸š
- LeetCodeï¼ˆçŸ©é˜µç±»ç®—æ³•é¢˜ï¼‰

---

## é™„å½•Aï¼šNumPyé€ŸæŸ¥è¡¨

### åˆ›å»ºçŸ©é˜µ

```python
import numpy as np

# åŸºæœ¬åˆ›å»º
A = np.array([[1, 2], [3, 4]])  # ä»åˆ—è¡¨åˆ›å»º
I = np.eye(3)                    # å•ä½çŸ©é˜µ
Z = np.zeros((3, 3))             # é›¶çŸ©é˜µ
O = np.ones((2, 3))              # å…¨1çŸ©é˜µ
D = np.diag([1, 2, 3])           # å¯¹è§’çŸ©é˜µ
R = np.random.rand(3, 3)         # éšæœºçŸ©é˜µ[0,1)
N = np.random.randn(3, 3)        # æ ‡å‡†æ­£æ€åˆ†å¸ƒ

# ç‰¹æ®Šæ„é€ 
V = np.arange(10)                # å‘é‡[0,1,...,9]
L = np.linspace(0, 1, 5)         # ç­‰é—´éš”[0, 0.25, 0.5, 0.75, 1]
M = np.meshgrid(...)             # ç½‘æ ¼çŸ©é˜µ
```

### åŸºæœ¬è¿ç®—

```python
# ç®—æœ¯è¿ç®—
A + B          # é€å…ƒç´ åŠ æ³•
A - B          # é€å…ƒç´ å‡æ³•
A * B          # é€å…ƒç´ ä¹˜æ³•
A / B          # é€å…ƒç´ é™¤æ³•
A ** 2         # é€å…ƒç´ å¹³æ–¹

# çŸ©é˜µè¿ç®—
A @ B          # çŸ©é˜µä¹˜æ³•ï¼ˆPython 3.5+ï¼‰
np.dot(A, B)   # çŸ©é˜µä¹˜æ³•
A.T            # è½¬ç½®
np.linalg.matrix_power(A, 3)  # A^3
```

### çº¿æ€§ä»£æ•°å‡½æ•°

```python
# è¡Œåˆ—å¼ä¸é€†
np.linalg.det(A)      # è¡Œåˆ—å¼
np.linalg.inv(A)      # é€†çŸ©é˜µ
np.linalg.pinv(A)     # ä¼ªé€†ï¼ˆMoore-Penroseï¼‰

# ç§©ä¸è¿¹
np.linalg.matrix_rank(A)  # ç§©
np.trace(A)               # è¿¹ï¼ˆå¯¹è§’çº¿å’Œï¼‰

# ç‰¹å¾å€¼åˆ†è§£
eigenvalues, eigenvectors = np.linalg.eig(A)
eigenvalues = np.linalg.eigvals(A)  # ä»…ç‰¹å¾å€¼

# SVDåˆ†è§£
U, s, Vt = np.linalg.svd(A)
U, s, Vt = np.linalg.svd(A, full_matrices=False)  # ç»æµå‹

# QRåˆ†è§£
Q, R = np.linalg.qr(A)

# Choleskyåˆ†è§£ï¼ˆæ­£å®šçŸ©é˜µï¼‰
L = np.linalg.cholesky(A)

# è§£æ–¹ç¨‹ç»„
x = np.linalg.solve(A, b)     # Ax = b
x = np.linalg.lstsq(A, b)[0]  # æœ€å°äºŒä¹˜è§£
```

### å‘é‡è¿ç®—

```python
# å†…ç§¯ä¸èŒƒæ•°
np.dot(u, v)             # å†…ç§¯
np.inner(u, v)           # åŒä¸Š
np.linalg.norm(v)        # L2èŒƒæ•°
np.linalg.norm(v, 1)     # L1èŒƒæ•°
np.linalg.norm(v, np.inf) # æ— ç©·èŒƒæ•°

# å¤–ç§¯ä¸å‰ç§¯
np.outer(u, v)           # å¤–ç§¯ï¼ˆuâŠ—vï¼‰
np.cross(u, v)           # å‰ç§¯ï¼ˆä»…3Dï¼‰
```

### çŸ©é˜µåˆ†æ

```python
# æ¡ä»¶æ•°
np.linalg.cond(A)        # æ¡ä»¶æ•°ï¼ˆæµ‹é‡ç—…æ€ç¨‹åº¦ï¼‰

# çŸ©é˜µèŒƒæ•°
np.linalg.norm(A)        # FrobeniusèŒƒæ•°
np.linalg.norm(A, 2)     # è°±èŒƒæ•°ï¼ˆæœ€å¤§å¥‡å¼‚å€¼ï¼‰
np.linalg.norm(A, 'fro') # FrobeniusèŒƒæ•°
```

---

## é™„å½•Bï¼šå¸¸è§çŸ©é˜µå…¬å¼

### çŸ©é˜µæ±‚å¯¼

$$
\frac{\partial (\mathbf{x}^T \mathbf{a})}{\partial \mathbf{x}} = \mathbf{a}
$$

$$
\frac{\partial (\mathbf{x}^T A \mathbf{x})}{\partial \mathbf{x}} = (A + A^T) \mathbf{x}
$$

$$
\frac{\partial (A\mathbf{x})}{\partial \mathbf{x}} = A^T
$$

### çŸ©é˜µæ’ç­‰å¼

$$
(AB)^T = B^T A^T
$$

$$
(AB)^{-1} = B^{-1} A^{-1}
$$

$$
\det(AB) = \det(A) \det(B)
$$

$$
\text{tr}(AB) = \text{tr}(BA)
$$

$$
\text{tr}(A + B) = \text{tr}(A) + \text{tr}(B)
$$

### Sherman-Morrisonå…¬å¼

$$
(A + \mathbf{u}\mathbf{v}^T)^{-1} = A^{-1} - \frac{A^{-1}\mathbf{u}\mathbf{v}^T A^{-1}}{1 + \mathbf{v}^T A^{-1} \mathbf{u}}
$$

ï¼ˆç”¨äºå¿«é€Ÿæ›´æ–°é€†çŸ©é˜µï¼‰

---

## é™„å½•Cï¼šé¢è¯•/è€ƒè¯•é«˜é¢‘é¢˜

### 1. è¯æ˜é¢˜

**é¢˜ç›®**ï¼šè¯æ˜ $\text{rank}(A + B) \leq \text{rank}(A) + \text{rank}(B)$

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

è®¾ $A$ çš„åˆ—ç©ºé—´ä¸º $C(A)$ï¼Œ$B$ çš„åˆ—ç©ºé—´ä¸º $C(B)$

$(A+B)\mathbf{x} = A\mathbf{x} + B\mathbf{x} \in C(A) + C(B)$

å› æ­¤ $C(A+B) \subseteq C(A) + C(B)$

æ‰€ä»¥ $\dim(C(A+B)) \leq \dim(C(A) + C(B)) \leq \dim(C(A)) + \dim(C(B))$

å³ $\text{rank}(A+B) \leq \text{rank}(A) + \text{rank}(B)$ âˆ
</details>

### 2. è®¡ç®—é¢˜

**é¢˜ç›®**ï¼šæ±‚çŸ©é˜µ $A = \begin{bmatrix} 3 & 1 \\ 1 & 3 \end{bmatrix}$ çš„å¯¹è§’åŒ–

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

1. **ç‰¹å¾å€¼**ï¼š
   $$\det(A - \lambda I) = (3-\lambda)^2 - 1 = 0 \Rightarrow \lambda_1 = 4, \lambda_2 = 2$$

2. **ç‰¹å¾å‘é‡**ï¼š
   - $\lambda_1 = 4$: $\mathbf{v}_1 = \begin{bmatrix} 1 \\ 1 \end{bmatrix}$
   - $\lambda_2 = 2$: $\mathbf{v}_2 = \begin{bmatrix} -1 \\ 1 \end{bmatrix}$

3. **å¯¹è§’åŒ–**ï¼š
   $$A = PDP^{-1} = \frac{1}{2}\begin{bmatrix} 1 & -1 \\ 1 & 1 \end{bmatrix}\begin{bmatrix} 4 & 0 \\ 0 & 2 \end{bmatrix}\begin{bmatrix} 1 & 1 \\ -1 & 1 \end{bmatrix}$$
</details>

### 3. åº”ç”¨é¢˜

**é¢˜ç›®**ï¼šç”¨æœ€å°äºŒä¹˜æ³•æ‹Ÿåˆæ•°æ®ç‚¹ $(0,1), (1,2), (2,4)$ åˆ°ç›´çº¿ $y = ax + b$

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

æ„é€ æ–¹ç¨‹ç»„ï¼š
$$X = \begin{bmatrix} 1 & 0 \\ 1 & 1 \\ 1 & 2 \end{bmatrix}, \quad \mathbf{y} = \begin{bmatrix} 1 \\ 2 \\ 4 \end{bmatrix}$$

æœ€å°äºŒä¹˜è§£ï¼š
$$\begin{bmatrix} b \\ a \end{bmatrix} = (X^T X)^{-1} X^T \mathbf{y} = \begin{bmatrix} 0.33 \\ 1.5 \end{bmatrix}$$

æ‹Ÿåˆç›´çº¿ï¼š$y = 1.5x + 0.33$
</details>

---

## é™„å½•Dï¼šPythonå®ç°å®Œæ•´çº¿æ€§ä»£æ•°åº“

```python
class LinearAlgebra:
    """ä»é›¶å®ç°çº¿æ€§ä»£æ•°åº“ï¼ˆæ•™å­¦ç”¨ï¼‰"""
    
    @staticmethod
    def det(A):
        """è®¡ç®—è¡Œåˆ—å¼ï¼ˆé€’å½’ï¼‰"""
        n = len(A)
        if n == 1:
            return A[0][0]
        if n == 2:
            return A[0][0]*A[1][1] - A[0][1]*A[1][0]
        
        det_val = 0
        for j in range(n):
            # ä½™å­å¼
            minor = [row[:j] + row[j+1:] for row in A[1:]]
            det_val += ((-1)**j) * A[0][j] * LinearAlgebra.det(minor)
        return det_val
    
    @staticmethod
    def transpose(A):
        """è½¬ç½®"""
        return [[A[i][j] for i in range(len(A))] for j in range(len(A[0]))]
    
    @staticmethod
    def multiply(A, B):
        """çŸ©é˜µä¹˜æ³•"""
        rows_A, cols_A = len(A), len(A[0])
        rows_B, cols_B = len(B), len(B[0])
        
        C = [[0 for _ in range(cols_B)] for _ in range(rows_A)]
        for i in range(rows_A):
            for j in range(cols_B):
                for k in range(cols_A):
                    C[i][j] += A[i][k] * B[k][j]
        return C
    
    @staticmethod
    def gauss_eliminate(A, b):
        """é«˜æ–¯æ¶ˆå…ƒæ³•"""
        n = len(b)
        # å¢å¹¿çŸ©é˜µ
        M = [A[i] + [b[i]] for i in range(n)]
        
        # å‰å‘æ¶ˆå…ƒ
        for i in range(n):
            # é€‰ä¸»å…ƒ
            max_row = max(range(i, n), key=lambda r: abs(M[r][i]))
            M[i], M[max_row] = M[max_row], M[i]
            
            # æ¶ˆå…ƒ
            for j in range(i+1, n):
                factor = M[j][i] / M[i][i]
                for k in range(i, n+1):
                    M[j][k] -= factor * M[i][k]
        
        # å›ä»£
        x = [0] * n
        for i in range(n-1, -1, -1):
            x[i] = M[i][n] - sum(M[i][j] * x[j] for j in range(i+1, n))
            x[i] /= M[i][i]
        
        return x

# æµ‹è¯•
A = [[2, 1], [1, -1]]
b = [5, 1]
x = LinearAlgebra.gauss_eliminate(A, b)
print(f"è§£: {x}")  # [2.0, 1.0]
```

---

> **ğŸ“ å­¦ä¹ å®Œçº¿æ€§ä»£æ•°åï¼Œä½ åº”è¯¥èƒ½å¤Ÿ**ï¼š
> 1. âœ… ç†è§£å‘é‡ç©ºé—´ã€çº¿æ€§å˜æ¢çš„æœ¬è´¨
> 2. âœ… ç†Ÿç»ƒä½¿ç”¨NumPyè¿›è¡ŒçŸ©é˜µè¿ç®—
> 3. âœ… å°†çº¿æ€§ä»£æ•°åº”ç”¨åˆ°æœºå™¨å­¦ä¹ ã€å›¾å½¢å­¦ã€ä¼˜åŒ–ç­‰é¢†åŸŸ
> 4. âœ… é˜…è¯»è®ºæ–‡ä¸­çš„çŸ©é˜µå…¬å¼
> 5. âœ… ç‹¬ç«‹å®ç°PCAã€SVDç­‰ç®—æ³•
>
> **ä¸‹ä¸€æ­¥å­¦ä¹ æ–¹å‘**ï¼š
> - ğŸ“ˆ æœºå™¨å­¦ä¹ ï¼ˆæ¢¯åº¦ä¸‹é™ã€ç¥ç»ç½‘ç»œï¼‰
> - ğŸ® è®¡ç®—æœºå›¾å½¢å­¦ï¼ˆ3Då˜æ¢ã€å…‰ç…§ï¼‰
> - ğŸ”¢ æ•°å€¼ä¼˜åŒ–ï¼ˆå‡¸ä¼˜åŒ–ã€éçº¿æ€§è§„åˆ’ï¼‰
> - ğŸ§® é«˜ç­‰çº¿æ€§ä»£æ•°ï¼ˆå¼ é‡ã€æ³›å‡½åˆ†æï¼‰
>
> **Good luck!** ğŸš€

---

**æœ¬ç« å®Œ**
